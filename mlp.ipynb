{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Notebook for hyperparameter tuning of the MLP model](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Notebook for hyperparameter tuning of the MLP model](#toc1_)    \n",
    "- [Import libraries](#toc2_)    \n",
    "- [Import data](#toc3_)    \n",
    "- [Prepare data for training](#toc4_)    \n",
    "- [Test hyperparameters](#toc5_)    \n",
    "  - [Constant scheme](#toc5_1_)    \n",
    "  - [Pyramid scheme](#toc5_2_)    \n",
    "    - [Depth and width comparison](#toc5_2_1_)    \n",
    "    - [Model convergence](#toc5_2_2_)    \n",
    "  - [Regularization strength](#toc5_3_)    \n",
    "  - [Pyramid](#toc5_4_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Import libraries](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "import math\n",
    "import ast\n",
    "import gc\n",
    "import time\n",
    "\n",
    "from libs.models import *\n",
    "from libs.functions import *\n",
    "\n",
    "plt.rcParams.update({'font.size': 12, 'figure.figsize': (10, 4), 'figure.dpi': 300})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Import data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df = pd.read_csv('data/data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Prepare data for training](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare expanding window splits\n",
    "periods = {\n",
    "    '21' : '2020-01-01', # 2021 is the test set\n",
    "    # '22' : '2021-01-01', # 2022 is the test set\n",
    "    # '23' : '2022-01-01', # 2023 is the test set\n",
    "    # '24': '2023-01-01' # 2024 is the test set\n",
    "}\n",
    "\n",
    "# identify dummy vs. numeric columns\n",
    "feature_cols = [col for col in df.columns if col not in ['timestamp', 'ticker', 'target']]\n",
    "nace_cols = [c for c in feature_cols if c.startswith('NACE_')]\n",
    "dummy_cols = ['divi','divo'] # sin removed\n",
    "macro_cols = ['discount', 'tms', 'dp', 'ep', 'svar'] # 'bm_macro'\n",
    "\n",
    "# nummeric cols = cols not in cat and macro cols\n",
    "numeric_cols = [c for c in feature_cols if c not in dummy_cols and c not in nace_cols and c not in macro_cols]\n",
    "\n",
    "df_raw = df.copy(deep=True)\n",
    "df_raw['timestamp'] = pd.to_datetime(df_raw['timestamp'])\n",
    "\n",
    "# drop data from 2025\n",
    "df_raw = df_raw[df_raw['timestamp'] < '2025-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create interaction features between numeric and macro features\n",
    "C = df[numeric_cols].values         # shape = (n_rows, P_c)\n",
    "X = df[macro_cols].values           # shape = (n_rows, P_x)\n",
    "\n",
    "# compute all pairwise products with broadcasting:\n",
    "K = C[:,:,None] * X[:,None,:]\n",
    "\n",
    "# reshape to (n_rows, P_c * P_x)\n",
    "Z = K.reshape(len(df), -1)\n",
    "\n",
    "# build the column names in the same order\n",
    "xc_names = [\n",
    "    f\"{c}_x_{m}\"\n",
    "    for c in numeric_cols\n",
    "    for m in macro_cols\n",
    "]\n",
    "\n",
    "# wrap back into a DataFrame\n",
    "df_xc = pd.DataFrame(Z, columns=xc_names, index=df.index)\n",
    "\n",
    "feature_cols = numeric_cols + xc_names + dummy_cols + nace_cols\n",
    "numeric_cols = numeric_cols + xc_names\n",
    "cat_cols = dummy_cols + nace_cols\n",
    "df_z = df_raw.merge(df_xc, left_index=True, right_index=True)\n",
    "# drop macro_cols\n",
    "df_z = df_z.drop(columns=macro_cols)\n",
    "# sort columns by feature_cols\n",
    "df_norm = df_z[['timestamp', 'ticker', 'target'] + feature_cols]\n",
    "\n",
    "y_values = df_norm['target'].values.astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare containers\n",
    "X_train, X_val, X_test = {}, {}, {}\n",
    "y_train, y_val, y_test = {}, {}, {}\n",
    "preprocessors = {}\n",
    "y_scalers = {}\n",
    "\n",
    "for y, period in periods.items():\n",
    "    period = pd.to_datetime(period)\n",
    "\n",
    "    # split masks\n",
    "    tr_mask = df_norm['timestamp'] < period\n",
    "    va_mask = (df_norm['timestamp'] >= period) & \\\n",
    "              (df_norm['timestamp'] - pd.DateOffset(years=1) < period)\n",
    "    te_mask = (df_norm['timestamp'] - pd.DateOffset(years=1) >= period) & \\\n",
    "              (df_norm['timestamp'] - pd.DateOffset(years=2) < period)\n",
    "\n",
    "    # extract raw feature DataFrames\n",
    "    X_tr_df = df_norm.loc[tr_mask, feature_cols].copy()\n",
    "    X_va_df = df_norm.loc[va_mask, feature_cols].copy()\n",
    "    X_te_df = df_norm.loc[te_mask, feature_cols].copy()\n",
    "    y_tr    = y_values[tr_mask]\n",
    "    y_va    = y_values[va_mask]\n",
    "    y_te    = y_values[te_mask]\n",
    "\n",
    "    # compute winsorization bounds on train\n",
    "    lower = X_tr_df[numeric_cols].quantile(0.01)\n",
    "    upper = X_tr_df[numeric_cols].quantile(0.99)\n",
    "\n",
    "    # apply clipping to train, val, test\n",
    "    X_tr_df[numeric_cols] = X_tr_df[numeric_cols].clip(lower=lower, upper=upper, axis=1)\n",
    "    X_va_df[numeric_cols] = X_va_df[numeric_cols].clip(lower=lower, upper=upper, axis=1)\n",
    "    X_te_df[numeric_cols] = X_te_df[numeric_cols].clip(lower=lower, upper=upper, axis=1)\n",
    "\n",
    "\n",
    "    # now fit scaler on numeric only\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', StandardScaler(), numeric_cols),\n",
    "        ('cat', 'passthrough',  cat_cols)\n",
    "    ])\n",
    "    preprocessor.fit(X_tr_df)\n",
    "    preprocessors[y] = preprocessor\n",
    "\n",
    "    # transform all splits\n",
    "    X_train[y] = preprocessor.transform(X_tr_df).astype('float32')\n",
    "    X_val[y]   = preprocessor.transform(X_va_df).astype('float32')\n",
    "    X_test[y]  = preprocessor.transform(X_te_df).astype('float32')\n",
    "\n",
    "    # # fit standard scaler on y values\n",
    "    # y_scaler = StandardScaler()\n",
    "    # y_scaler.fit(y_tr.reshape(-1, 1))\n",
    "    # y_scalers[y] = y_scaler\n",
    "    # y_tr = y_scaler.transform(y_tr.reshape(-1, 1)).flatten()\n",
    "    # y_va = y_scaler.transform(y_va.reshape(-1, 1)).flatten()\n",
    "    # y_te = y_scaler.transform(y_te.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # store targets as before\n",
    "    y_train[y] = y_tr.reshape(-1, 1)\n",
    "    y_val[y]   = y_va.reshape(-1, 1)\n",
    "    y_test[y]  = y_te.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# moving to metal or CUDA GPU if available\n",
    "device = torch.device((\"cuda\" if torch.cuda.is_available() \n",
    "                       else \"mps\" if torch.backends.mps.is_available() \n",
    "                       else \"cpu\"))\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# general hyperparameters\n",
    "hidden_depth = None \n",
    "hidden_width = None \n",
    "learning_rate = 1e-4 \n",
    "activation_fun = nn.ReLU # nn.ReLU, nn.Tanh, nn.Sigmoid, nn.LeakyReLU\n",
    "\n",
    "# general critereon and regularization parameters\n",
    "criterion = nn.MSELoss()\n",
    "lambda_l1 = 1e-5 # baseline l1 regularization\n",
    "lambda_l2 = 1e-4 # baseline l2 regularization\n",
    "drop = 0.0\n",
    "\n",
    "# general parmeters\n",
    "patience = 25\n",
    "print_freq = 100\n",
    "epochs = 250\n",
    "batch_size = 4096 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = '21'\n",
    "models_21 = {}\n",
    "n_runs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model loader fun\n",
    "def load_model(w, d, run, lambda_l1, lambda_l2, drop, lr, activ = None, crit = None):\n",
    "    m = MLPModel(\n",
    "        input_dim  = X_train[year].shape[1],\n",
    "        depth      = d,\n",
    "        width      = w,\n",
    "        dropout    = drop,\n",
    "        activation = activation_fun,\n",
    "    ).to(device)\n",
    "    path = (\n",
    "        f\"models/hyperparam_test/mlp_y{year}\"\n",
    "        f\"_l1{lambda_l1}_l2{lambda_l2}\"\n",
    "        f\"_drop{drop}_lr{lr}\"\n",
    "        f\"_w{w}_d{d}_run{run+1}.pth\"\n",
    "    )\n",
    "    if activ is not None:\n",
    "        path = path.replace('.pth', f'_activ{activ}.pth')\n",
    "    if crit is not None:\n",
    "        path = path.replace('.pth', f'_crit{crit}.pth')\n",
    "    m.load_state_dict(torch.load(path, map_location=device))\n",
    "    m.eval()\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Test hyperparameters](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_1_'></a>[Constant scheme](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model with most data on multiple parameters\n",
    "l1_space = [lambda_l1]\n",
    "l2_space = [lambda_l2]\n",
    "dropout_space = [drop]\n",
    "learning_rate_space = [learning_rate]\n",
    "depth_space = [1, 2, 3, 4, 5, 6, 7]\n",
    "width_space = [8, 16, 32, 64, 128]\n",
    "best_models_size = {}\n",
    "history_size = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # current runtime: 3h 45m\n",
    "# for lambda_l1 in l1_space:\n",
    "#     for lambda_l2 in l2_space:\n",
    "#         for dropout in dropout_space:\n",
    "#             for learning_rate in learning_rate_space:\n",
    "#                 for hidden_depth in depth_space:\n",
    "#                     for hidden_width in width_space:\n",
    "#                         print(f\"\"\"Training model for year '{year}...: \n",
    "#                                 lambda_l1       ={lambda_l1:.0e}\n",
    "#                                 lambda_l2       ={lambda_l2:.0e}\n",
    "#                                 dropout         ={dropout:.0e}\n",
    "#                                 learning_rate   ={learning_rate:.0e}\n",
    "#                                 hidden_depth    ={hidden_depth}\n",
    "#                                 hidden_width    ={hidden_width}\"\"\")\n",
    "#                         for run in range(n_runs):\n",
    "#                             print(f\"Run {run+1} of {n_runs}\")\n",
    "#                             seed = 42+run   \n",
    "#                             np.random.seed(seed)\n",
    "#                             torch.manual_seed(seed)\n",
    "#                             # Initialize the model\n",
    "#                             input_dim = X_train[year].shape[1]\n",
    "#                             name = f'l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}'\n",
    "#                             models_21[name] = MLPModel(input_dim, depth=hidden_depth, width=hidden_width, dropout=dropout, activation=activation_fun).to(device)\n",
    "#                             optimizer = torch.optim.Adam(models_21[name].parameters(), lr=learning_rate)\n",
    "#                             train = MLPdataset(X_train[year], y_train[year])\n",
    "#                             val = MLPdataset(X_val[year], y_val[year])\n",
    "#                             best_models_size[name], history_size[name] = train_mlp(train,          \n",
    "#                                                             val,\n",
    "#                                                             models_21[name],\n",
    "#                                                             criterion,\n",
    "#                                                             epochs,\n",
    "#                                                             patience,\n",
    "#                                                             print_freq,\n",
    "#                                                             device,\n",
    "#                                                             optimizer,\n",
    "#                                                             lambda_l1=lambda_l1,\n",
    "#                                                             lambda_l2=lambda_l2,\n",
    "#                                                             batch_size=batch_size,\n",
    "#                                                             shuffle_train=True,\n",
    "#                                                             shuffle_val=False,\n",
    "#                                                             save_path=f'models/hyperparam_test/mlp_y{year}_l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}.pth'\n",
    "#                                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten_history(history_size, \n",
    "#                 save_csv='models/hyperparam_test/history/history_size.csv')\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_2_'></a>[Pyramid scheme](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model with most data on multiple parameters\n",
    "l1_space = [lambda_l1]\n",
    "l2_space = [lambda_l2]\n",
    "dropout_space = [drop]\n",
    "learning_rate_space = [learning_rate]\n",
    "depth_space = None\n",
    "width_space = [[32], \n",
    "               [32, 16], \n",
    "               [32, 16, 8], \n",
    "               [32, 16, 8, 4], \n",
    "               [32, 16, 8, 4, 2]]\n",
    "best_models_pyramid = {}\n",
    "history_pyramid = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # current runtime: 45m\n",
    "# for lambda_l1 in l1_space:\n",
    "#     for lambda_l2 in l2_space:\n",
    "#         for dropout in dropout_space:\n",
    "#             for learning_rate in learning_rate_space:\n",
    "#                 for hidden_width in width_space:\n",
    "#                     hidden_depth = len(hidden_width)\n",
    "#                     print(f\"\"\"Training model for year '{year}...: \n",
    "#                             lambda_l1       ={lambda_l1:.0e}\n",
    "#                             lambda_l2       ={lambda_l2:.0e}\n",
    "#                             dropout         ={dropout:.0e}\n",
    "#                             learning_rate   ={learning_rate:.0e}\n",
    "#                             hidden_depth    ={hidden_depth}\n",
    "#                             hidden_width    ={hidden_width}\"\"\")\n",
    "#                     for run in range(n_runs):\n",
    "#                         print(f\"Run {run+1} of {n_runs}\")\n",
    "#                         seed = 42+run\n",
    "#                         np.random.seed(seed)\n",
    "#                         torch.manual_seed(seed)\n",
    "#                         # Initialize the model\n",
    "#                         input_dim = X_train[year].shape[1]\n",
    "#                         name = f'l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}'\n",
    "#                         models_21[name] = MLPModel(input_dim, depth=hidden_depth, width=hidden_width, dropout=dropout, activation=activation_fun).to(device)\n",
    "#                         optimizer = torch.optim.Adam(models_21[name].parameters(), lr=learning_rate)\n",
    "#                         train = MLPdataset(X_train[year], y_train[year])\n",
    "#                         val = MLPdataset(X_val[year], y_val[year])\n",
    "#                         best_models_pyramid[name], history_pyramid[name] = train_mlp(train,          \n",
    "#                                                         val,\n",
    "#                                                         models_21[name],\n",
    "#                                                         criterion,\n",
    "#                                                         epochs,\n",
    "#                                                         patience,\n",
    "#                                                         print_freq,\n",
    "#                                                         device,\n",
    "#                                                         optimizer,\n",
    "#                                                         lambda_l1=lambda_l1,\n",
    "#                                                         lambda_l2=lambda_l2,\n",
    "#                                                         batch_size=batch_size,\n",
    "#                                                         shuffle_train=True,\n",
    "#                                                         shuffle_val=False,\n",
    "#                                                         save_path=f'models/hyperparam_test/mlp_y{year}_l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}.pth'\n",
    "#                                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten_history(history_pyramid, \n",
    "#                 save_csv='models/hyperparam_test/history/history_pyramid.csv')\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_2_1_'></a>[Depth and width comparison](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant-width grid\n",
    "const_widths = [8, 16, 32, 64, 128]\n",
    "const_depths = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "# pyramid sequences (each column of the right heatmap)\n",
    "pyr_seqs = [\n",
    "    [32],\n",
    "    [32,16],\n",
    "    [32,16,8],\n",
    "    [32,16,8,4],\n",
    "    [32,16,8,4,2],\n",
    "]\n",
    "Wc = len(const_widths)\n",
    "Dc = len(const_depths)\n",
    "Wp = len(pyr_seqs)\n",
    "\n",
    "# DataLoader\n",
    "val_ds     = MLPdataset(X_val[year], y_val[year])\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# constant-width loss matrix\n",
    "loss_const = np.zeros((Dc, Wc))\n",
    "for di, d in enumerate(const_depths):\n",
    "    for wj, w in enumerate(const_widths):\n",
    "        run_losses = []\n",
    "        for run in range(n_runs):\n",
    "            m = load_model(w, d, run, lambda_l1, lambda_l2, drop, learning_rate)\n",
    "            with torch.no_grad():\n",
    "                batch = [criterion(m(x.to(device)), y.to(device)).item()\n",
    "                         for x,y in val_loader]\n",
    "            run_losses.append(np.mean(batch))\n",
    "        loss_const[di, wj] = np.mean(run_losses)\n",
    "\n",
    "# pyramid loss matrix\n",
    "loss_pyr = np.full((Wp, Wp), np.nan)\n",
    "for sj, seq in enumerate(pyr_seqs):\n",
    "    depth = len(seq)\n",
    "    run_losses = []\n",
    "    for run in range(n_runs):\n",
    "        # we “flatten” seq into width param for loader\n",
    "        m = load_model(seq, depth, run, lambda_l1, lambda_l2, drop, learning_rate)\n",
    "        with torch.no_grad():\n",
    "            batch = [criterion(m(x.to(device)), y.to(device)).item()\n",
    "                     for x,y in val_loader]\n",
    "        run_losses.append(np.mean(batch))\n",
    "    loss_pyr[depth-1, sj] = np.mean(run_losses)\n",
    "\n",
    "# plot\n",
    "fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(12,5),\n",
    "                               gridspec_kw={'width_ratios':[4,1]})\n",
    "\n",
    "# color‐scale\n",
    "all_vals = loss_const.ravel()\n",
    "vmin    = all_vals.min()-0.0005\n",
    "vmax    = all_vals.max()\n",
    "vcenter = vmin + 0.5*(vmax - vmin)\n",
    "norm    = TwoSlopeNorm(vmin=vmin, vcenter=vcenter, vmax=vmax)\n",
    "cmap    = 'viridis'\n",
    "\n",
    "# left: constant‐width heatmap\n",
    "im0 = ax0.imshow(loss_const, aspect='auto', cmap=cmap, norm=norm)\n",
    "im0 = ax0.imshow(loss_const, aspect='auto', cmap=cmap, norm=norm)\n",
    "ax0.set_xticks(range(Wc))\n",
    "ax0.set_xticklabels(const_widths)\n",
    "ax0.set_yticks(range(Dc))\n",
    "ax0.set_yticklabels(const_depths)\n",
    "ax0.set_xlabel('Hidden Width')\n",
    "ax0.set_ylabel('Hidden Depth')\n",
    "ax0.set_title(f'Constant Width')\n",
    "\n",
    "# right: collapsed‐x pyramid heatmap\n",
    "diag_pyr = np.diag(loss_pyr)\n",
    "diag_mat = diag_pyr[:, np.newaxis]\n",
    "\n",
    "im1 = ax1.imshow(diag_mat, aspect='auto', cmap=cmap, norm=norm)\n",
    "ax1.set_xticks([0])\n",
    "ax1.set_xticklabels(['Geometric\\npyramid'])\n",
    "ax1.set_yticks(np.arange(Wp))\n",
    "ax1.set_yticklabels(np.arange(1, Wp+1))\n",
    "ax1.set_ylabel('Hidden Depth')\n",
    "ax1.set_title('Pyramid Scheme')\n",
    "\n",
    "# shared colorbar\n",
    "cbar = fig.colorbar(im0, ax=[ax0,ax1], shrink=0.9, pad=0.02)\n",
    "cbar.set_label('Average Validation Loss (MSE)')\n",
    "\n",
    "plt.savefig('figs/width_depth.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre‐compute global vmin/vmax across all runs so color‐scale is consistent\n",
    "all_vals = []\n",
    "for run in range(n_runs):\n",
    "    # constant‐width for this run\n",
    "    tmp = []\n",
    "    for d in const_depths:\n",
    "        for w in const_widths:\n",
    "            m = load_model(w, d, run, lambda_l1, lambda_l2, drop, learning_rate)\n",
    "            with torch.no_grad():\n",
    "                batch = [criterion(m(x.to(device)), y.to(device)).item()\n",
    "                         for x,y in val_loader]\n",
    "            tmp.append(np.mean(batch))\n",
    "    all_vals.extend(tmp)\n",
    "\n",
    "    # pyramid diag for this run\n",
    "    diag = []\n",
    "    for seq in pyr_seqs:\n",
    "        depth = len(seq)\n",
    "        m = load_model(seq, depth, run, lambda_l1, lambda_l2, drop, learning_rate)\n",
    "        with torch.no_grad():\n",
    "            batch = [criterion(m(x.to(device)), y.to(device)).item()\n",
    "                     for x,y in val_loader]\n",
    "        diag.append(np.mean(batch))\n",
    "    all_vals.extend(diag)\n",
    "\n",
    "runs = list(range(n_runs))\n",
    "# split into pages of 4 runs each\n",
    "pages = [runs[i:i+4] for i in range(0, len(runs), 4)]\n",
    "\n",
    "for p, page_runs in enumerate(pages, start=1):\n",
    "    nrows = len(page_runs)\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows, 2,\n",
    "        figsize=(10, 3*nrows),\n",
    "        gridspec_kw={'width_ratios': [4, 1]},\n",
    "        constrained_layout=False  # turn off for manual colorbar placement\n",
    "    )\n",
    "\n",
    "    # if only one row, wrap axes\n",
    "    if nrows == 1:\n",
    "        axes = np.expand_dims(axes, 0)\n",
    "\n",
    "    for i, run in enumerate(page_runs):\n",
    "        ax0, ax1 = axes[i]\n",
    "\n",
    "        # ---- left: constant-width heatmap for this run ----\n",
    "        loss_const = np.zeros((Dc, Wc))\n",
    "        for di, d in enumerate(const_depths):\n",
    "            for wj, w in enumerate(const_widths):\n",
    "                m = load_model(w, d, run, lambda_l1, lambda_l2, drop, learning_rate)\n",
    "                with torch.no_grad():\n",
    "                    batch = [criterion(m(x.to(device)), y.to(device)).item()\n",
    "                             for x,y in val_loader]\n",
    "                loss_const[di, wj] = np.mean(batch)\n",
    "\n",
    "        im0 = ax0.imshow(loss_const, aspect='auto', cmap=cmap, norm=norm)\n",
    "        if i == 0:\n",
    "            ax0.set_title('Constant Width')\n",
    "        ax0.set_ylabel(f'Run {run+1}\\nHidden Depth')\n",
    "        ax0.set_xticks(range(Wc))\n",
    "        ax0.set_xticklabels(const_widths)\n",
    "        ax0.set_yticks(range(Dc))\n",
    "        ax0.set_yticklabels(const_depths)\n",
    "        if i == nrows - 1:\n",
    "            ax0.set_xlabel('Hidden Width')\n",
    "        else:\n",
    "            ax0.set_xlabel('')\n",
    "            ax0.tick_params(labelbottom=False)\n",
    "\n",
    "        # ---- right: collapsed pyramid for this run ----\n",
    "        diag = []\n",
    "        for seq in pyr_seqs:\n",
    "            depth = len(seq)\n",
    "            m = load_model(seq, depth, run, lambda_l1, lambda_l2, drop, learning_rate)\n",
    "            with torch.no_grad():\n",
    "                batch = [criterion(m(x.to(device)), y.to(device)).item()\n",
    "                         for x,y in val_loader]\n",
    "            diag.append(np.mean(batch))\n",
    "        diag_mat = np.array(diag)[:, None]\n",
    "\n",
    "        im1 = ax1.imshow(diag_mat, aspect='auto', cmap=cmap, norm=norm)\n",
    "        if i == 0:\n",
    "            ax1.set_title('Pyramid Scheme')\n",
    "        ax1.set_xticks([0])\n",
    "        # only label the bottom subplot\n",
    "        if i == nrows - 1:\n",
    "            ax1.set_xticklabels(['Geometric\\npyramid'])\n",
    "        else:\n",
    "            ax1.set_xticklabels([])\n",
    "        ax1.set_yticks(range(Wp))\n",
    "        ax1.set_yticklabels(range(1, Wp+1))\n",
    "        ax1.set_ylabel('Hidden Depth')\n",
    "\n",
    "    # shared colorbar on the right of this page\n",
    "    cax = fig.add_axes([0.92,  # 92% from left\n",
    "                        0.11,  # 10% from bottom\n",
    "                        0.02,  # 2% figure‐width\n",
    "                        0.77   # 80% figure‐height\n",
    "                       ])\n",
    "    cbar = fig.colorbar(im0, cax=cax)\n",
    "    cbar.set_label('Validation Loss (MSE)')\n",
    "\n",
    "    fig.savefig(f'figs/width_depth_page{p}.png', dpi=300, bbox_inches='tight')\n",
    "    # plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_2_2_'></a>[Model convergence](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSVs\n",
    "df_size = pd.read_csv('models/hyperparam_test/history/history_size.csv')\n",
    "df_pyr  = pd.read_csv('models/hyperparam_test/history/history_pyramid.csv')\n",
    "\n",
    "# extract name\n",
    "df_size['widths'] = df_size['widths'].apply(ast.literal_eval)\n",
    "df_pyr ['widths'] = df_pyr ['widths'].apply(ast.literal_eval)\n",
    "\n",
    "df_size['width'] = df_size['widths'].apply(lambda w: w[0])\n",
    "df_pyr ['width'] = df_pyr ['widths'].apply(lambda w: w[0])\n",
    "\n",
    "df_size['name'] = df_size['width'].apply(\n",
    "    lambda w: f\"Constant width = {w}\"\n",
    ")\n",
    "\n",
    "width_to_str = (\n",
    "    df_pyr\n",
    "    .groupby('width')['widths_str']\n",
    "    .agg(lambda ss: max(ss, key=len))\n",
    "    .to_dict()\n",
    ")\n",
    "df_pyr['name'] = df_pyr['width'].map(\n",
    "    lambda w: f\"Pyramid width = {width_to_str[w]}\"\n",
    ")\n",
    "\n",
    "# combine\n",
    "df = pd.concat([df_size, df_pyr], ignore_index=True)\n",
    "\n",
    "# sort by width\n",
    "panel_df = (\n",
    "    df[['name','width']]            # pick only the two columns we care about\n",
    "      .drop_duplicates()            # one row per unique panel\n",
    "      .assign(is_pyr=lambda x: x['name'].str.startswith('Pyramid'))\n",
    "      .sort_values(['is_pyr','width'])  # constants (is_pyr=False) first, then pyramids; each by ascending width\n",
    ")\n",
    "\n",
    "plot_names = panel_df['name'].tolist()\n",
    "n = len(plot_names)\n",
    "n_cols = min(3, n)\n",
    "n_rows = math.ceil(n / n_cols)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols,\n",
    "                         figsize=(4*n_cols, 4*n_rows),\n",
    "                         sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "# build a unified color map over depths\n",
    "depths = sorted(df['depth'].unique())\n",
    "cmap   = plt.get_cmap('viridis')\n",
    "colors = {d: cmap(i/(len(depths)-1)) for i, d in enumerate(depths)}\n",
    "\n",
    "for idx, name in enumerate(plot_names):\n",
    "    ax = axes[idx]\n",
    "    sub_df = df[df['name'] == name]\n",
    "    for d in depths:\n",
    "        sd = sub_df[sub_df['depth'] == d]\n",
    "        for run in sd['run'].unique():\n",
    "            run_df = sd[sd['run'] == run].sort_values('epoch')\n",
    "            ax.plot(\n",
    "                run_df['epoch'],\n",
    "                run_df['val_loss'],\n",
    "                color=colors[d],\n",
    "                alpha=0.5,\n",
    "                linewidth=2,\n",
    "                label=(f\"d={d}\" if run == sd['run'].min() else None)\n",
    "            )\n",
    "\n",
    "    ax.set_title(name)\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_ylim(1.9e-2, 4.1e-2)\n",
    "    ax.set_xlim(0, 250)\n",
    "    ax.set_xticks(np.linspace(0, 250, 6, dtype=int))\n",
    "\n",
    "    # only bottom row: x-label\n",
    "    if idx // n_cols == n_rows - 1:\n",
    "        ax.set_xlabel(\"Epoch\", fontsize=12)\n",
    "    else:\n",
    "        ax.set_xticklabels([])\n",
    "\n",
    "    # only leftmost column: y-label\n",
    "    if idx % n_cols == 0:\n",
    "        ax.set_ylabel(\"Validation Loss\", fontsize=12)\n",
    "\n",
    "# remove any unused axes\n",
    "for j in range(len(plot_names), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# global legend on right\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels,\n",
    "           title=\"Hidden Depth\",\n",
    "           loc='center left',\n",
    "           bbox_to_anchor=(1, 0.5),\n",
    "           fontsize=12)\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.savefig('figs/val_loss_history.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_3_'></a>[Regularization strength](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model with most data on multiple parameters\n",
    "l1_space = [0.0, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "l2_space = [0.0, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "dropout_space = [0.0, 0.1, 0.2, 0.3] \n",
    "learning_rate_space = [learning_rate] \n",
    "depth_space = [4] \n",
    "width_space = [16]\n",
    "best_models_reg = {}\n",
    "history_reg = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # current runtime: 15h\n",
    "# for lambda_l1 in l1_space:\n",
    "#     for lambda_l2 in l2_space:\n",
    "#         for dropout in dropout_space:\n",
    "#             for learning_rate in learning_rate_space:\n",
    "#                 for hidden_depth in depth_space:\n",
    "#                     for hidden_width in width_space:\n",
    "#                         print(f\"\"\"Training model for year '{year}...: \n",
    "#                                 lambda_l1       ={lambda_l1:.0e}\n",
    "#                                 lambda_l2       ={lambda_l2:.0e}\n",
    "#                                 dropout         ={dropout:.0e}\n",
    "#                                 learning_rate   ={learning_rate:.0e}\n",
    "#                                 hidden_depth    ={hidden_depth}\n",
    "#                                 hidden_width    ={hidden_width}\"\"\")\n",
    "#                         for run in range(n_runs):\n",
    "#                             print(f\"Run {run+1} of {n_runs}\")\n",
    "#                             seed = 42+run\n",
    "#                             np.random.seed(seed)\n",
    "#                             torch.manual_seed(seed)\n",
    "#                             # Initialize the model\n",
    "#                             input_dim = X_train[year].shape[1]\n",
    "#                             name = f'l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}'\n",
    "#                             models_21[name] = MLPModel(input_dim, depth=hidden_depth, width=hidden_width, dropout=dropout, activation=activation_fun).to(device)\n",
    "#                             optimizer = torch.optim.Adam(models_21[name].parameters(), lr=learning_rate)\n",
    "#                             train = MLPdataset(X_train[year], y_train[year])\n",
    "#                             val = MLPdataset(X_val[year], y_val[year])\n",
    "#                             best_models_reg[name], history_reg[name] = train_mlp(train,          \n",
    "#                                                             val,\n",
    "#                                                             models_21[name],\n",
    "#                                                             criterion,\n",
    "#                                                             epochs,\n",
    "#                                                             patience,\n",
    "#                                                             print_freq,\n",
    "#                                                             device,\n",
    "#                                                             optimizer,\n",
    "#                                                             lambda_l1=lambda_l1,\n",
    "#                                                             lambda_l2=lambda_l2,\n",
    "#                                                             batch_size=batch_size,\n",
    "#                                                             shuffle_train=True,\n",
    "#                                                             shuffle_val=False,\n",
    "#                                                             save_path=f'models/hyperparam_test/mlp_y{year}_l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}.pth'\n",
    "#                                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten_history(history_reg, \n",
    "#                 save_csv='models/hyperparam_test/history/history_reg.csv')\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model with most data on multiple parameters\n",
    "l1_space = [0.0, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "l2_space = [0.0, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "dropout_space = [0.0, 0.1, 0.2, 0.3] \n",
    "learning_rate_space = [learning_rate] \n",
    "depth_space = [4] \n",
    "width_space = [16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "val_ds     = MLPdataset(X_val[year], y_val[year])\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# build (n_drop, n_l1, n_l2) mean-loss array\n",
    "n_l1   = len(l1_space)\n",
    "n_l2   = len(l2_space)\n",
    "n_drop = len(dropout_space)\n",
    "\n",
    "losses = np.zeros((n_drop, n_l1, n_l2), dtype=float)\n",
    "\n",
    "for di, drop in enumerate(dropout_space):\n",
    "    for i, l1 in enumerate(l1_space):\n",
    "        for j, l2 in enumerate(l2_space):\n",
    "            run_losses = []\n",
    "            # for each seed, load & eval\n",
    "            for run in range(n_runs):\n",
    "                # since depth_space & width_space each have one entry, \n",
    "                # we can just index 0 here — but this will generalize\n",
    "                tmp_losses = []\n",
    "                for d in depth_space:\n",
    "                    for w in width_space:\n",
    "                        m = load_model(w, d, run, l1, l2, drop, learning_rate, )\n",
    "                        with torch.no_grad():\n",
    "                            batch = [criterion(m(x.to(device)), y.to(device)).item()\n",
    "                                     for x,y in val_loader]\n",
    "                        tmp_losses.append(np.mean(batch))\n",
    "                run_losses.append(np.mean(tmp_losses))\n",
    "            # now average over runs\n",
    "            losses[di, i, j] = np.mean(run_losses)\n",
    "\n",
    "\n",
    "# global color‐scale\n",
    "vmin    = losses.min()\n",
    "vmax    = losses.max()\n",
    "vcenter = vmin + 0.5*(vmax - vmin)\n",
    "norm    = TwoSlopeNorm(vmin=vmin, vcenter=vcenter, vmax=vmax)\n",
    "cmap    = 'viridis'\n",
    "\n",
    "# layout: up to 2 columns\n",
    "ncols = min(2, n_drop)\n",
    "nrows = math.ceil(n_drop / ncols)\n",
    "\n",
    "fig, axes = plt.subplots(nrows, ncols,\n",
    "                         figsize=(5*ncols, 4*nrows),\n",
    "                         squeeze=False)\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for idx, drop in enumerate(dropout_space):\n",
    "    ax = axes_flat[idx]\n",
    "    im = ax.imshow(\n",
    "        losses[idx],\n",
    "        aspect='auto',\n",
    "        cmap=cmap,\n",
    "        norm=norm,\n",
    "        origin='lower',\n",
    "    )\n",
    "\n",
    "    # y = l1, x = l2\n",
    "    ax.set_yticks(np.arange(n_l1))\n",
    "    ax.set_yticklabels(l1_space)\n",
    "    ax.set_xticks(np.arange(n_l2))\n",
    "    ax.set_xticklabels(l2_space)\n",
    "\n",
    "    # only bottom row shows x‐labels\n",
    "    if idx < (nrows-1)*ncols:\n",
    "        ax.tick_params(labelbottom=False)\n",
    "    else:\n",
    "        ax.set_xlabel(r'$\\ell_2$')\n",
    "\n",
    "    # only first‐column shows y‐labels\n",
    "    if idx % ncols != 0:\n",
    "        ax.tick_params(labelleft=False)\n",
    "    else:\n",
    "        ax.set_ylabel(r'$\\ell_1$')\n",
    "\n",
    "    ax.set_title(f\"dropout = {drop:.2f}\")\n",
    "\n",
    "# clear unused\n",
    "for ax in axes_flat[n_drop:]:\n",
    "    fig.delaxes(ax)\n",
    "\n",
    "# colorbar at the right edge\n",
    "fig.subplots_adjust(right=0.88)\n",
    "cax = fig.add_axes([0.90, 0.15, 0.02, 0.7])  # [left, bottom, width, height]\n",
    "cbar = fig.colorbar(im, cax=cax)\n",
    "cbar.set_label('Average Validation Loss (MSE)')\n",
    "\n",
    "plt.savefig('figs/l1_l2_dropout_loss.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_4_'></a>[Pyramid](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model with most data on multiple parameters\n",
    "l1_space = [0.0, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "l2_space = [0.0, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "dropout_space = [0.0, 0.1, 0.2, 0.3] \n",
    "learning_rate_space = [learning_rate]\n",
    "depth_space = None\n",
    "width_space = [[32, 16, 8]]\n",
    "\n",
    "best_models_reg_pyramid = {}\n",
    "history_reg_pyramid = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # current runtime: 15h\n",
    "# for lambda_l1 in l1_space:\n",
    "#     for lambda_l2 in l2_space:\n",
    "#         for dropout in dropout_space:\n",
    "#             for learning_rate in learning_rate_space:\n",
    "#                 for hidden_width in width_space:\n",
    "#                     hidden_depth = len(hidden_width)\n",
    "#                     print(f\"\"\"Training model for year '{year}...: \n",
    "#                             lambda_l1       ={lambda_l1:.0e}\n",
    "#                             lambda_l2       ={lambda_l2:.0e}\n",
    "#                             dropout         ={dropout:.0e}\n",
    "#                             learning_rate   ={learning_rate:.0e}\n",
    "#                             hidden_depth    ={hidden_depth}\n",
    "#                             hidden_width    ={hidden_width}\"\"\")\n",
    "#                     for run in range(n_runs):\n",
    "#                         print(f\"Run {run+1} of {n_runs}\")\n",
    "#                         seed = 42+run\n",
    "#                         np.random.seed(seed)\n",
    "#                         torch.manual_seed(seed)\n",
    "#                         # Initialize the model\n",
    "#                         input_dim = X_train[year].shape[1]\n",
    "#                         name = f'l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}'\n",
    "#                         models_21[name] = MLPModel(input_dim, depth=hidden_depth, width=hidden_width, dropout=dropout, activation=activation_fun).to(device)\n",
    "#                         optimizer = torch.optim.Adam(models_21[name].parameters(), lr=learning_rate)\n",
    "#                         train = MLPdataset(X_train[year], y_train[year])\n",
    "#                         val = MLPdataset(X_val[year], y_val[year])\n",
    "#                         best_models_reg_pyramid[name], history_reg_pyramid[name] = train_mlp(train,          \n",
    "#                                                         val,\n",
    "#                                                         models_21[name],\n",
    "#                                                         criterion,\n",
    "#                                                         epochs,\n",
    "#                                                         patience,\n",
    "#                                                         print_freq,\n",
    "#                                                         device,\n",
    "#                                                         optimizer,\n",
    "#                                                         lambda_l1=lambda_l1,\n",
    "#                                                         lambda_l2=lambda_l2,\n",
    "#                                                         batch_size=batch_size,\n",
    "#                                                         shuffle_train=True,\n",
    "#                                                         shuffle_val=False,\n",
    "#                                                         save_path=f'models/hyperparam_test/mlp_y{year}_l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}.pth'\n",
    "#                                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "val_ds     = MLPdataset(X_val[year], y_val[year])\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# build (n_drop, n_l1, n_l2) mean-loss array\n",
    "n_l1   = len(l1_space)\n",
    "n_l2   = len(l2_space)\n",
    "n_drop = len(dropout_space)\n",
    "\n",
    "losses = np.zeros((n_drop, n_l1, n_l2), dtype=float)\n",
    "\n",
    "for di, drop in enumerate(dropout_space):\n",
    "    for i, l1 in enumerate(l1_space):\n",
    "        for j, l2 in enumerate(l2_space):\n",
    "            run_losses = []\n",
    "            # for each seed, load & eval\n",
    "            for run in range(n_runs):\n",
    "                # since depth_space & width_space each have one entry, \n",
    "                # we can just index 0 here — but this will generalize\n",
    "                tmp_losses = []\n",
    "                for w in width_space:\n",
    "                    d = len(w)  # since w is a list, we can use its length\n",
    "                    m = load_model(w, d, run, l1, l2, drop, learning_rate, )\n",
    "                    with torch.no_grad():\n",
    "                        batch = [criterion(m(x.to(device)), y.to(device)).item()\n",
    "                                    for x,y in val_loader]\n",
    "                    tmp_losses.append(np.mean(batch))\n",
    "                run_losses.append(np.mean(tmp_losses))\n",
    "            # now average over runs\n",
    "            losses[di, i, j] = np.mean(run_losses)\n",
    "\n",
    "\n",
    "# global color‐scale\n",
    "vmin    = losses.min()\n",
    "vmax    = losses.max()\n",
    "vcenter = vmin + 0.5*(vmax - vmin)\n",
    "norm    = TwoSlopeNorm(vmin=vmin, vcenter=vcenter, vmax=vmax)\n",
    "cmap    = 'viridis'\n",
    "\n",
    "# layout: up to 2 columns\n",
    "ncols = min(2, n_drop)\n",
    "nrows = math.ceil(n_drop / ncols)\n",
    "\n",
    "fig, axes = plt.subplots(nrows, ncols,\n",
    "                         figsize=(5*ncols, 4*nrows),\n",
    "                         squeeze=False)\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for idx, drop in enumerate(dropout_space):\n",
    "    ax = axes_flat[idx]\n",
    "    im = ax.imshow(\n",
    "        losses[idx],\n",
    "        aspect='auto',\n",
    "        cmap=cmap,\n",
    "        norm=norm,\n",
    "        origin='lower',\n",
    "    )\n",
    "\n",
    "    # y = l1, x = l2\n",
    "    ax.set_yticks(np.arange(n_l1))\n",
    "    ax.set_yticklabels(l1_space)\n",
    "    ax.set_xticks(np.arange(n_l2))\n",
    "    ax.set_xticklabels(l2_space)\n",
    "\n",
    "    # only bottom row shows x‐labels\n",
    "    if idx < (nrows-1)*ncols:\n",
    "        ax.tick_params(labelbottom=False)\n",
    "    else:\n",
    "        ax.set_xlabel(r'$\\ell_2$')\n",
    "\n",
    "    # only first‐column shows y‐labels\n",
    "    if idx % ncols != 0:\n",
    "        ax.tick_params(labelleft=False)\n",
    "    else:\n",
    "        ax.set_ylabel(r'$\\ell_1$')\n",
    "\n",
    "    ax.set_title(f\"dropout = {drop:.2f}\")\n",
    "\n",
    "# clear unused\n",
    "for ax in axes_flat[n_drop:]:\n",
    "    fig.delaxes(ax)\n",
    "\n",
    "# colorbar at the right edge\n",
    "fig.subplots_adjust(right=0.88)\n",
    "cax = fig.add_axes([0.90, 0.15, 0.02, 0.7])  # [left, bottom, width, height]\n",
    "cbar = fig.colorbar(im, cax=cax)\n",
    "cbar.set_label('Average Validation Loss (MSE)')\n",
    "\n",
    "plt.savefig('figs/l1_l2_dropout_loss_pyramid.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test of activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general hyperparameters\n",
    "hidden_depth = 4 \n",
    "hidden_width = 16 \n",
    "learning_rate = 1e-4 \n",
    "\n",
    "# general critereon and regularization parameters\n",
    "criterion = nn.MSELoss()\n",
    "lambda_l1 = 1e-5 # baseline l1 regularization\n",
    "lambda_l2 = 1e-4 # baseline l2 regularization\n",
    "dropout = 0.0\n",
    "\n",
    "activation_space = {'ReLU':nn.ReLU, 'LeakyReLU':nn.LeakyReLU, 'Tanh':nn.Tanh, 'Sigmoid':nn.Sigmoid}\n",
    "\n",
    "best_models_activ = {}\n",
    "history_activ = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for activ_name, activ_fun in activation_space.items():\n",
    "#     run_time = []\n",
    "#     start_time = time.time()\n",
    "#     print(f\"\"\"Training model for year '{year}...: \n",
    "#             activation      ={activ_name}\n",
    "#             lambda_l1       ={lambda_l1:.0e}\n",
    "#             lambda_l2       ={lambda_l2:.0e}\n",
    "#             dropout         ={dropout:.0e}\n",
    "#             learning_rate   ={learning_rate:.0e}\n",
    "#             hidden_depth    ={hidden_depth}\n",
    "#             hidden_width    ={hidden_width}\"\"\")\n",
    "#     for run in range(n_runs):\n",
    "#         print(f\"Run {run+1} of {n_runs}\")\n",
    "#         seed = 42+run\n",
    "#         np.random.seed(seed)\n",
    "#         torch.manual_seed(seed)\n",
    "#         # Initialize the model\n",
    "#         input_dim = X_train[year].shape[1]\n",
    "#         name = f'l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}_activ{activ_name}'\n",
    "#         models_21[name] = MLPModel(input_dim, depth=hidden_depth, width=hidden_width, dropout=dropout, activation=activ_fun).to(device)\n",
    "#         optimizer = torch.optim.Adam(models_21[name].parameters(), lr=learning_rate)\n",
    "#         train = MLPdataset(X_train[year], y_train[year])\n",
    "#         val = MLPdataset(X_val[year], y_val[year])\n",
    "#         best_models_activ[name], history_activ[name], time_ = train_mlp(train,          \n",
    "#                                         val,\n",
    "#                                         models_21[name],\n",
    "#                                         criterion,\n",
    "#                                         epochs,\n",
    "#                                         patience,\n",
    "#                                         print_freq,\n",
    "#                                         device,\n",
    "#                                         optimizer,\n",
    "#                                         lambda_l1=lambda_l1,\n",
    "#                                         lambda_l2=lambda_l2,\n",
    "#                                         batch_size=batch_size,\n",
    "#                                         shuffle_train=True,\n",
    "#                                         shuffle_val=False,\n",
    "#                                         save_path=f'models/hyperparam_test/mlp_y{year}_l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}_activ{activ_name}.pth',\n",
    "#                                         timing = True\n",
    "#                                         )\n",
    "#         run_time.append(time_)\n",
    "#     end_time = time.time()\n",
    "#     print(f\"Training with activation nn.{activ_name} took {end_time - start_time:.2f} seconds.\")\n",
    "#     print(f\"Average time per epoch: {np.concatenate(run_time).mean()} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten_history_activ(history_activ, \n",
    "#                 save_csv='models/hyperparam_test/history/history_activ.csv')\n",
    "# # gc.collect()\n",
    "# # torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_activ = pd.read_csv('models/hyperparam_test/history/history_activ.csv')\n",
    "\n",
    "activations = sorted(df_activ['activ_name'].unique())\n",
    "n_acts = len(activations)+1\n",
    "cmap = plt.get_cmap('viridis', n_acts)\n",
    "color_map = {act: cmap(i) for i, act in enumerate(activations)}\n",
    "\n",
    "# create 2x2 grid\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "# plot\n",
    "for i, act in enumerate(activations):\n",
    "    ax = axes[i]\n",
    "    df_act = df_activ[df_activ['activ_name'] == act]\n",
    "    # Plot each run for this activation\n",
    "    for run_id, grp in df_act.groupby('run'):\n",
    "        ax.plot(\n",
    "            grp['epoch'],\n",
    "            grp['val_loss'],\n",
    "            color=color_map[act],\n",
    "            alpha=0.5,\n",
    "            linewidth=2,\n",
    "        )\n",
    "    ax.set_title(act)\n",
    "    # show x-label on bottom row \n",
    "    if i // 2 == 1:\n",
    "        ax.set_xlabel('Epoch', fontsize=12)\n",
    "    else:\n",
    "        ax.tick_params(labelbottom=False)\n",
    "    # show y-label on left column\n",
    "    if i % 2 == 0:\n",
    "        ax.set_ylabel('Validation Loss', fontsize=12)\n",
    "    else:\n",
    "        ax.tick_params(labelleft=False)\n",
    "    ax.set_ylim(0.019, 0.03)\n",
    "\n",
    "# remove any unused subplots\n",
    "for j in range(len(activations), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('figs/activation_fun_history.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models for activation ReLU...\n",
      "Loading models for activation LeakyReLU...\n",
      "Loading models for activation Tanh...\n",
      "Loading models for activation Sigmoid...\n"
     ]
    }
   ],
   "source": [
    "# load the best model for each activation function for each run\n",
    "preds_activ = {}\n",
    "\n",
    "for activ_name, activ_fun in activation_space.items():\n",
    "    print(f\"Loading models for activation {activ_name}...\")\n",
    "    all_preds = []\n",
    "    activation_fun = activ_fun\n",
    "    for run in range(n_runs):\n",
    "        best_models_activ[f'activ{activ_name}_run{run+1}'] = load_model(\n",
    "            hidden_width, hidden_depth, run, \n",
    "            lambda_l1, lambda_l2, dropout, learning_rate, activ=activ_name\n",
    "        )\n",
    "        preds = predict_mlp(\n",
    "            best_models_activ[f'activ{activ_name}_run{run+1}'], \n",
    "            X_val[year], \n",
    "            y_val[year], \n",
    "            scaler = None,\n",
    "            batch_size=batch_size,\n",
    "            device=device,\n",
    "        )\n",
    "        all_preds.append(preds)\n",
    "    np.stack(all_preds, axis=0)\n",
    "    preds_activ[activ_name] = np.mean(all_preds, axis=0)\n",
    "\n",
    "df_preds_activ = pd.DataFrame(preds_activ)\n",
    "df_preds_activ['y_true'] = y_val[year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_full = {}\n",
    "results_full = {}\n",
    "for activ_name in activation_space.keys():\n",
    "    y_true = df_preds_activ['y_true'].values\n",
    "    y_pred = df_preds_activ[activ_name].values\n",
    "    results_full[activ_name] = {\n",
    "        'RMSE': rmse_fun(y_true, y_pred),\n",
    "        'MAE': mae_fun(y_true, y_pred),\n",
    "        'AMADL': amadl_fun(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "for metric in ['RMSE', 'MAE', 'AMADL']:\n",
    "        vals = [\n",
    "        results_full['ReLU'][metric],\n",
    "        results_full['LeakyReLU'][metric],\n",
    "        results_full['Tanh'][metric],\n",
    "        results_full['Sigmoid'][metric],\n",
    "        ]\n",
    "        metrics_full[metric] = vals\n",
    "\n",
    "tab_activ = latex_table(list(activation_space.keys()),metrics_full)\n",
    "with open('tabs/activ_fun_perf.tex', 'w') as f:\n",
    "    f.write(tab_activ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test of criterion functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general hyperparameters\n",
    "hidden_depth = 4 \n",
    "hidden_width = 16 \n",
    "learning_rate = 1e-4 \n",
    "\n",
    "# general critereon and regularization parameters\n",
    "lambda_l1 = 1e-5 # baseline l1 regularization\n",
    "lambda_l2 = 1e-4 # baseline l2 regularization\n",
    "dropout = 0.0\n",
    "activation_fun = nn.ReLU\n",
    "\n",
    "criterion_space = {'MSE':nn.MSELoss(),'MAE':nn.L1Loss(),'Huber':nn.HuberLoss()} # nn.HuberLoss(delta=1.0) and nn.SmoothL1Loss(beta=1.0) are equivalent, they only differ if delta, beta \\neq 1.0\n",
    "\n",
    "best_models_crit = {}\n",
    "history_crit = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for year '21...: \n",
      "            criterion       =MSE\n",
      "            lambda_l1       =1e-05\n",
      "            lambda_l2       =1e-04\n",
      "            dropout         =0e+00\n",
      "            learning_rate   =1e-04\n",
      "            hidden_depth    =4\n",
      "            hidden_width    =16\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 45\n",
      "Best val loss: 1.70083E-02\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run1_critMSE.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.67659E-02  - Val Loss: 1.68824E-02\n",
      "Early stopping at epoch 184\n",
      "Best val loss: 1.67540E-02\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run2_critMSE.pth\n",
      "Run 3 of 5\n",
      "Early stopping at epoch 43\n",
      "Best val loss: 1.68717E-02\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run3_critMSE.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 53\n",
      "Best val loss: 1.68753E-02\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run4_critMSE.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.88845E-02  - Val Loss: 1.72669E-02\n",
      "Epoch 200/250  - Train Loss: 1.74747E-02  - Val Loss: 1.70462E-02\n",
      "Early stopping at epoch 246\n",
      "Best val loss: 1.69938E-02\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run5_critMSE.pth\n",
      "Training with criterion MSELoss() took 223.79 seconds.\n",
      "Average time per epoch: 0.3897913542747699 seconds.\n",
      "Training model for year '21...: \n",
      "            criterion       =MAE\n",
      "            lambda_l1       =1e-05\n",
      "            lambda_l2       =1e-04\n",
      "            dropout         =0e+00\n",
      "            learning_rate   =1e-04\n",
      "            hidden_depth    =4\n",
      "            hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 7.74594E-02  - Val Loss: 8.40910E-02\n",
      "Early stopping at epoch 106\n",
      "Best val loss: 8.40336E-02\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run1_critMAE.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 7.62891E-02  - Val Loss: 8.42415E-02\n",
      "Early stopping at epoch 106\n",
      "Best val loss: 8.42267E-02\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run2_critMAE.pth\n",
      "Run 3 of 5\n",
      "Early stopping at epoch 39\n",
      "Best val loss: 8.36955E-02\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run3_critMAE.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 50\n",
      "Best val loss: 8.42466E-02\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run4_critMAE.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 7.86665E-02  - Val Loss: 8.48821E-02\n",
      "Epoch 200/250  - Train Loss: 7.69383E-02  - Val Loss: 8.44408E-02\n",
      "Best val loss: 8.43591E-02\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run5_critMAE.pth\n",
      "Training with criterion L1Loss() took 230.12 seconds.\n",
      "Average time per epoch: 0.41628343237461707 seconds.\n",
      "Training model for year '21...: \n",
      "            criterion       =Huber\n",
      "            lambda_l1       =1e-05\n",
      "            lambda_l2       =1e-04\n",
      "            dropout         =0e+00\n",
      "            learning_rate   =1e-04\n",
      "            hidden_depth    =4\n",
      "            hidden_width    =16\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 62\n",
      "Best val loss: 8.49121E-03\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run1_critHuber.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 8.91225E-03  - Val Loss: 8.45075E-03\n",
      "Epoch 200/250  - Train Loss: 8.12776E-03  - Val Loss: 8.36888E-03\n",
      "Best val loss: 8.34570E-03\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run2_critHuber.pth\n",
      "Run 3 of 5\n",
      "Early stopping at epoch 40\n",
      "Best val loss: 8.44709E-03\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run3_critHuber.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 46\n",
      "Best val loss: 8.42968E-03\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run4_critHuber.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05910E-02  - Val Loss: 8.62498E-03\n",
      "Epoch 200/250  - Train Loss: 9.34249E-03  - Val Loss: 8.40055E-03\n",
      "Best val loss: 8.33851E-03\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run5_critHuber.pth\n",
      "Training with criterion HuberLoss() took 282.52 seconds.\n",
      "Average time per epoch: 0.43460677995455715 seconds.\n"
     ]
    }
   ],
   "source": [
    "for crit, crit_fun in criterion_space.items():\n",
    "    run_time = []\n",
    "    start_time = time.time()\n",
    "    print(f\"\"\"Training model for year '{year}...: \n",
    "            criterion       ={crit}\n",
    "            lambda_l1       ={lambda_l1:.0e}\n",
    "            lambda_l2       ={lambda_l2:.0e}\n",
    "            dropout         ={dropout:.0e}\n",
    "            learning_rate   ={learning_rate:.0e}\n",
    "            hidden_depth    ={hidden_depth}\n",
    "            hidden_width    ={hidden_width}\"\"\")\n",
    "    for run in range(n_runs):\n",
    "        print(f\"Run {run+1} of {n_runs}\")\n",
    "        seed = 42+run\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        # Initialize the model\n",
    "        input_dim = X_train[year].shape[1]\n",
    "        name = f'l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}_crit{crit}'\n",
    "        models_21[name] = MLPModel(input_dim, depth=hidden_depth, width=hidden_width, dropout=dropout, activation=activation_fun).to(device)\n",
    "        optimizer = torch.optim.Adam(models_21[name].parameters(), lr=learning_rate)\n",
    "        train = MLPdataset(X_train[year], y_train[year])\n",
    "        val = MLPdataset(X_val[year], y_val[year])\n",
    "        best_models_activ[name], history_activ[name], time_ = train_mlp(train,          \n",
    "                                        val,\n",
    "                                        models_21[name],\n",
    "                                        crit_fun,\n",
    "                                        epochs,\n",
    "                                        patience,\n",
    "                                        print_freq,\n",
    "                                        device,\n",
    "                                        optimizer,\n",
    "                                        lambda_l1=lambda_l1,\n",
    "                                        lambda_l2=lambda_l2,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle_train=True,\n",
    "                                        shuffle_val=False,\n",
    "                                        save_path=f'models/hyperparam_test/mlp_y{year}_l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}_crit{crit}.pth',\n",
    "                                        timing = True\n",
    "                                        )\n",
    "        run_time.append(time_)\n",
    "    end_time = time.time()\n",
    "    print(f\"Training with criterion {crit_fun} took {end_time - start_time:.2f} seconds.\")\n",
    "    print(f\"Average time per epoch: {np.concatenate(run_time).mean()} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten_history_crit(history_crit, \n",
    "#                 save_csv='models/hyperparam_test/history/history_crit.csv')\n",
    "# # gc.collect()\n",
    "# # torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_crit = pd.read_csv('models/hyperparam_test/history/history_crit.csv')\n",
    "\n",
    "# criterions = sorted(df_crit['crit_name'].unique())\n",
    "# n_crits = len(criterions)+1\n",
    "# cmap = plt.get_cmap('viridis', n_crits)\n",
    "# color_map = {crit: cmap(i) for i, crit in enumerate(criterions)}\n",
    "\n",
    "# # create 2x2 grid\n",
    "# fig, axes = plt.subplots(2, 2, figsize=(12, 8), sharex=True, sharey=True)\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# # plot\n",
    "# for i, crit in enumerate(criterions):\n",
    "#     ax = axes[i]\n",
    "#     df_crit = df_crit[df_crit['crit_name'] == crit]\n",
    "#     # Plot each run for this criterion\n",
    "#     for run_id, grp in df_crit.groupby('run'):\n",
    "#         ax.plot(\n",
    "#             grp['epoch'],\n",
    "#             grp['val_loss'],\n",
    "#             color=color_map[crit],\n",
    "#             alpha=0.5,\n",
    "#             linewidth=2,\n",
    "#         )\n",
    "#     ax.set_title(crit)\n",
    "#     # show x-label on bottom row \n",
    "#     if i // 2 == 1:\n",
    "#         ax.set_xlabel('Epoch', fontsize=12)\n",
    "#     else:\n",
    "#         ax.tick_params(labelbottom=False)\n",
    "#     # show y-label on left column\n",
    "#     if i % 2 == 0:\n",
    "#         ax.set_ylabel('Validation Loss', fontsize=12)\n",
    "#     else:\n",
    "#         ax.tick_params(labelleft=False)\n",
    "#     ax.set_ylim(0.019, 0.03)\n",
    "\n",
    "# # remove any unused subplots\n",
    "# for j in range(len(criterions), len(axes)):\n",
    "#     fig.delaxes(axes[j])\n",
    "\n",
    "# fig.tight_layout()\n",
    "# fig.savefig('figs/criterion_fun_history.png', dpi=300, bbox_inches='tight')\n",
    "# # plt.show()\n",
    "# plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models for criterion MSE...\n",
      "Loading models for criterion MAE...\n",
      "Loading models for criterion Huber...\n"
     ]
    }
   ],
   "source": [
    "# load the best model for each criterion function for each run\n",
    "preds_crit = {}\n",
    "\n",
    "for crit_name, crit_fun in criterion_space.items():\n",
    "    print(f\"Loading models for criterion {crit_name}...\")\n",
    "    all_preds = []\n",
    "    for run in range(n_runs):\n",
    "        best_models_crit[f'crit{crit_name}_run{run+1}'] = load_model(\n",
    "            hidden_width, hidden_depth, run, \n",
    "            lambda_l1, lambda_l2, dropout, learning_rate, crit=crit_name\n",
    "        )\n",
    "        preds = predict_mlp(\n",
    "            best_models_crit[f'crit{crit_name}_run{run+1}'], \n",
    "            X_val[year], \n",
    "            y_val[year], \n",
    "            scaler = None,\n",
    "            batch_size=batch_size,\n",
    "            device=device,\n",
    "        )\n",
    "        all_preds.append(preds)\n",
    "    np.stack(all_preds, axis=0)\n",
    "    preds_crit[crit_name] = np.mean(all_preds, axis=0)\n",
    "\n",
    "df_preds_crit = pd.DataFrame(preds_crit)\n",
    "df_preds_crit['y_true'] = y_val[year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lccc}\n",
      "\\hline\\hline \\\\ [-1.8ex]\n",
      " & MSE & MAE & Huber \\\\ \n",
      " \\hline \n",
      "RMSE & 0.12952 & 0.13075 & 0.12942 \\\\ \n",
      "MAE & 0.08414 & 0.08364 & 0.08410 \\\\ \n",
      "AMADL & 0.00717 & 0.00786 & 0.00714 \\\\ \n",
      "\\hline\\hline\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "metrics_full = {}\n",
    "results_full = {}\n",
    "for crit_name in criterion_space.keys():\n",
    "    y_true = df_preds_crit['y_true'].values\n",
    "    y_pred = df_preds_crit[crit_name].values\n",
    "    results_full[crit_name] = {\n",
    "        'RMSE': rmse_fun(y_true, y_pred),\n",
    "        'MAE': mae_fun(y_true, y_pred),\n",
    "        'AMADL': amadl_fun(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "for metric in ['RMSE', 'MAE', 'AMADL']:\n",
    "        vals = [\n",
    "        results_full['MSE'][metric],\n",
    "        results_full['MAE'][metric],\n",
    "        results_full['Huber'][metric],\n",
    "        ]\n",
    "        metrics_full[metric] = vals\n",
    "\n",
    "tab_crit = latex_table(list(criterion_space.keys()),metrics_full)\n",
    "with open('tabs/crit_fun_perf.tex', 'w') as f:\n",
    "    f.write(tab_crit)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

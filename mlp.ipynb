{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Notebook for hyperparameter tuning of the MLP model](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Notebook for hyperparameter tuning of the MLP model](#toc1_)    \n",
    "- [Import libraries](#toc2_)    \n",
    "- [Import data](#toc3_)    \n",
    "- [Prepare data for training](#toc4_)    \n",
    "- [Test hyperparameters](#toc5_)    \n",
    "  - [Constant scheme](#toc5_1_)    \n",
    "  - [Pyramid scheme](#toc5_2_)    \n",
    "    - [Depth and width comparison](#toc5_2_1_)    \n",
    "    - [Model convergence](#toc5_2_2_)    \n",
    "  - [Regularization strength](#toc5_3_)    \n",
    "  - [Pyramid](#toc5_4_)    \n",
    "- [Test of activation functions](#toc6_)    \n",
    "- [Test of criterion functions](#toc7_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Import libraries](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "import matplotlib.gridspec as gridspec\n",
    "import math\n",
    "import ast\n",
    "import gc\n",
    "import time\n",
    "from scipy.stats import t as student_t\n",
    "\n",
    "from libs.models import *\n",
    "from libs.functions import *\n",
    "\n",
    "plt.rcParams.update({'font.size': 12, 'figure.figsize': (10, 4), 'figure.dpi': 300})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Import data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df = pd.read_csv('data/data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Prepare data for training](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare expanding window splits\n",
    "periods = {\n",
    "    '21' : '2019-12-31', # 2021 is the test set\n",
    "    # '22' : '2020-12-31', # 2022 is the test set\n",
    "    # '23' : '2021-12-31', # 2023 is the test set\n",
    "    # '24': '2022-12-31' # 2024 is the test set\n",
    "}\n",
    "\n",
    "# identify dummy vs. numeric columns\n",
    "feature_cols = [col for col in df.columns if col not in ['timestamp', 'ticker', 'target']]\n",
    "nace_cols = [c for c in feature_cols if c.startswith('NACE_')]\n",
    "dummy_cols = ['divi','divo'] # sin removed\n",
    "macro_cols = ['discount', 'tms', 'dp', 'ep', 'svar'] # 'bm_macro'\n",
    "\n",
    "# nummeric cols = cols not in cat and macro cols\n",
    "numeric_cols = [c for c in feature_cols if c not in dummy_cols and c not in nace_cols and c not in macro_cols]\n",
    "\n",
    "df_raw = df.copy(deep=True)\n",
    "df_raw['timestamp'] = pd.to_datetime(df_raw['timestamp'])\n",
    "\n",
    "# drop data from 2025\n",
    "df_raw = df_raw[df_raw['timestamp'] < '2025-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create interaction features between numeric and macro features\n",
    "C = df[numeric_cols].values         # shape = (n_rows, P_c)\n",
    "X = df[macro_cols].values           # shape = (n_rows, P_x)\n",
    "\n",
    "# compute all pairwise products with broadcasting:\n",
    "K = C[:,:,None] * X[:,None,:]\n",
    "\n",
    "# reshape to (n_rows, P_c * P_x)\n",
    "Z = K.reshape(len(df), -1)\n",
    "\n",
    "# build the column names in the same order\n",
    "xc_names = [\n",
    "    f\"{c}_x_{m}\"\n",
    "    for c in numeric_cols\n",
    "    for m in macro_cols\n",
    "]\n",
    "\n",
    "# wrap back into a DataFrame\n",
    "df_xc = pd.DataFrame(Z, columns=xc_names, index=df.index)\n",
    "\n",
    "feature_cols = numeric_cols + xc_names + dummy_cols + nace_cols\n",
    "numeric_cols = numeric_cols + xc_names\n",
    "cat_cols = dummy_cols + nace_cols\n",
    "df_z = df_raw.merge(df_xc, left_index=True, right_index=True)\n",
    "# drop macro_cols\n",
    "df_z = df_z.drop(columns=macro_cols)\n",
    "# sort columns by feature_cols\n",
    "df_norm = df_z[['timestamp', 'ticker', 'target'] + feature_cols]\n",
    "\n",
    "y_values = df_norm['target'].values.astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare containers\n",
    "X_train, X_val, X_test = {}, {}, {}\n",
    "y_train, y_val, y_test = {}, {}, {}\n",
    "preprocessors = {}\n",
    "\n",
    "for y, period in periods.items():\n",
    "    period = pd.to_datetime(period)\n",
    "\n",
    "    # split masks\n",
    "    tr_mask = df_norm['timestamp'] < period\n",
    "    va_mask = (df_norm['timestamp'] >= period) & \\\n",
    "              (df_norm['timestamp'] - pd.DateOffset(years=1) < period)\n",
    "    te_mask = (df_norm['timestamp'] - pd.DateOffset(years=1) >= period) & \\\n",
    "              (df_norm['timestamp'] - pd.DateOffset(years=2) < period)\n",
    "\n",
    "    # extract raw feature DataFrames\n",
    "    X_tr_df = df_norm.loc[tr_mask, feature_cols].copy()\n",
    "    X_va_df = df_norm.loc[va_mask, feature_cols].copy()\n",
    "    X_te_df = df_norm.loc[te_mask, feature_cols].copy()\n",
    "    y_tr    = y_values[tr_mask]\n",
    "    y_va    = y_values[va_mask]\n",
    "    y_te    = y_values[te_mask]\n",
    "\n",
    "    # compute winsorization bounds on train\n",
    "    lower = X_tr_df[numeric_cols].quantile(0.01)\n",
    "    upper = X_tr_df[numeric_cols].quantile(0.99)\n",
    "\n",
    "    # apply clipping to train, val, test\n",
    "    X_tr_df[numeric_cols] = X_tr_df[numeric_cols].clip(lower=lower, upper=upper, axis=1)\n",
    "    X_va_df[numeric_cols] = X_va_df[numeric_cols].clip(lower=lower, upper=upper, axis=1)\n",
    "    X_te_df[numeric_cols] = X_te_df[numeric_cols].clip(lower=lower, upper=upper, axis=1)\n",
    "\n",
    "    # now fit scaler on numeric only\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', StandardScaler(), numeric_cols),\n",
    "        ('cat', 'passthrough',  cat_cols)\n",
    "    ])\n",
    "    preprocessor.fit(X_tr_df)\n",
    "    preprocessors[y] = preprocessor\n",
    "\n",
    "    # transform all splits\n",
    "    X_train[y] = preprocessor.transform(X_tr_df).astype('float32')\n",
    "    X_val[y]   = preprocessor.transform(X_va_df).astype('float32')\n",
    "    X_test[y]  = preprocessor.transform(X_te_df).astype('float32')\n",
    "\n",
    "    # store targets as before\n",
    "    y_train[y] = y_tr.reshape(-1, 1)\n",
    "    y_val[y]   = y_va.reshape(-1, 1)\n",
    "    y_test[y]  = y_te.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# moving to metal or CUDA GPU if available\n",
    "device = torch.device((\"cuda\" if torch.cuda.is_available() \n",
    "                       else \"mps\" if torch.backends.mps.is_available() \n",
    "                       else \"cpu\"))\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# general hyperparameters\n",
    "hidden_depth = None \n",
    "hidden_width = None \n",
    "learning_rate = 1e-4 \n",
    "activation_fun = nn.ReLU # nn.ReLU, nn.Tanh, nn.Sigmoid, nn.LeakyReLU\n",
    "\n",
    "# general critereon and regularization parameters\n",
    "criterion = nn.MSELoss()\n",
    "lambda_l1 = 1e-5 # baseline l1 regularization\n",
    "lambda_l2 = 1e-4 # baseline l2 regularization\n",
    "drop = 0.0\n",
    "\n",
    "# general parmeters\n",
    "patience = 25\n",
    "print_freq = 100\n",
    "epochs = 250\n",
    "batch_size = 4096 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = '21'\n",
    "models_21 = {}\n",
    "n_runs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model loader fun\n",
    "def load_model(w, d, run, lambda_l1, lambda_l2, drop, lr, activ = None, crit = None):\n",
    "    m = MLPModel(\n",
    "        input_dim  = X_train[year].shape[1],\n",
    "        depth      = d,\n",
    "        width      = w,\n",
    "        dropout    = drop,\n",
    "        activation = activation_fun,\n",
    "    ).to(device)\n",
    "    path = (\n",
    "        f\"models/hyperparam_test/mlp_y{year}\"\n",
    "        f\"_l1{lambda_l1}_l2{lambda_l2}\"\n",
    "        f\"_drop{drop}_lr{lr}\"\n",
    "        f\"_w{w}_d{d}_run{run+1}.pth\"\n",
    "    )\n",
    "    if activ is not None:\n",
    "        path = path.replace('.pth', f'_activ{activ}.pth')\n",
    "    if crit is not None:\n",
    "        path = path.replace('.pth', f'_crit{crit}.pth')\n",
    "    m.load_state_dict(torch.load(path, map_location=device))\n",
    "    m.eval()\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Test hyperparameters](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_1_'></a>[Constant scheme](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training model with most data on multiple parameters\n",
    "# l1_space = [lambda_l1]\n",
    "# l2_space = [lambda_l2]\n",
    "# dropout_space = [drop]\n",
    "# learning_rate_space = [learning_rate]\n",
    "# depth_space = [1, 2, 3, 4, 5, 6, 7]\n",
    "# width_space = [8, 16, 32, 64, 128]\n",
    "# best_models_size = {}\n",
    "# history_size = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # current runtime: 2h 25m at 5 runs per model\n",
    "# for lambda_l1 in l1_space:\n",
    "#     for lambda_l2 in l2_space:\n",
    "#         for dropout in dropout_space:\n",
    "#             for learning_rate in learning_rate_space:\n",
    "#                 for hidden_depth in depth_space:\n",
    "#                     for hidden_width in width_space:\n",
    "#                         print(f\"\"\"Training model for year '{year}...: \n",
    "#                                 lambda_l1       ={lambda_l1:.0e}\n",
    "#                                 lambda_l2       ={lambda_l2:.0e}\n",
    "#                                 dropout         ={dropout:.0e}\n",
    "#                                 learning_rate   ={learning_rate:.0e}\n",
    "#                                 hidden_depth    ={hidden_depth}\n",
    "#                                 hidden_width    ={hidden_width}\"\"\")\n",
    "#                         for run in range(n_runs):\n",
    "#                             print(f\"Run {run+1} of {n_runs}\")\n",
    "#                             seed = 42+run   \n",
    "#                             np.random.seed(seed)\n",
    "#                             torch.manual_seed(seed)\n",
    "#                             # Initialize the model\n",
    "#                             input_dim = X_train[year].shape[1]\n",
    "#                             name = f'l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}'\n",
    "#                             models_21[name] = MLPModel(input_dim, depth=hidden_depth, width=hidden_width, dropout=dropout, activation=activation_fun).to(device)\n",
    "#                             optimizer = torch.optim.Adam(models_21[name].parameters(), lr=learning_rate)\n",
    "#                             train = MLPdataset(X_train[year], y_train[year])\n",
    "#                             val = MLPdataset(X_val[year], y_val[year])\n",
    "#                             best_models_size[name], history_size[name] = train_mlp(train,          \n",
    "#                                                             val,\n",
    "#                                                             models_21[name],\n",
    "#                                                             criterion,\n",
    "#                                                             epochs,\n",
    "#                                                             patience,\n",
    "#                                                             print_freq,\n",
    "#                                                             device,\n",
    "#                                                             optimizer,\n",
    "#                                                             lambda_l1=lambda_l1,\n",
    "#                                                             lambda_l2=lambda_l2,\n",
    "#                                                             batch_size=batch_size,\n",
    "#                                                             shuffle_train=True,\n",
    "#                                                             shuffle_val=False,\n",
    "#                                                             save_path=f'models/hyperparam_test/mlp_y{year}_l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}.pth'\n",
    "#                                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten_history(history_size, \n",
    "#                 save_csv='models/hyperparam_test/history/history_size.csv')\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_2_'></a>[Pyramid scheme](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training model with most data on multiple parameters\n",
    "# l1_space = [lambda_l1]\n",
    "# l2_space = [lambda_l2]\n",
    "# dropout_space = [drop]\n",
    "# learning_rate_space = [learning_rate]\n",
    "# depth_space = None\n",
    "# width_space = [[32], \n",
    "#                [32, 16], \n",
    "#                [32, 16, 8], \n",
    "#                [32, 16, 8, 4], \n",
    "#                [32, 16, 8, 4, 2]]\n",
    "# best_models_pyramid = {}\n",
    "# history_pyramid = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # current runtime: 30m\n",
    "# for lambda_l1 in l1_space:\n",
    "#     for lambda_l2 in l2_space:\n",
    "#         for dropout in dropout_space:\n",
    "#             for learning_rate in learning_rate_space:\n",
    "#                 for hidden_width in width_space:\n",
    "#                     hidden_depth = len(hidden_width)\n",
    "#                     print(f\"\"\"Training model for year '{year}...: \n",
    "#                             lambda_l1       ={lambda_l1:.0e}\n",
    "#                             lambda_l2       ={lambda_l2:.0e}\n",
    "#                             dropout         ={dropout:.0e}\n",
    "#                             learning_rate   ={learning_rate:.0e}\n",
    "#                             hidden_depth    ={hidden_depth}\n",
    "#                             hidden_width    ={hidden_width}\"\"\")\n",
    "#                     for run in range(n_runs):\n",
    "#                         print(f\"Run {run+1} of {n_runs}\")\n",
    "#                         seed = 42+run\n",
    "#                         np.random.seed(seed)\n",
    "#                         torch.manual_seed(seed)\n",
    "#                         # Initialize the model\n",
    "#                         input_dim = X_train[year].shape[1]\n",
    "#                         name = f'l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}'\n",
    "#                         models_21[name] = MLPModel(input_dim, depth=hidden_depth, width=hidden_width, dropout=dropout, activation=activation_fun).to(device)\n",
    "#                         optimizer = torch.optim.Adam(models_21[name].parameters(), lr=learning_rate)\n",
    "#                         train = MLPdataset(X_train[year], y_train[year])\n",
    "#                         val = MLPdataset(X_val[year], y_val[year])\n",
    "#                         best_models_pyramid[name], history_pyramid[name] = train_mlp(train,          \n",
    "#                                                         val,\n",
    "#                                                         models_21[name],\n",
    "#                                                         criterion,\n",
    "#                                                         epochs,\n",
    "#                                                         patience,\n",
    "#                                                         print_freq,\n",
    "#                                                         device,\n",
    "#                                                         optimizer,\n",
    "#                                                         lambda_l1=lambda_l1,\n",
    "#                                                         lambda_l2=lambda_l2,\n",
    "#                                                         batch_size=batch_size,\n",
    "#                                                         shuffle_train=True,\n",
    "#                                                         shuffle_val=False,\n",
    "#                                                         save_path=f'models/hyperparam_test/mlp_y{year}_l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}.pth'\n",
    "#                                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten_history(history_pyramid, \n",
    "#                 save_csv='models/hyperparam_test/history/history_pyramid.csv')\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_2_1_'></a>[Depth and width comparison](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # constant-width grid\n",
    "# const_widths = [8, 16, 32, 64, 128]\n",
    "# const_depths = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "# # pyramid sequences (each column of the right heatmap)\n",
    "# pyr_seqs = [\n",
    "#     [32],\n",
    "#     [32,16],\n",
    "#     [32,16,8],\n",
    "#     [32,16,8,4],\n",
    "#     [32,16,8,4,2],\n",
    "# ]\n",
    "# Wc = len(const_widths)\n",
    "# Dc = len(const_depths)\n",
    "# Wp = len(pyr_seqs)\n",
    "\n",
    "# # DataLoader\n",
    "# val_ds     = MLPdataset(X_val[year], y_val[year])\n",
    "# val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # constant-width loss matrix\n",
    "# loss_const = np.zeros((Dc, Wc))\n",
    "# for di, d in enumerate(const_depths):\n",
    "#     for wj, w in enumerate(const_widths):\n",
    "#         run_losses = []\n",
    "#         for run in range(n_runs):\n",
    "#             m = load_model(w, d, run, lambda_l1, lambda_l2, drop, learning_rate)\n",
    "#             with torch.no_grad():\n",
    "#                 batch = [criterion(m(x.to(device)), y.to(device)).item()\n",
    "#                          for x,y in val_loader]\n",
    "#             run_losses.append(np.mean(batch))\n",
    "#         loss_const[di, wj] = np.mean(run_losses)\n",
    "\n",
    "# # pyramid loss matrix\n",
    "# loss_pyr = np.full((Wp, Wp), np.nan)\n",
    "# for sj, seq in enumerate(pyr_seqs):\n",
    "#     depth = len(seq)\n",
    "#     run_losses = []\n",
    "#     for run in range(n_runs):\n",
    "#         # we “flatten” seq into width param for loader\n",
    "#         m = load_model(seq, depth, run, lambda_l1, lambda_l2, drop, learning_rate)\n",
    "#         with torch.no_grad():\n",
    "#             batch = [criterion(m(x.to(device)), y.to(device)).item()\n",
    "#                      for x,y in val_loader]\n",
    "#         run_losses.append(np.mean(batch))\n",
    "#     loss_pyr[depth-1, sj] = np.mean(run_losses)\n",
    "\n",
    "# # plot\n",
    "# fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(12,5),\n",
    "#                                gridspec_kw={'width_ratios':[4,1]})\n",
    "\n",
    "# # color‐scale\n",
    "# all_vals = loss_const.ravel()\n",
    "# vmin    = all_vals.min()-0.005\n",
    "# vmax    = all_vals.max()+0.005\n",
    "# vcenter = vmin + 0.5*(vmax - vmin)\n",
    "# norm    = TwoSlopeNorm(vmin=vmin, vcenter=vcenter, vmax=vmax)\n",
    "# cmap    = 'viridis'\n",
    "\n",
    "# # left: constant‐width heatmap\n",
    "# im0 = ax0.imshow(loss_const, aspect='auto', cmap=cmap, norm=norm)\n",
    "# im0 = ax0.imshow(loss_const, aspect='auto', cmap=cmap, norm=norm)\n",
    "# ax0.set_xticks(range(Wc))\n",
    "# ax0.set_xticklabels(const_widths)\n",
    "# ax0.set_yticks(range(Dc))\n",
    "# ax0.set_yticklabels(const_depths)\n",
    "# ax0.set_xlabel('Hidden Width')\n",
    "# ax0.set_ylabel('Hidden Depth')\n",
    "# ax0.set_title(f'Constant Width')\n",
    "\n",
    "# # right: collapsed‐x pyramid heatmap\n",
    "# diag_pyr = np.diag(loss_pyr)\n",
    "# diag_mat = diag_pyr[:, np.newaxis]\n",
    "\n",
    "# im1 = ax1.imshow(diag_mat, aspect='auto', cmap=cmap, norm=norm)\n",
    "# ax1.set_xticks([0])\n",
    "# ax1.set_xticklabels(['Geometric\\npyramid'])\n",
    "# ax1.set_yticks(np.arange(Wp))\n",
    "# ax1.set_yticklabels(np.arange(1, Wp+1))\n",
    "# ax1.set_ylabel('Hidden Depth')\n",
    "# ax1.set_title('Pyramid Scheme')\n",
    "\n",
    "# # shared colorbar\n",
    "# cbar = fig.colorbar(im0, ax=[ax0,ax1], shrink=0.9, pad=0.02)\n",
    "# cbar.set_label('Average Validation Loss (MSE)')\n",
    "\n",
    "# plt.savefig('figs/width_depth.png', dpi=300, bbox_inches='tight')\n",
    "# # plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pre‐compute global vmin/vmax across all runs so color‐scale is consistent\n",
    "# all_vals = []\n",
    "# for run in range(n_runs):\n",
    "#     # constant‐width for this run\n",
    "#     tmp = []\n",
    "#     for d in const_depths:\n",
    "#         for w in const_widths:\n",
    "#             m = load_model(w, d, run, lambda_l1, lambda_l2, drop, learning_rate)\n",
    "#             with torch.no_grad():\n",
    "#                 batch = [criterion(m(x.to(device)), y.to(device)).item()\n",
    "#                          for x,y in val_loader]\n",
    "#             tmp.append(np.mean(batch))\n",
    "#     all_vals.extend(tmp)\n",
    "\n",
    "#     # pyramid diag for this run\n",
    "#     diag = []\n",
    "#     for seq in pyr_seqs:\n",
    "#         depth = len(seq)\n",
    "#         m = load_model(seq, depth, run, lambda_l1, lambda_l2, drop, learning_rate)\n",
    "#         with torch.no_grad():\n",
    "#             batch = [criterion(m(x.to(device)), y.to(device)).item()\n",
    "#                      for x,y in val_loader]\n",
    "#         diag.append(np.mean(batch))\n",
    "#     all_vals.extend(diag)\n",
    "\n",
    "# runs = list(range(n_runs))\n",
    "# # split into pages of 4 runs each\n",
    "# pages = [runs[i:i+4] for i in range(0, len(runs), 4)]\n",
    "\n",
    "# for p, page_runs in enumerate(pages, start=1):\n",
    "#     nrows = len(page_runs)\n",
    "#     fig, axes = plt.subplots(\n",
    "#         nrows, 2,\n",
    "#         figsize=(10, 3*nrows),\n",
    "#         gridspec_kw={'width_ratios': [4, 1]},\n",
    "#         constrained_layout=False  # turn off for manual colorbar placement\n",
    "#     )\n",
    "\n",
    "#     # if only one row, wrap axes\n",
    "#     if nrows == 1:\n",
    "#         axes = np.expand_dims(axes, 0)\n",
    "\n",
    "#     for i, run in enumerate(page_runs):\n",
    "#         ax0, ax1 = axes[i]\n",
    "\n",
    "#         # ---- left: constant-width heatmap for this run ----\n",
    "#         loss_const = np.zeros((Dc, Wc))\n",
    "#         for di, d in enumerate(const_depths):\n",
    "#             for wj, w in enumerate(const_widths):\n",
    "#                 m = load_model(w, d, run, lambda_l1, lambda_l2, drop, learning_rate)\n",
    "#                 with torch.no_grad():\n",
    "#                     batch = [criterion(m(x.to(device)), y.to(device)).item()\n",
    "#                              for x,y in val_loader]\n",
    "#                 loss_const[di, wj] = np.mean(batch)\n",
    "\n",
    "#         im0 = ax0.imshow(loss_const, aspect='auto', cmap=cmap, norm=norm)\n",
    "#         if i == 0:\n",
    "#             ax0.set_title('Constant Width')\n",
    "#         ax0.set_ylabel(f'Run {run+1}\\nHidden Depth')\n",
    "#         ax0.set_xticks(range(Wc))\n",
    "#         ax0.set_xticklabels(const_widths)\n",
    "#         ax0.set_yticks(range(Dc))\n",
    "#         ax0.set_yticklabels(const_depths)\n",
    "#         if i == nrows - 1:\n",
    "#             ax0.set_xlabel('Hidden Width')\n",
    "#         else:\n",
    "#             ax0.set_xlabel('')\n",
    "#             ax0.tick_params(labelbottom=False)\n",
    "\n",
    "#         # ---- right: collapsed pyramid for this run ----\n",
    "#         diag = []\n",
    "#         for seq in pyr_seqs:\n",
    "#             depth = len(seq)\n",
    "#             m = load_model(seq, depth, run, lambda_l1, lambda_l2, drop, learning_rate)\n",
    "#             with torch.no_grad():\n",
    "#                 batch = [criterion(m(x.to(device)), y.to(device)).item()\n",
    "#                          for x,y in val_loader]\n",
    "#             diag.append(np.mean(batch))\n",
    "#         diag_mat = np.array(diag)[:, None]\n",
    "\n",
    "#         im1 = ax1.imshow(diag_mat, aspect='auto', cmap=cmap, norm=norm)\n",
    "#         if i == 0:\n",
    "#             ax1.set_title('Pyramid Scheme')\n",
    "#         ax1.set_xticks([0])\n",
    "#         # only label the bottom subplot\n",
    "#         if i == nrows - 1:\n",
    "#             ax1.set_xticklabels(['Geometric\\npyramid'])\n",
    "#         else:\n",
    "#             ax1.set_xticklabels([])\n",
    "#         ax1.set_yticks(range(Wp))\n",
    "#         ax1.set_yticklabels(range(1, Wp+1))\n",
    "#         ax1.set_ylabel('Hidden Depth')\n",
    "\n",
    "#     # shared colorbar on the right of this page\n",
    "#     cax = fig.add_axes([0.92,  # 92% from left\n",
    "#                         0.11,  # 10% from bottom\n",
    "#                         0.02,  # 2% figure‐width\n",
    "#                         0.77   # 80% figure‐height\n",
    "#                        ])\n",
    "#     cbar = fig.colorbar(im0, cax=cax)\n",
    "#     cbar.set_label('Validation Loss (MSE)')\n",
    "\n",
    "#     fig.savefig(f'figs/width_depth_page{p}.png', dpi=300, bbox_inches='tight')\n",
    "#     # plt.show()\n",
    "#     plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_2_2_'></a>[Model convergence](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CSVs\n",
    "# df_size = pd.read_csv('models/hyperparam_test/history/history_size.csv')\n",
    "# df_pyr  = pd.read_csv('models/hyperparam_test/history/history_pyramid.csv')\n",
    "\n",
    "# # extract name\n",
    "# df_size['widths'] = df_size['widths'].apply(ast.literal_eval)\n",
    "# df_pyr ['widths'] = df_pyr ['widths'].apply(ast.literal_eval)\n",
    "\n",
    "# df_size['width'] = df_size['widths'].apply(lambda w: w[0])\n",
    "# df_pyr ['width'] = df_pyr ['widths'].apply(lambda w: w[0])\n",
    "\n",
    "# df_size['name'] = df_size['width'].apply(\n",
    "#     lambda w: f\"Constant width = {w}\"\n",
    "# )\n",
    "\n",
    "# width_to_str = (\n",
    "#     df_pyr\n",
    "#     .groupby('width')['widths_str']\n",
    "#     .agg(lambda ss: max(ss, key=len))\n",
    "#     .to_dict()\n",
    "# )\n",
    "# df_pyr['name'] = df_pyr['width'].map(\n",
    "#     lambda w: f\"Pyramid width = {width_to_str[w]}\"\n",
    "# )\n",
    "\n",
    "# # combine\n",
    "# df = pd.concat([df_size, df_pyr], ignore_index=True)\n",
    "\n",
    "# # sort by width\n",
    "# panel_df = (\n",
    "#     df[['name','width']]            # pick only the two columns we care about\n",
    "#       .drop_duplicates()            # one row per unique panel\n",
    "#       .assign(is_pyr=lambda x: x['name'].str.startswith('Pyramid'))\n",
    "#       .sort_values(['is_pyr','width'])  # constants (is_pyr=False) first, then pyramids; each by ascending width\n",
    "# )\n",
    "\n",
    "# plot_names = panel_df['name'].tolist()\n",
    "# n = len(plot_names)\n",
    "# n_cols = min(3, n)\n",
    "# n_rows = math.ceil(n / n_cols)\n",
    "\n",
    "# fig, axes = plt.subplots(n_rows, n_cols,\n",
    "#                          figsize=(4*n_cols, 4*n_rows),\n",
    "#                          sharey=True)\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# # build a unified color map over depths\n",
    "# depths = sorted(df['depth'].unique())\n",
    "# cmap   = plt.get_cmap('viridis')\n",
    "# colors = {d: cmap(i/(len(depths)-1)) for i, d in enumerate(depths)}\n",
    "\n",
    "# for idx, name in enumerate(plot_names):\n",
    "#     ax = axes[idx]\n",
    "#     sub_df = df[df['name'] == name]\n",
    "#     for d in depths:\n",
    "#         sd = sub_df[sub_df['depth'] == d]\n",
    "#         for run in sd['run'].unique():\n",
    "#             run_df = sd[sd['run'] == run].sort_values('epoch')\n",
    "#             ax.plot(\n",
    "#                 run_df['epoch'],\n",
    "#                 run_df['val_loss'],\n",
    "#                 color=colors[d],\n",
    "#                 alpha=0.5,\n",
    "#                 linewidth=2,\n",
    "#                 label=(f\"d={d}\" if run == sd['run'].min() else None)\n",
    "#             )\n",
    "\n",
    "#     ax.set_title(name)\n",
    "#     # ax.set_yscale('log')\n",
    "#     ax.set_ylim(1.1, 1.3)\n",
    "#     ax.set_xlim(0, 250)\n",
    "#     ax.set_xticks(np.linspace(0, 250, 6, dtype=int))\n",
    "\n",
    "#     # only bottom row: x-label\n",
    "#     if idx // n_cols == n_rows - 1:\n",
    "#         ax.set_xlabel(\"Epoch\", fontsize=12)\n",
    "#     else:\n",
    "#         ax.set_xticklabels([])\n",
    "\n",
    "#     # only leftmost column: y-label\n",
    "#     if idx % n_cols == 0:\n",
    "#         ax.set_ylabel(\"Validation Loss\", fontsize=12)\n",
    "\n",
    "# # remove any unused axes\n",
    "# for j in range(len(plot_names), len(axes)):\n",
    "#     fig.delaxes(axes[j])\n",
    "\n",
    "# # global legend on right\n",
    "# handles, labels = axes[0].get_legend_handles_labels()\n",
    "# fig.legend(handles, labels,\n",
    "#            title=\"Hidden Depth\",\n",
    "#            loc='center left',\n",
    "#            bbox_to_anchor=(1, 0.5),\n",
    "#            fontsize=12)\n",
    "\n",
    "# plt.tight_layout() \n",
    "# plt.savefig('figs/val_loss_history.png', dpi=300, bbox_inches='tight')\n",
    "# # plt.show()\n",
    "# plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_3_'></a>[Regularization strength](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training model with most data on multiple parameters\n",
    "# l1_space = [0.0, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "# l2_space = [0.0, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "# dropout_space = [0.0, 0.1, 0.2, 0.3] \n",
    "# learning_rate_space = [learning_rate] \n",
    "# depth_space = [5] \n",
    "# width_space = [16]\n",
    "# best_models_reg = {}\n",
    "# history_reg = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for year '21...: \n",
      "                                lambda_l1       =0e+00\n",
      "                                lambda_l2       =0e+00\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.30620E-01  - Val Loss: 1.11159E+00\n",
      "Early stopping at epoch 121\n",
      "Best val loss: 1.11106E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.0_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.37960E-01  - Val Loss: 1.14132E+00\n",
      "Epoch 200/250  - Train Loss: 8.93540E-01  - Val Loss: 1.11690E+00\n",
      "Best val loss: 1.11424E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.0_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.26777E-01  - Val Loss: 1.13855E+00\n",
      "Early stopping at epoch 196\n",
      "Best val loss: 1.12647E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.0_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.25328E-01  - Val Loss: 1.13389E+00\n",
      "Epoch 200/250  - Train Loss: 8.88072E-01  - Val Loss: 1.11994E+00\n",
      "Early stopping at epoch 245\n",
      "Best val loss: 1.11815E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.0_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 30\n",
      "Best val loss: 1.13526E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.0_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =0e+00\n",
      "                                lambda_l2       =0e+00\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.62949E-01  - Val Loss: 1.12559E+00\n",
      "Early stopping at epoch 156\n",
      "Best val loss: 1.12246E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.1_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.60482E-01  - Val Loss: 1.14303E+00\n",
      "Epoch 200/250  - Train Loss: 9.41870E-01  - Val Loss: 1.12745E+00\n",
      "Best val loss: 1.11948E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.1_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.54095E-01  - Val Loss: 1.13443E+00\n",
      "Epoch 200/250  - Train Loss: 9.37410E-01  - Val Loss: 1.12358E+00\n",
      "Best val loss: 1.11928E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.1_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.50944E-01  - Val Loss: 1.13518E+00\n",
      "Early stopping at epoch 172\n",
      "Best val loss: 1.13343E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.1_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 30\n",
      "Best val loss: 1.13518E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.1_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =0e+00\n",
      "                                lambda_l2       =0e+00\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.74982E-01  - Val Loss: 1.13577E+00\n",
      "Early stopping at epoch 140\n",
      "Best val loss: 1.13435E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.2_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.70054E-01  - Val Loss: 1.14173E+00\n",
      "Epoch 200/250  - Train Loss: 9.58110E-01  - Val Loss: 1.13515E+00\n",
      "Best val loss: 1.13333E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.2_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.66960E-01  - Val Loss: 1.13920E+00\n",
      "Epoch 200/250  - Train Loss: 9.57537E-01  - Val Loss: 1.13630E+00\n",
      "Early stopping at epoch 233\n",
      "Best val loss: 1.13496E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.2_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.62878E-01  - Val Loss: 1.13692E+00\n",
      "Early stopping at epoch 168\n",
      "Best val loss: 1.13454E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.2_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 31\n",
      "Best val loss: 1.13517E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.2_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =0e+00\n",
      "                                lambda_l2       =0e+00\n",
      "                                dropout         =3e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 58\n",
      "Best val loss: 1.15474E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.3_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.80410E-01  - Val Loss: 1.14408E+00\n",
      "Epoch 200/250  - Train Loss: 9.68613E-01  - Val Loss: 1.13923E+00\n",
      "Best val loss: 1.13762E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.3_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.81047E-01  - Val Loss: 1.14345E+00\n",
      "Early stopping at epoch 192\n",
      "Best val loss: 1.14112E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.3_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.74237E-01  - Val Loss: 1.14021E+00\n",
      "Epoch 200/250  - Train Loss: 9.65967E-01  - Val Loss: 1.13811E+00\n",
      "Best val loss: 1.13681E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.3_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 31\n",
      "Best val loss: 1.13521E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.3_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =0e+00\n",
      "                                lambda_l2       =1e-05\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.30993E-01  - Val Loss: 1.11076E+00\n",
      "Early stopping at epoch 117\n",
      "Best val loss: 1.11019E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.0_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.38233E-01  - Val Loss: 1.14111E+00\n",
      "Epoch 200/250  - Train Loss: 8.93347E-01  - Val Loss: 1.11926E+00\n",
      "Early stopping at epoch 215\n",
      "Best val loss: 1.11817E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.0_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.27716E-01  - Val Loss: 1.13732E+00\n",
      "Early stopping at epoch 189\n",
      "Best val loss: 1.13007E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.0_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.25127E-01  - Val Loss: 1.13277E+00\n",
      "Epoch 200/250  - Train Loss: 8.88902E-01  - Val Loss: 1.11541E+00\n",
      "Early stopping at epoch 245\n",
      "Best val loss: 1.11391E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.0_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 30\n",
      "Best val loss: 1.13525E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.0_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =0e+00\n",
      "                                lambda_l2       =1e-05\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.63077E-01  - Val Loss: 1.12509E+00\n",
      "Early stopping at epoch 150\n",
      "Best val loss: 1.12211E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.1_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.59876E-01  - Val Loss: 1.14309E+00\n",
      "Epoch 200/250  - Train Loss: 9.42016E-01  - Val Loss: 1.12783E+00\n",
      "Best val loss: 1.11974E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.1_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.54564E-01  - Val Loss: 1.13460E+00\n",
      "Epoch 200/250  - Train Loss: 9.37955E-01  - Val Loss: 1.12222E+00\n",
      "Best val loss: 1.11741E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.1_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.51274E-01  - Val Loss: 1.13522E+00\n",
      "Early stopping at epoch 127\n",
      "Best val loss: 1.13482E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.1_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 30\n",
      "Best val loss: 1.13518E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.1_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =0e+00\n",
      "                                lambda_l2       =1e-05\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.75238E-01  - Val Loss: 1.13584E+00\n",
      "Early stopping at epoch 140\n",
      "Best val loss: 1.13377E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.2_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.69948E-01  - Val Loss: 1.14158E+00\n",
      "Epoch 200/250  - Train Loss: 9.58221E-01  - Val Loss: 1.13590E+00\n",
      "Best val loss: 1.13364E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.2_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.67413E-01  - Val Loss: 1.13893E+00\n",
      "Epoch 200/250  - Train Loss: 9.57927E-01  - Val Loss: 1.13593E+00\n",
      "Early stopping at epoch 233\n",
      "Best val loss: 1.13445E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.2_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.63127E-01  - Val Loss: 1.13662E+00\n",
      "Early stopping at epoch 166\n",
      "Best val loss: 1.13432E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.2_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 31\n",
      "Best val loss: 1.13516E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.2_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =0e+00\n",
      "                                lambda_l2       =1e-05\n",
      "                                dropout         =3e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 58\n",
      "Best val loss: 1.15471E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.3_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.80530E-01  - Val Loss: 1.14399E+00\n",
      "Early stopping at epoch 181\n",
      "Best val loss: 1.13974E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.3_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.81430E-01  - Val Loss: 1.14347E+00\n",
      "Epoch 200/250  - Train Loss: 9.66539E-01  - Val Loss: 1.14199E+00\n",
      "Early stopping at epoch 207\n",
      "Best val loss: 1.14119E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.3_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.74569E-01  - Val Loss: 1.14005E+00\n",
      "Epoch 200/250  - Train Loss: 9.66401E-01  - Val Loss: 1.13813E+00\n",
      "Early stopping at epoch 213\n",
      "Best val loss: 1.13712E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.3_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 31\n",
      "Best val loss: 1.13521E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.3_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =0e+00\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.34798E-01  - Val Loss: 1.11079E+00\n",
      "Early stopping at epoch 121\n",
      "Best val loss: 1.11044E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.0_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.42542E-01  - Val Loss: 1.14205E+00\n",
      "Epoch 200/250  - Train Loss: 8.97495E-01  - Val Loss: 1.11911E+00\n",
      "Early stopping at epoch 215\n",
      "Best val loss: 1.11763E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.0_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.30471E-01  - Val Loss: 1.13531E+00\n",
      "Early stopping at epoch 189\n",
      "Best val loss: 1.12535E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.0_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.28284E-01  - Val Loss: 1.13511E+00\n",
      "Epoch 200/250  - Train Loss: 8.91124E-01  - Val Loss: 1.11372E+00\n",
      "Early stopping at epoch 237\n",
      "Best val loss: 1.11219E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.0_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 30\n",
      "Best val loss: 1.13525E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.0_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =0e+00\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.65250E-01  - Val Loss: 1.12556E+00\n",
      "Early stopping at epoch 184\n",
      "Best val loss: 1.12031E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.1_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.63049E-01  - Val Loss: 1.14311E+00\n",
      "Epoch 200/250  - Train Loss: 9.44504E-01  - Val Loss: 1.12369E+00\n",
      "Best val loss: 1.11559E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.1_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.57522E-01  - Val Loss: 1.13436E+00\n",
      "Epoch 200/250  - Train Loss: 9.41144E-01  - Val Loss: 1.12193E+00\n",
      "Best val loss: 1.11700E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.1_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.53430E-01  - Val Loss: 1.13526E+00\n",
      "Early stopping at epoch 127\n",
      "Best val loss: 1.13489E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.1_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 30\n",
      "Best val loss: 1.13517E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.1_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =0e+00\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.77530E-01  - Val Loss: 1.13576E+00\n",
      "Early stopping at epoch 140\n",
      "Best val loss: 1.13334E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.2_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.72270E-01  - Val Loss: 1.14098E+00\n",
      "Epoch 200/250  - Train Loss: 9.60516E-01  - Val Loss: 1.13475E+00\n",
      "Best val loss: 1.13203E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.2_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.70016E-01  - Val Loss: 1.13872E+00\n",
      "Epoch 200/250  - Train Loss: 9.60108E-01  - Val Loss: 1.13598E+00\n",
      "Best val loss: 1.13326E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.2_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.65311E-01  - Val Loss: 1.13698E+00\n",
      "Epoch 200/250  - Train Loss: 9.57925E-01  - Val Loss: 1.13353E+00\n",
      "Best val loss: 1.13167E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.2_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 31\n",
      "Best val loss: 1.13517E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.2_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =0e+00\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =3e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 58\n",
      "Best val loss: 1.15468E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.3_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.82448E-01  - Val Loss: 1.14383E+00\n",
      "Epoch 200/250  - Train Loss: 9.70855E-01  - Val Loss: 1.13889E+00\n",
      "Best val loss: 1.13726E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.3_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.84032E-01  - Val Loss: 1.14311E+00\n",
      "Early stopping at epoch 192\n",
      "Best val loss: 1.14071E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.3_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.76379E-01  - Val Loss: 1.14020E+00\n",
      "Epoch 200/250  - Train Loss: 9.68292E-01  - Val Loss: 1.13798E+00\n",
      "Best val loss: 1.13624E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.3_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 31\n",
      "Best val loss: 1.13520E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.3_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =0e+00\n",
      "                                lambda_l2       =1e-03\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.58454E-01  - Val Loss: 1.11375E+00\n",
      "Early stopping at epoch 140\n",
      "Best val loss: 1.10786E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.0_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.64354E-01  - Val Loss: 1.12810E+00\n",
      "Early stopping at epoch 168\n",
      "Best val loss: 1.10686E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.0_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.53909E-01  - Val Loss: 1.13002E+00\n",
      "Epoch 200/250  - Train Loss: 9.13113E-01  - Val Loss: 1.11601E+00\n",
      "Early stopping at epoch 202\n",
      "Best val loss: 1.11425E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.0_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.45627E-01  - Val Loss: 1.13178E+00\n",
      "Epoch 200/250  - Train Loss: 9.06206E-01  - Val Loss: 1.11108E+00\n",
      "Early stopping at epoch 229\n",
      "Best val loss: 1.11085E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.0_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 31\n",
      "Best val loss: 1.13519E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.0_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =0e+00\n",
      "                                lambda_l2       =1e-03\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.80671E-01  - Val Loss: 1.12398E+00\n",
      "Epoch 200/250  - Train Loss: 9.57828E-01  - Val Loss: 1.11629E+00\n",
      "Early stopping at epoch 205\n",
      "Best val loss: 1.11410E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.1_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.79883E-01  - Val Loss: 1.13903E+00\n",
      "Epoch 200/250  - Train Loss: 9.54634E-01  - Val Loss: 1.12088E+00\n",
      "Best val loss: 1.11899E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.1_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.76863E-01  - Val Loss: 1.13494E+00\n",
      "Epoch 200/250  - Train Loss: 9.57325E-01  - Val Loss: 1.12389E+00\n",
      "Best val loss: 1.11831E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.1_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.68079E-01  - Val Loss: 1.13757E+00\n",
      "Epoch 200/250  - Train Loss: 9.55008E-01  - Val Loss: 1.12531E+00\n",
      "Early stopping at epoch 248\n",
      "Best val loss: 1.12238E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.1_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 31\n",
      "Best val loss: 1.13509E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.1_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =0e+00\n",
      "                                lambda_l2       =1e-03\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.92535E-01  - Val Loss: 1.13682E+00\n",
      "Epoch 200/250  - Train Loss: 9.74713E-01  - Val Loss: 1.13213E+00\n",
      "Early stopping at epoch 211\n",
      "Best val loss: 1.13029E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.2_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.89282E-01  - Val Loss: 1.14125E+00\n",
      "Epoch 200/250  - Train Loss: 9.71079E-01  - Val Loss: 1.13183E+00\n",
      "Best val loss: 1.12792E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.2_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.88730E-01  - Val Loss: 1.13728E+00\n",
      "Epoch 200/250  - Train Loss: 9.73762E-01  - Val Loss: 1.13328E+00\n",
      "Best val loss: 1.12997E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.2_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.78643E-01  - Val Loss: 1.13991E+00\n",
      "Early stopping at epoch 108\n",
      "Best val loss: 1.13960E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.2_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 31\n",
      "Best val loss: 1.13513E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.2_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =0e+00\n",
      "                                lambda_l2       =1e-03\n",
      "                                dropout         =3e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.99033E-01  - Val Loss: 1.14511E+00\n",
      "Epoch 200/250  - Train Loss: 9.81037E-01  - Val Loss: 1.13698E+00\n",
      "Early stopping at epoch 211\n",
      "Best val loss: 1.13613E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.3_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.97689E-01  - Val Loss: 1.14447E+00\n",
      "Epoch 200/250  - Train Loss: 9.79470E-01  - Val Loss: 1.13651E+00\n",
      "Best val loss: 1.13310E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.3_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.00155E+00  - Val Loss: 1.14179E+00\n",
      "Early stopping at epoch 192\n",
      "Best val loss: 1.13748E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.3_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.88231E-01  - Val Loss: 1.14034E+00\n",
      "Early stopping at epoch 127\n",
      "Best val loss: 1.14020E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.3_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 31\n",
      "Best val loss: 1.13518E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.3_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =0e+00\n",
      "                                lambda_l2       =1e-02\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06579E+00  - Val Loss: 1.15385E+00\n",
      "Early stopping at epoch 105\n",
      "Best val loss: 1.15367E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.0_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07047E+00  - Val Loss: 1.14967E+00\n",
      "Epoch 200/250  - Train Loss: 1.01470E+00  - Val Loss: 1.14050E+00\n",
      "Best val loss: 1.13897E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.0_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07488E+00  - Val Loss: 1.14757E+00\n",
      "Epoch 200/250  - Train Loss: 1.01690E+00  - Val Loss: 1.13807E+00\n",
      "Best val loss: 1.13241E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.0_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06923E+00  - Val Loss: 1.15559E+00\n",
      "Epoch 200/250  - Train Loss: 1.01613E+00  - Val Loss: 1.15483E+00\n",
      "Early stopping at epoch 206\n",
      "Best val loss: 1.15437E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.0_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 1.13503E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.0_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =0e+00\n",
      "                                lambda_l2       =1e-02\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06582E+00  - Val Loss: 1.15374E+00\n",
      "Early stopping at epoch 105\n",
      "Best val loss: 1.15350E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.1_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07209E+00  - Val Loss: 1.15229E+00\n",
      "Epoch 200/250  - Train Loss: 1.01905E+00  - Val Loss: 1.14401E+00\n",
      "Best val loss: 1.14099E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.1_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07875E+00  - Val Loss: 1.15020E+00\n",
      "Epoch 200/250  - Train Loss: 1.02428E+00  - Val Loss: 1.14457E+00\n",
      "Best val loss: 1.14116E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.1_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06945E+00  - Val Loss: 1.15556E+00\n",
      "Epoch 200/250  - Train Loss: 1.01619E+00  - Val Loss: 1.15487E+00\n",
      "Early stopping at epoch 206\n",
      "Best val loss: 1.15440E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.1_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 1.13504E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.1_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =0e+00\n",
      "                                lambda_l2       =1e-02\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06591E+00  - Val Loss: 1.15376E+00\n",
      "Early stopping at epoch 105\n",
      "Best val loss: 1.15352E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.2_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07270E+00  - Val Loss: 1.15433E+00\n",
      "Epoch 200/250  - Train Loss: 1.02205E+00  - Val Loss: 1.15109E+00\n",
      "Best val loss: 1.14961E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.2_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.08097E+00  - Val Loss: 1.15205E+00\n",
      "Epoch 200/250  - Train Loss: 1.02736E+00  - Val Loss: 1.15015E+00\n",
      "Best val loss: 1.14852E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.2_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06963E+00  - Val Loss: 1.15556E+00\n",
      "Epoch 200/250  - Train Loss: 1.01627E+00  - Val Loss: 1.15486E+00\n",
      "Early stopping at epoch 206\n",
      "Best val loss: 1.15438E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.2_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 1.13505E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.2_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =0e+00\n",
      "                                lambda_l2       =1e-02\n",
      "                                dropout         =3e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06604E+00  - Val Loss: 1.15381E+00\n",
      "Early stopping at epoch 105\n",
      "Best val loss: 1.15361E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.3_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07347E+00  - Val Loss: 1.15595E+00\n",
      "Early stopping at epoch 141\n",
      "Best val loss: 1.15489E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.3_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.08231E+00  - Val Loss: 1.15475E+00\n",
      "Epoch 200/250  - Train Loss: 1.02632E+00  - Val Loss: 1.15382E+00\n",
      "Early stopping at epoch 201\n",
      "Best val loss: 1.15322E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.3_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06977E+00  - Val Loss: 1.15553E+00\n",
      "Epoch 200/250  - Train Loss: 1.01638E+00  - Val Loss: 1.15485E+00\n",
      "Early stopping at epoch 206\n",
      "Best val loss: 1.15438E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.3_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 1.13506E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.3_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-05\n",
      "                                lambda_l2       =0e+00\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.33257E-01  - Val Loss: 1.11148E+00\n",
      "Early stopping at epoch 117\n",
      "Best val loss: 1.11108E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.0_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.40565E-01  - Val Loss: 1.14032E+00\n",
      "Epoch 200/250  - Train Loss: 8.93814E-01  - Val Loss: 1.12400E+00\n",
      "Early stopping at epoch 215\n",
      "Best val loss: 1.12271E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.0_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.30163E-01  - Val Loss: 1.13684E+00\n",
      "Early stopping at epoch 183\n",
      "Best val loss: 1.12999E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.0_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.27402E-01  - Val Loss: 1.13645E+00\n",
      "Epoch 200/250  - Train Loss: 8.88752E-01  - Val Loss: 1.12101E+00\n",
      "Early stopping at epoch 236\n",
      "Best val loss: 1.12069E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.0_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 30\n",
      "Best val loss: 1.13527E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.0_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-05\n",
      "                                lambda_l2       =0e+00\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.65348E-01  - Val Loss: 1.12520E+00\n",
      "Early stopping at epoch 156\n",
      "Best val loss: 1.12113E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.1_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.62001E-01  - Val Loss: 1.14316E+00\n",
      "Epoch 200/250  - Train Loss: 9.44416E-01  - Val Loss: 1.12471E+00\n",
      "Best val loss: 1.11714E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.1_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.57257E-01  - Val Loss: 1.13433E+00\n",
      "Epoch 200/250  - Train Loss: 9.40445E-01  - Val Loss: 1.12279E+00\n",
      "Best val loss: 1.11892E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.1_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.53453E-01  - Val Loss: 1.13546E+00\n",
      "Early stopping at epoch 127\n",
      "Best val loss: 1.13522E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.1_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 30\n",
      "Best val loss: 1.13516E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.1_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-05\n",
      "                                lambda_l2       =0e+00\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.77368E-01  - Val Loss: 1.13572E+00\n",
      "Early stopping at epoch 140\n",
      "Best val loss: 1.13427E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.2_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.71926E-01  - Val Loss: 1.14128E+00\n",
      "Epoch 200/250  - Train Loss: 9.60510E-01  - Val Loss: 1.13490E+00\n",
      "Best val loss: 1.13242E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.2_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.69979E-01  - Val Loss: 1.13870E+00\n",
      "Epoch 200/250  - Train Loss: 9.60173E-01  - Val Loss: 1.13563E+00\n",
      "Early stopping at epoch 233\n",
      "Best val loss: 1.13425E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.2_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.65276E-01  - Val Loss: 1.13709E+00\n",
      "Early stopping at epoch 166\n",
      "Best val loss: 1.13469E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.2_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 31\n",
      "Best val loss: 1.13517E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.2_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-05\n",
      "                                lambda_l2       =0e+00\n",
      "                                dropout         =3e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 58\n",
      "Best val loss: 1.15482E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.3_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.82280E-01  - Val Loss: 1.14351E+00\n",
      "Early stopping at epoch 181\n",
      "Best val loss: 1.13936E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.3_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.83872E-01  - Val Loss: 1.14296E+00\n",
      "Early stopping at epoch 192\n",
      "Best val loss: 1.14098E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.3_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.76455E-01  - Val Loss: 1.14038E+00\n",
      "Epoch 200/250  - Train Loss: 9.68327E-01  - Val Loss: 1.13823E+00\n",
      "Best val loss: 1.13627E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.3_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 31\n",
      "Best val loss: 1.13520E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.3_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-05\n",
      "                                lambda_l2       =1e-05\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.33019E-01  - Val Loss: 1.11163E+00\n",
      "Early stopping at epoch 117\n",
      "Best val loss: 1.11049E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.0_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.41752E-01  - Val Loss: 1.14155E+00\n",
      "Epoch 200/250  - Train Loss: 8.95348E-01  - Val Loss: 1.12238E+00\n",
      "Early stopping at epoch 215\n",
      "Best val loss: 1.12094E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.0_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.30896E-01  - Val Loss: 1.13693E+00\n",
      "Early stopping at epoch 189\n",
      "Best val loss: 1.12571E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.0_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.27503E-01  - Val Loss: 1.13529E+00\n",
      "Epoch 200/250  - Train Loss: 8.89122E-01  - Val Loss: 1.11701E+00\n",
      "Early stopping at epoch 236\n",
      "Best val loss: 1.11639E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.0_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 30\n",
      "Best val loss: 1.13526E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.0_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-05\n",
      "                                lambda_l2       =1e-05\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.65711E-01  - Val Loss: 1.12576E+00\n",
      "Early stopping at epoch 156\n",
      "Best val loss: 1.12156E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.1_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.62371E-01  - Val Loss: 1.14311E+00\n",
      "Epoch 200/250  - Train Loss: 9.44529E-01  - Val Loss: 1.12517E+00\n",
      "Best val loss: 1.11691E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.1_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.57550E-01  - Val Loss: 1.13434E+00\n",
      "Epoch 200/250  - Train Loss: 9.40791E-01  - Val Loss: 1.12174E+00\n",
      "Best val loss: 1.11754E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.1_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.53770E-01  - Val Loss: 1.13582E+00\n",
      "Early stopping at epoch 127\n",
      "Best val loss: 1.13557E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.1_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 30\n",
      "Best val loss: 1.13516E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.1_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-05\n",
      "                                lambda_l2       =1e-05\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.77617E-01  - Val Loss: 1.13547E+00\n",
      "Early stopping at epoch 140\n",
      "Best val loss: 1.13430E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.2_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.72272E-01  - Val Loss: 1.14117E+00\n",
      "Epoch 200/250  - Train Loss: 9.60433E-01  - Val Loss: 1.13474E+00\n",
      "Best val loss: 1.13263E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.2_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.70294E-01  - Val Loss: 1.13869E+00\n",
      "Epoch 200/250  - Train Loss: 9.60377E-01  - Val Loss: 1.13556E+00\n",
      "Early stopping at epoch 233\n",
      "Best val loss: 1.13389E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.2_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.65654E-01  - Val Loss: 1.13693E+00\n",
      "Early stopping at epoch 166\n",
      "Best val loss: 1.13448E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.2_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 31\n",
      "Best val loss: 1.13517E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.2_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-05\n",
      "                                lambda_l2       =1e-05\n",
      "                                dropout         =3e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 58\n",
      "Best val loss: 1.15480E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.3_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.82628E-01  - Val Loss: 1.14350E+00\n",
      "Early stopping at epoch 181\n",
      "Best val loss: 1.13915E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.3_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.84131E-01  - Val Loss: 1.14298E+00\n",
      "Early stopping at epoch 192\n",
      "Best val loss: 1.14079E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.3_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.76735E-01  - Val Loss: 1.14041E+00\n",
      "Epoch 200/250  - Train Loss: 9.68558E-01  - Val Loss: 1.13834E+00\n",
      "Best val loss: 1.13671E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.3_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 31\n",
      "Best val loss: 1.13520E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.3_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-05\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.36605E-01  - Val Loss: 1.11316E+00\n",
      "Early stopping at epoch 127\n",
      "Best val loss: 1.11244E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.45733E-01  - Val Loss: 1.14212E+00\n",
      "Epoch 200/250  - Train Loss: 8.99104E-01  - Val Loss: 1.12273E+00\n",
      "Early stopping at epoch 215\n",
      "Best val loss: 1.12201E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.33748E-01  - Val Loss: 1.13520E+00\n",
      "Early stopping at epoch 196\n",
      "Best val loss: 1.12419E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.31142E-01  - Val Loss: 1.13638E+00\n",
      "Epoch 200/250  - Train Loss: 8.93456E-01  - Val Loss: 1.11576E+00\n",
      "Early stopping at epoch 225\n",
      "Best val loss: 1.11576E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 30\n",
      "Best val loss: 1.13526E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-05\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.67598E-01  - Val Loss: 1.12492E+00\n",
      "Early stopping at epoch 184\n",
      "Best val loss: 1.11910E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.1_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.64676E-01  - Val Loss: 1.14300E+00\n",
      "Epoch 200/250  - Train Loss: 9.46485E-01  - Val Loss: 1.12213E+00\n",
      "Best val loss: 1.11506E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.1_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.60222E-01  - Val Loss: 1.13432E+00\n",
      "Epoch 200/250  - Train Loss: 9.43459E-01  - Val Loss: 1.12194E+00\n",
      "Best val loss: 1.11726E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.1_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.55736E-01  - Val Loss: 1.13564E+00\n",
      "Early stopping at epoch 127\n",
      "Best val loss: 1.13542E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.1_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 31\n",
      "Best val loss: 1.13516E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.1_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-05\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.79860E-01  - Val Loss: 1.13561E+00\n",
      "Early stopping at epoch 140\n",
      "Best val loss: 1.13394E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.2_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.74783E-01  - Val Loss: 1.14086E+00\n",
      "Early stopping at epoch 181\n",
      "Best val loss: 1.13607E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.2_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.72697E-01  - Val Loss: 1.13824E+00\n",
      "Epoch 200/250  - Train Loss: 9.62372E-01  - Val Loss: 1.13488E+00\n",
      "Best val loss: 1.13242E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.2_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.67260E-01  - Val Loss: 1.13686E+00\n",
      "Early stopping at epoch 166\n",
      "Best val loss: 1.13447E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.2_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 31\n",
      "Best val loss: 1.13516E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.2_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-05\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =3e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 58\n",
      "Best val loss: 1.15469E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.3_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.84579E-01  - Val Loss: 1.14375E+00\n",
      "Epoch 200/250  - Train Loss: 9.72506E-01  - Val Loss: 1.13864E+00\n",
      "Early stopping at epoch 218\n",
      "Best val loss: 1.13766E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.3_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.86556E-01  - Val Loss: 1.14283E+00\n",
      "Early stopping at epoch 192\n",
      "Best val loss: 1.14018E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.3_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.78425E-01  - Val Loss: 1.14029E+00\n",
      "Epoch 200/250  - Train Loss: 9.70046E-01  - Val Loss: 1.13812E+00\n",
      "Best val loss: 1.13601E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.3_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 31\n",
      "Best val loss: 1.13520E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.3_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-05\n",
      "                                lambda_l2       =1e-03\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.60747E-01  - Val Loss: 1.11412E+00\n",
      "Early stopping at epoch 140\n",
      "Best val loss: 1.10800E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.0_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.66215E-01  - Val Loss: 1.12706E+00\n",
      "Early stopping at epoch 164\n",
      "Best val loss: 1.10829E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.0_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.55936E-01  - Val Loss: 1.12835E+00\n",
      "Epoch 200/250  - Train Loss: 9.15247E-01  - Val Loss: 1.11119E+00\n",
      "Early stopping at epoch 202\n",
      "Best val loss: 1.11014E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.0_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.47992E-01  - Val Loss: 1.13128E+00\n",
      "Epoch 200/250  - Train Loss: 9.08058E-01  - Val Loss: 1.11441E+00\n",
      "Early stopping at epoch 203\n",
      "Best val loss: 1.11336E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.0_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 1.13517E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.0_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-05\n",
      "                                lambda_l2       =1e-03\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.82074E-01  - Val Loss: 1.12379E+00\n",
      "Epoch 200/250  - Train Loss: 9.59573E-01  - Val Loss: 1.11694E+00\n",
      "Early stopping at epoch 211\n",
      "Best val loss: 1.11410E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.1_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.81884E-01  - Val Loss: 1.13817E+00\n",
      "Epoch 200/250  - Train Loss: 9.55951E-01  - Val Loss: 1.11992E+00\n",
      "Best val loss: 1.11802E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.1_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.78768E-01  - Val Loss: 1.13415E+00\n",
      "Epoch 200/250  - Train Loss: 9.58952E-01  - Val Loss: 1.12265E+00\n",
      "Best val loss: 1.11819E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.1_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 92\n",
      "Best val loss: 1.13898E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.1_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 31\n",
      "Best val loss: 1.13509E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.1_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-05\n",
      "                                lambda_l2       =1e-03\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.93870E-01  - Val Loss: 1.13681E+00\n",
      "Epoch 200/250  - Train Loss: 9.75223E-01  - Val Loss: 1.13203E+00\n",
      "Early stopping at epoch 211\n",
      "Best val loss: 1.13019E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.2_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.90675E-01  - Val Loss: 1.14120E+00\n",
      "Epoch 200/250  - Train Loss: 9.71964E-01  - Val Loss: 1.13121E+00\n",
      "Best val loss: 1.12621E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.2_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.90700E-01  - Val Loss: 1.13719E+00\n",
      "Early stopping at epoch 136\n",
      "Best val loss: 1.13607E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.2_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.80094E-01  - Val Loss: 1.14012E+00\n",
      "Epoch 200/250  - Train Loss: 9.69437E-01  - Val Loss: 1.13368E+00\n",
      "Best val loss: 1.12864E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.2_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 31\n",
      "Best val loss: 1.13513E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.2_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-05\n",
      "                                lambda_l2       =1e-03\n",
      "                                dropout         =3e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.00062E+00  - Val Loss: 1.14574E+00\n",
      "Epoch 200/250  - Train Loss: 9.82392E-01  - Val Loss: 1.13640E+00\n",
      "Early stopping at epoch 211\n",
      "Best val loss: 1.13559E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.3_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.98973E-01  - Val Loss: 1.14447E+00\n",
      "Epoch 200/250  - Train Loss: 9.80386E-01  - Val Loss: 1.13573E+00\n",
      "Best val loss: 1.13251E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.3_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.00331E+00  - Val Loss: 1.14155E+00\n",
      "Early stopping at epoch 191\n",
      "Best val loss: 1.13704E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.3_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.89660E-01  - Val Loss: 1.14071E+00\n",
      "Early stopping at epoch 127\n",
      "Best val loss: 1.14059E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.3_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 31\n",
      "Best val loss: 1.13518E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.3_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-05\n",
      "                                lambda_l2       =1e-02\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06634E+00  - Val Loss: 1.15385E+00\n",
      "Early stopping at epoch 105\n",
      "Best val loss: 1.15367E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.0_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07137E+00  - Val Loss: 1.14994E+00\n",
      "Epoch 200/250  - Train Loss: 1.01551E+00  - Val Loss: 1.14051E+00\n",
      "Best val loss: 1.13883E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.0_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07594E+00  - Val Loss: 1.14761E+00\n",
      "Epoch 200/250  - Train Loss: 1.01801E+00  - Val Loss: 1.13904E+00\n",
      "Best val loss: 1.13340E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.0_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06981E+00  - Val Loss: 1.15560E+00\n",
      "Epoch 200/250  - Train Loss: 1.01631E+00  - Val Loss: 1.15484E+00\n",
      "Early stopping at epoch 206\n",
      "Best val loss: 1.15437E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.0_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 1.13502E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.0_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-05\n",
      "                                lambda_l2       =1e-02\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06637E+00  - Val Loss: 1.15374E+00\n",
      "Early stopping at epoch 105\n",
      "Best val loss: 1.15350E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.1_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07289E+00  - Val Loss: 1.15249E+00\n",
      "Epoch 200/250  - Train Loss: 1.01989E+00  - Val Loss: 1.14454E+00\n",
      "Best val loss: 1.14133E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.1_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07973E+00  - Val Loss: 1.15026E+00\n",
      "Epoch 200/250  - Train Loss: 1.02512E+00  - Val Loss: 1.14476E+00\n",
      "Best val loss: 1.14139E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.1_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07002E+00  - Val Loss: 1.15557E+00\n",
      "Epoch 200/250  - Train Loss: 1.01638E+00  - Val Loss: 1.15488E+00\n",
      "Early stopping at epoch 206\n",
      "Best val loss: 1.15441E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.1_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 1.13504E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.1_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-05\n",
      "                                lambda_l2       =1e-02\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06646E+00  - Val Loss: 1.15376E+00\n",
      "Early stopping at epoch 105\n",
      "Best val loss: 1.15352E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.2_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07346E+00  - Val Loss: 1.15452E+00\n",
      "Early stopping at epoch 183\n",
      "Best val loss: 1.15239E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.2_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.08189E+00  - Val Loss: 1.15216E+00\n",
      "Epoch 200/250  - Train Loss: 1.02801E+00  - Val Loss: 1.15050E+00\n",
      "Early stopping at epoch 212\n",
      "Best val loss: 1.15043E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.2_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07019E+00  - Val Loss: 1.15558E+00\n",
      "Epoch 200/250  - Train Loss: 1.01645E+00  - Val Loss: 1.15487E+00\n",
      "Early stopping at epoch 206\n",
      "Best val loss: 1.15439E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.2_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 1.13505E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.2_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-05\n",
      "                                lambda_l2       =1e-02\n",
      "                                dropout         =3e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06660E+00  - Val Loss: 1.15382E+00\n",
      "Early stopping at epoch 105\n",
      "Best val loss: 1.15362E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.3_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07416E+00  - Val Loss: 1.15609E+00\n",
      "Early stopping at epoch 141\n",
      "Best val loss: 1.15508E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.3_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.08311E+00  - Val Loss: 1.15493E+00\n",
      "Early stopping at epoch 178\n",
      "Best val loss: 1.15356E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.3_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07034E+00  - Val Loss: 1.15555E+00\n",
      "Epoch 200/250  - Train Loss: 1.01656E+00  - Val Loss: 1.15485E+00\n",
      "Early stopping at epoch 206\n",
      "Best val loss: 1.15439E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.3_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 1.13506E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.3_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =0e+00\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.55250E-01  - Val Loss: 1.10922E+00\n",
      "Early stopping at epoch 140\n",
      "Best val loss: 1.10428E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.0_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.61741E-01  - Val Loss: 1.13087E+00\n",
      "Epoch 200/250  - Train Loss: 9.05183E-01  - Val Loss: 1.11046E+00\n",
      "Early stopping at epoch 200\n",
      "Best val loss: 1.10515E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.0_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.50631E-01  - Val Loss: 1.12521E+00\n",
      "Epoch 200/250  - Train Loss: 9.07498E-01  - Val Loss: 1.10992E+00\n",
      "Early stopping at epoch 202\n",
      "Best val loss: 1.10797E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.0_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.44420E-01  - Val Loss: 1.13833E+00\n",
      "Early stopping at epoch 172\n",
      "Best val loss: 1.12831E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.0_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 1.13519E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.0_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =0e+00\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.79927E-01  - Val Loss: 1.12420E+00\n",
      "Epoch 200/250  - Train Loss: 9.58243E-01  - Val Loss: 1.11655E+00\n",
      "Early stopping at epoch 205\n",
      "Best val loss: 1.11471E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.1_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.79254E-01  - Val Loss: 1.13813E+00\n",
      "Epoch 200/250  - Train Loss: 9.55891E-01  - Val Loss: 1.12000E+00\n",
      "Best val loss: 1.11593E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.1_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.75891E-01  - Val Loss: 1.13332E+00\n",
      "Early stopping at epoch 196\n",
      "Best val loss: 1.12375E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.1_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 78\n",
      "Best val loss: 1.14031E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.1_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 31\n",
      "Best val loss: 1.13514E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.1_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =0e+00\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.91988E-01  - Val Loss: 1.13659E+00\n",
      "Early stopping at epoch 140\n",
      "Best val loss: 1.13516E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.2_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.87136E-01  - Val Loss: 1.13848E+00\n",
      "Epoch 200/250  - Train Loss: 9.71636E-01  - Val Loss: 1.12829E+00\n",
      "Early stopping at epoch 234\n",
      "Best val loss: 1.12622E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.2_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.88840E-01  - Val Loss: 1.13701E+00\n",
      "Epoch 200/250  - Train Loss: 9.74711E-01  - Val Loss: 1.13337E+00\n",
      "Best val loss: 1.13003E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.2_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 93\n",
      "Best val loss: 1.14098E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.2_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 31\n",
      "Best val loss: 1.13516E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.2_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =0e+00\n",
      "                                dropout         =3e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.99770E-01  - Val Loss: 1.14477E+00\n",
      "Early stopping at epoch 185\n",
      "Best val loss: 1.13905E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.3_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.96525E-01  - Val Loss: 1.14227E+00\n",
      "Epoch 200/250  - Train Loss: 9.81514E-01  - Val Loss: 1.13463E+00\n",
      "Best val loss: 1.13188E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.3_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.00221E+00  - Val Loss: 1.14131E+00\n",
      "Epoch 200/250  - Train Loss: 9.83142E-01  - Val Loss: 1.13807E+00\n",
      "Early stopping at epoch 207\n",
      "Best val loss: 1.13736E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.3_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.88917E-01  - Val Loss: 1.14149E+00\n",
      "Early stopping at epoch 127\n",
      "Best val loss: 1.14130E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.3_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 31\n",
      "Best val loss: 1.13518E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.3_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-05\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.55759E-01  - Val Loss: 1.10941E+00\n",
      "Early stopping at epoch 145\n",
      "Best val loss: 1.10513E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.0_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.62140E-01  - Val Loss: 1.13065E+00\n",
      "Epoch 200/250  - Train Loss: 9.03057E-01  - Val Loss: 1.10675E+00\n",
      "Early stopping at epoch 215\n",
      "Best val loss: 1.10205E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.0_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.51020E-01  - Val Loss: 1.12526E+00\n",
      "Epoch 200/250  - Train Loss: 9.08009E-01  - Val Loss: 1.10988E+00\n",
      "Early stopping at epoch 205\n",
      "Best val loss: 1.10717E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.0_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.44725E-01  - Val Loss: 1.13870E+00\n",
      "Early stopping at epoch 183\n",
      "Best val loss: 1.12709E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.0_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 1.13519E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.0_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-05\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.80063E-01  - Val Loss: 1.12441E+00\n",
      "Early stopping at epoch 193\n",
      "Best val loss: 1.11498E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.1_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.79461E-01  - Val Loss: 1.13807E+00\n",
      "Epoch 200/250  - Train Loss: 9.55777E-01  - Val Loss: 1.11946E+00\n",
      "Best val loss: 1.11525E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.1_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.76078E-01  - Val Loss: 1.13312E+00\n",
      "Epoch 200/250  - Train Loss: 9.57924E-01  - Val Loss: 1.12211E+00\n",
      "Best val loss: 1.11884E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.1_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 78\n",
      "Best val loss: 1.14042E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.1_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 31\n",
      "Best val loss: 1.13514E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.1_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-05\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.92140E-01  - Val Loss: 1.13671E+00\n",
      "Early stopping at epoch 140\n",
      "Best val loss: 1.13525E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.2_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.87365E-01  - Val Loss: 1.13857E+00\n",
      "Epoch 200/250  - Train Loss: 9.71562E-01  - Val Loss: 1.12912E+00\n",
      "Best val loss: 1.12567E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.2_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.89029E-01  - Val Loss: 1.13711E+00\n",
      "Early stopping at epoch 136\n",
      "Best val loss: 1.13613E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.2_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 92\n",
      "Best val loss: 1.14120E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.2_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 31\n",
      "Best val loss: 1.13516E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.2_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-05\n",
      "                                dropout         =3e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.99908E-01  - Val Loss: 1.14482E+00\n",
      "Epoch 200/250  - Train Loss: 9.84856E-01  - Val Loss: 1.13894E+00\n",
      "Early stopping at epoch 211\n",
      "Best val loss: 1.13847E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.3_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.96658E-01  - Val Loss: 1.14230E+00\n",
      "Epoch 200/250  - Train Loss: 9.81598E-01  - Val Loss: 1.13486E+00\n",
      "Best val loss: 1.13171E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.3_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.00235E+00  - Val Loss: 1.14114E+00\n",
      "Epoch 200/250  - Train Loss: 9.83201E-01  - Val Loss: 1.13857E+00\n",
      "Early stopping at epoch 207\n",
      "Best val loss: 1.13723E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.3_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.89066E-01  - Val Loss: 1.14163E+00\n",
      "Epoch 200/250  - Train Loss: 9.79576E-01  - Val Loss: 1.14029E+00\n",
      "Best val loss: 1.13641E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.3_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 31\n",
      "Best val loss: 1.13518E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.3_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.58726E-01  - Val Loss: 1.11062E+00\n",
      "Early stopping at epoch 150\n",
      "Best val loss: 1.10350E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.64814E-01  - Val Loss: 1.12984E+00\n",
      "Early stopping at epoch 181\n",
      "Best val loss: 1.10412E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.53305E-01  - Val Loss: 1.12518E+00\n",
      "Epoch 200/250  - Train Loss: 9.10141E-01  - Val Loss: 1.10783E+00\n",
      "Early stopping at epoch 230\n",
      "Best val loss: 1.10702E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.47065E-01  - Val Loss: 1.13670E+00\n",
      "Epoch 200/250  - Train Loss: 8.97243E-01  - Val Loss: 1.12668E+00\n",
      "Early stopping at epoch 203\n",
      "Best val loss: 1.12369E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 1.13518E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.81416E-01  - Val Loss: 1.12462E+00\n",
      "Early stopping at epoch 193\n",
      "Best val loss: 1.11619E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.1_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.81097E-01  - Val Loss: 1.13775E+00\n",
      "Epoch 200/250  - Train Loss: 9.57355E-01  - Val Loss: 1.11850E+00\n",
      "Best val loss: 1.11485E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.1_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.77796E-01  - Val Loss: 1.13317E+00\n",
      "Epoch 200/250  - Train Loss: 9.59249E-01  - Val Loss: 1.12269E+00\n",
      "Best val loss: 1.11942E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.1_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 78\n",
      "Best val loss: 1.14092E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.1_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 31\n",
      "Best val loss: 1.13514E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.1_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.93462E-01  - Val Loss: 1.13707E+00\n",
      "Epoch 200/250  - Train Loss: 9.76821E-01  - Val Loss: 1.13335E+00\n",
      "Early stopping at epoch 211\n",
      "Best val loss: 1.13187E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.2_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.89078E-01  - Val Loss: 1.13884E+00\n",
      "Epoch 200/250  - Train Loss: 9.72964E-01  - Val Loss: 1.12930E+00\n",
      "Best val loss: 1.12476E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.2_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.90789E-01  - Val Loss: 1.13710E+00\n",
      "Early stopping at epoch 136\n",
      "Best val loss: 1.13605E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.2_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 93\n",
      "Best val loss: 1.14129E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.2_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 31\n",
      "Best val loss: 1.13515E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.2_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =3e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.00107E+00  - Val Loss: 1.14475E+00\n",
      "Epoch 200/250  - Train Loss: 9.85537E-01  - Val Loss: 1.13830E+00\n",
      "Early stopping at epoch 211\n",
      "Best val loss: 1.13777E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.3_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.98162E-01  - Val Loss: 1.14244E+00\n",
      "Epoch 200/250  - Train Loss: 9.82374E-01  - Val Loss: 1.13436E+00\n",
      "Best val loss: 1.13180E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.3_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.00404E+00  - Val Loss: 1.14124E+00\n",
      "Epoch 200/250  - Train Loss: 9.84457E-01  - Val Loss: 1.13828E+00\n",
      "Early stopping at epoch 207\n",
      "Best val loss: 1.13689E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.3_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.90051E-01  - Val Loss: 1.14131E+00\n",
      "Early stopping at epoch 127\n",
      "Best val loss: 1.14120E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.3_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 31\n",
      "Best val loss: 1.13518E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.3_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-03\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.79662E-01  - Val Loss: 1.12044E+00\n",
      "Early stopping at epoch 184\n",
      "Best val loss: 1.10190E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.0_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.83242E-01  - Val Loss: 1.13298E+00\n",
      "Early stopping at epoch 199\n",
      "Best val loss: 1.10312E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.0_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.71976E-01  - Val Loss: 1.12424E+00\n",
      "Epoch 200/250  - Train Loss: 9.25333E-01  - Val Loss: 1.10742E+00\n",
      "Early stopping at epoch 202\n",
      "Best val loss: 1.10357E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.0_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.70032E-01  - Val Loss: 1.13642E+00\n",
      "Epoch 200/250  - Train Loss: 9.14203E-01  - Val Loss: 1.10154E+00\n",
      "Early stopping at epoch 216\n",
      "Best val loss: 1.10022E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.0_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 1.13513E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.0_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-03\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.94992E-01  - Val Loss: 1.13035E+00\n",
      "Epoch 200/250  - Train Loss: 9.69992E-01  - Val Loss: 1.11883E+00\n",
      "Early stopping at epoch 209\n",
      "Best val loss: 1.11793E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.1_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.95057E-01  - Val Loss: 1.13897E+00\n",
      "Epoch 200/250  - Train Loss: 9.67873E-01  - Val Loss: 1.11649E+00\n",
      "Best val loss: 1.11430E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.1_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.93460E-01  - Val Loss: 1.13232E+00\n",
      "Early stopping at epoch 196\n",
      "Best val loss: 1.12346E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.1_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.84288E-01  - Val Loss: 1.14114E+00\n",
      "Epoch 200/250  - Train Loss: 9.67626E-01  - Val Loss: 1.12494E+00\n",
      "Early stopping at epoch 244\n",
      "Best val loss: 1.12248E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.1_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 1.13512E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.1_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-03\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.00431E+00  - Val Loss: 1.14051E+00\n",
      "Epoch 200/250  - Train Loss: 9.84561E-01  - Val Loss: 1.12967E+00\n",
      "Early stopping at epoch 211\n",
      "Best val loss: 1.12923E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.2_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.00254E+00  - Val Loss: 1.14189E+00\n",
      "Epoch 200/250  - Train Loss: 9.81430E-01  - Val Loss: 1.12934E+00\n",
      "Best val loss: 1.12410E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.2_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.00479E+00  - Val Loss: 1.13663E+00\n",
      "Epoch 200/250  - Train Loss: 9.84667E-01  - Val Loss: 1.13292E+00\n",
      "Early stopping at epoch 206\n",
      "Best val loss: 1.13174E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.2_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.91839E-01  - Val Loss: 1.14141E+00\n",
      "Epoch 200/250  - Train Loss: 9.79679E-01  - Val Loss: 1.13663E+00\n",
      "Best val loss: 1.13261E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.2_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 31\n",
      "Best val loss: 1.13513E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.2_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-03\n",
      "                                dropout         =3e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.00994E+00  - Val Loss: 1.14861E+00\n",
      "Epoch 200/250  - Train Loss: 9.91625E-01  - Val Loss: 1.13429E+00\n",
      "Best val loss: 1.13364E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.3_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.00927E+00  - Val Loss: 1.14501E+00\n",
      "Epoch 200/250  - Train Loss: 9.89878E-01  - Val Loss: 1.13670E+00\n",
      "Best val loss: 1.13309E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.3_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.01549E+00  - Val Loss: 1.14092E+00\n",
      "Early stopping at epoch 198\n",
      "Best val loss: 1.13567E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.3_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.99485E-01  - Val Loss: 1.14108E+00\n",
      "Epoch 200/250  - Train Loss: 9.85691E-01  - Val Loss: 1.13928E+00\n",
      "Best val loss: 1.13610E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.3_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 31\n",
      "Best val loss: 1.13517E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.3_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-02\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07134E+00  - Val Loss: 1.15385E+00\n",
      "Early stopping at epoch 161\n",
      "Best val loss: 1.15340E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.0_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07883E+00  - Val Loss: 1.15280E+00\n",
      "Epoch 200/250  - Train Loss: 1.02240E+00  - Val Loss: 1.14432E+00\n",
      "Best val loss: 1.14063E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.0_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.08460E+00  - Val Loss: 1.14898E+00\n",
      "Epoch 200/250  - Train Loss: 1.02683E+00  - Val Loss: 1.14340E+00\n",
      "Best val loss: 1.13681E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.0_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07500E+00  - Val Loss: 1.15572E+00\n",
      "Epoch 200/250  - Train Loss: 1.01791E+00  - Val Loss: 1.15488E+00\n",
      "Early stopping at epoch 206\n",
      "Best val loss: 1.15443E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.0_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 1.13502E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.0_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-02\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07136E+00  - Val Loss: 1.15376E+00\n",
      "Early stopping at epoch 105\n",
      "Best val loss: 1.15352E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.1_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07931E+00  - Val Loss: 1.15523E+00\n",
      "Early stopping at epoch 141\n",
      "Best val loss: 1.15427E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.1_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.08781E+00  - Val Loss: 1.15124E+00\n",
      "Epoch 200/250  - Train Loss: 1.03117E+00  - Val Loss: 1.14957E+00\n",
      "Early stopping at epoch 212\n",
      "Best val loss: 1.14952E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.1_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07515E+00  - Val Loss: 1.15568E+00\n",
      "Epoch 200/250  - Train Loss: 1.01799E+00  - Val Loss: 1.15491E+00\n",
      "Early stopping at epoch 206\n",
      "Best val loss: 1.15446E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.1_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 1.13505E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.1_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-02\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07143E+00  - Val Loss: 1.15377E+00\n",
      "Early stopping at epoch 105\n",
      "Best val loss: 1.15353E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.2_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07936E+00  - Val Loss: 1.15652E+00\n",
      "Early stopping at epoch 141\n",
      "Best val loss: 1.15574E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.2_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.08916E+00  - Val Loss: 1.15344E+00\n",
      "Early stopping at epoch 175\n",
      "Best val loss: 1.15244E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.2_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07535E+00  - Val Loss: 1.15566E+00\n",
      "Epoch 200/250  - Train Loss: 1.01808E+00  - Val Loss: 1.15491E+00\n",
      "Early stopping at epoch 206\n",
      "Best val loss: 1.15445E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.2_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 1.13504E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.2_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-02\n",
      "                                dropout         =3e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07154E+00  - Val Loss: 1.15382E+00\n",
      "Early stopping at epoch 105\n",
      "Best val loss: 1.15367E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.3_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07953E+00  - Val Loss: 1.15732E+00\n",
      "Early stopping at epoch 141\n",
      "Best val loss: 1.15641E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.3_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.08907E+00  - Val Loss: 1.15686E+00\n",
      "Epoch 200/250  - Train Loss: 1.02493E+00  - Val Loss: 1.15537E+00\n",
      "Early stopping at epoch 201\n",
      "Best val loss: 1.15537E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.3_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07550E+00  - Val Loss: 1.15564E+00\n",
      "Epoch 200/250  - Train Loss: 1.01819E+00  - Val Loss: 1.15490E+00\n",
      "Early stopping at epoch 206\n",
      "Best val loss: 1.15445E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.3_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 1.13505E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.3_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-03\n",
      "                                lambda_l2       =0e+00\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05176E+00  - Val Loss: 1.15335E+00\n",
      "Early stopping at epoch 105\n",
      "Best val loss: 1.15305E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.0_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05686E+00  - Val Loss: 1.15504E+00\n",
      "Early stopping at epoch 138\n",
      "Best val loss: 1.15420E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.0_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06071E+00  - Val Loss: 1.14198E+00\n",
      "Epoch 200/250  - Train Loss: 1.00621E+00  - Val Loss: 1.13749E+00\n",
      "Early stopping at epoch 247\n",
      "Best val loss: 1.13669E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.0_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05426E+00  - Val Loss: 1.15375E+00\n",
      "Early stopping at epoch 143\n",
      "Best val loss: 1.15356E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.0_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 1.13495E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.0_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-03\n",
      "                                lambda_l2       =0e+00\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05258E+00  - Val Loss: 1.15324E+00\n",
      "Early stopping at epoch 105\n",
      "Best val loss: 1.15294E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.1_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05691E+00  - Val Loss: 1.15523E+00\n",
      "Early stopping at epoch 141\n",
      "Best val loss: 1.15437E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.1_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06550E+00  - Val Loss: 1.14401E+00\n",
      "Epoch 200/250  - Train Loss: 1.01814E+00  - Val Loss: 1.13964E+00\n",
      "Best val loss: 1.13810E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.1_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05506E+00  - Val Loss: 1.15398E+00\n",
      "Epoch 200/250  - Train Loss: 1.00983E+00  - Val Loss: 1.15398E+00\n",
      "Early stopping at epoch 206\n",
      "Best val loss: 1.15324E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.1_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 1.13498E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.1_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-03\n",
      "                                lambda_l2       =0e+00\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05283E+00  - Val Loss: 1.15322E+00\n",
      "Early stopping at epoch 105\n",
      "Best val loss: 1.15290E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.2_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05728E+00  - Val Loss: 1.15545E+00\n",
      "Early stopping at epoch 141\n",
      "Best val loss: 1.15456E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.2_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06934E+00  - Val Loss: 1.14652E+00\n",
      "Epoch 200/250  - Train Loss: 1.02454E+00  - Val Loss: 1.14381E+00\n",
      "Early stopping at epoch 201\n",
      "Best val loss: 1.14356E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.2_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05549E+00  - Val Loss: 1.15391E+00\n",
      "Epoch 200/250  - Train Loss: 1.01011E+00  - Val Loss: 1.15400E+00\n",
      "Early stopping at epoch 206\n",
      "Best val loss: 1.15325E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.2_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 1.13501E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.2_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-03\n",
      "                                lambda_l2       =0e+00\n",
      "                                dropout         =3e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05338E+00  - Val Loss: 1.15320E+00\n",
      "Early stopping at epoch 111\n",
      "Best val loss: 1.15305E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.3_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05775E+00  - Val Loss: 1.15569E+00\n",
      "Early stopping at epoch 183\n",
      "Best val loss: 1.15438E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.3_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07196E+00  - Val Loss: 1.15026E+00\n",
      "Early stopping at epoch 174\n",
      "Best val loss: 1.14928E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.3_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05590E+00  - Val Loss: 1.15381E+00\n",
      "Early stopping at epoch 143\n",
      "Best val loss: 1.15362E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.3_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 1.13503E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.3_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-03\n",
      "                                lambda_l2       =1e-05\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05183E+00  - Val Loss: 1.15335E+00\n",
      "Early stopping at epoch 105\n",
      "Best val loss: 1.15305E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.0_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05694E+00  - Val Loss: 1.15504E+00\n",
      "Early stopping at epoch 138\n",
      "Best val loss: 1.15420E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.0_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06083E+00  - Val Loss: 1.14201E+00\n",
      "Epoch 200/250  - Train Loss: 1.00629E+00  - Val Loss: 1.13753E+00\n",
      "Early stopping at epoch 247\n",
      "Best val loss: 1.13673E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.0_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05434E+00  - Val Loss: 1.15376E+00\n",
      "Early stopping at epoch 143\n",
      "Best val loss: 1.15356E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.0_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 1.13495E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.0_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-03\n",
      "                                lambda_l2       =1e-05\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05266E+00  - Val Loss: 1.15324E+00\n",
      "Early stopping at epoch 105\n",
      "Best val loss: 1.15295E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.1_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05699E+00  - Val Loss: 1.15524E+00\n",
      "Early stopping at epoch 141\n",
      "Best val loss: 1.15438E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.1_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06562E+00  - Val Loss: 1.14404E+00\n",
      "Epoch 200/250  - Train Loss: 1.01819E+00  - Val Loss: 1.13969E+00\n",
      "Best val loss: 1.13816E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.1_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05514E+00  - Val Loss: 1.15398E+00\n",
      "Epoch 200/250  - Train Loss: 1.00987E+00  - Val Loss: 1.15398E+00\n",
      "Early stopping at epoch 206\n",
      "Best val loss: 1.15324E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.1_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 1.13498E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.1_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-03\n",
      "                                lambda_l2       =1e-05\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05291E+00  - Val Loss: 1.15322E+00\n",
      "Early stopping at epoch 105\n",
      "Best val loss: 1.15290E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.2_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05735E+00  - Val Loss: 1.15546E+00\n",
      "Early stopping at epoch 141\n",
      "Best val loss: 1.15456E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.2_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06943E+00  - Val Loss: 1.14655E+00\n",
      "Epoch 200/250  - Train Loss: 1.02458E+00  - Val Loss: 1.14384E+00\n",
      "Early stopping at epoch 201\n",
      "Best val loss: 1.14358E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.2_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05556E+00  - Val Loss: 1.15392E+00\n",
      "Epoch 200/250  - Train Loss: 1.01015E+00  - Val Loss: 1.15400E+00\n",
      "Early stopping at epoch 206\n",
      "Best val loss: 1.15326E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.2_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 1.13501E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.2_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-03\n",
      "                                lambda_l2       =1e-05\n",
      "                                dropout         =3e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05345E+00  - Val Loss: 1.15320E+00\n",
      "Early stopping at epoch 111\n",
      "Best val loss: 1.15305E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.3_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05782E+00  - Val Loss: 1.15570E+00\n",
      "Early stopping at epoch 183\n",
      "Best val loss: 1.15438E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.3_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07205E+00  - Val Loss: 1.15030E+00\n",
      "Early stopping at epoch 174\n",
      "Best val loss: 1.14932E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.3_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05598E+00  - Val Loss: 1.15381E+00\n",
      "Early stopping at epoch 143\n",
      "Best val loss: 1.15362E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.3_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 1.13503E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.3_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-03\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05250E+00  - Val Loss: 1.15335E+00\n",
      "Early stopping at epoch 105\n",
      "Best val loss: 1.15306E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.0_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05762E+00  - Val Loss: 1.15509E+00\n",
      "Early stopping at epoch 138\n",
      "Best val loss: 1.15424E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.0_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06184E+00  - Val Loss: 1.14228E+00\n",
      "Epoch 200/250  - Train Loss: 1.00711E+00  - Val Loss: 1.13756E+00\n",
      "Early stopping at epoch 247\n",
      "Best val loss: 1.13658E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.0_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05506E+00  - Val Loss: 1.15378E+00\n",
      "Epoch 200/250  - Train Loss: 1.00997E+00  - Val Loss: 1.15401E+00\n",
      "Early stopping at epoch 206\n",
      "Best val loss: 1.15322E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.0_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 1.13495E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.0_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-03\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05329E+00  - Val Loss: 1.15325E+00\n",
      "Early stopping at epoch 105\n",
      "Best val loss: 1.15294E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.1_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05766E+00  - Val Loss: 1.15527E+00\n",
      "Early stopping at epoch 141\n",
      "Best val loss: 1.15442E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.1_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06660E+00  - Val Loss: 1.14431E+00\n",
      "Epoch 200/250  - Train Loss: 1.01877E+00  - Val Loss: 1.13980E+00\n",
      "Best val loss: 1.13831E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.1_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05583E+00  - Val Loss: 1.15401E+00\n",
      "Epoch 200/250  - Train Loss: 1.01017E+00  - Val Loss: 1.15399E+00\n",
      "Early stopping at epoch 206\n",
      "Best val loss: 1.15326E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.1_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 1.13498E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.1_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-03\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05354E+00  - Val Loss: 1.15323E+00\n",
      "Early stopping at epoch 105\n",
      "Best val loss: 1.15291E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.2_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05802E+00  - Val Loss: 1.15550E+00\n",
      "Early stopping at epoch 141\n",
      "Best val loss: 1.15460E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.2_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07025E+00  - Val Loss: 1.14688E+00\n",
      "Epoch 200/250  - Train Loss: 1.02493E+00  - Val Loss: 1.14421E+00\n",
      "Early stopping at epoch 201\n",
      "Best val loss: 1.14396E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.2_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05625E+00  - Val Loss: 1.15394E+00\n",
      "Epoch 200/250  - Train Loss: 1.01045E+00  - Val Loss: 1.15401E+00\n",
      "Early stopping at epoch 206\n",
      "Best val loss: 1.15327E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.2_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 1.13501E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.2_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-03\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =3e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05407E+00  - Val Loss: 1.15321E+00\n",
      "Early stopping at epoch 111\n",
      "Best val loss: 1.15305E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.3_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05847E+00  - Val Loss: 1.15572E+00\n",
      "Early stopping at epoch 183\n",
      "Best val loss: 1.15440E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.3_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07278E+00  - Val Loss: 1.15059E+00\n",
      "Early stopping at epoch 122\n",
      "Best val loss: 1.15055E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.3_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05665E+00  - Val Loss: 1.15384E+00\n",
      "Early stopping at epoch 143\n",
      "Best val loss: 1.15364E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.3_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 1.13503E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.3_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-03\n",
      "                                lambda_l2       =1e-03\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05904E+00  - Val Loss: 1.15339E+00\n",
      "Early stopping at epoch 105\n",
      "Best val loss: 1.15313E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.0_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06410E+00  - Val Loss: 1.15542E+00\n",
      "Early stopping at epoch 182\n",
      "Best val loss: 1.15425E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.0_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07190E+00  - Val Loss: 1.14462E+00\n",
      "Epoch 200/250  - Train Loss: 1.01517E+00  - Val Loss: 1.13693E+00\n",
      "Early stopping at epoch 247\n",
      "Best val loss: 1.13624E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.0_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06203E+00  - Val Loss: 1.15405E+00\n",
      "Epoch 200/250  - Train Loss: 1.01291E+00  - Val Loss: 1.15419E+00\n",
      "Early stopping at epoch 206\n",
      "Best val loss: 1.15343E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.0_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 1.13495E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.0_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-03\n",
      "                                lambda_l2       =1e-03\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05954E+00  - Val Loss: 1.15331E+00\n",
      "Early stopping at epoch 105\n",
      "Best val loss: 1.15298E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.1_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06426E+00  - Val Loss: 1.15561E+00\n",
      "Early stopping at epoch 141\n",
      "Best val loss: 1.15472E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.1_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07556E+00  - Val Loss: 1.14688E+00\n",
      "Epoch 200/250  - Train Loss: 1.02422E+00  - Val Loss: 1.14261E+00\n",
      "Best val loss: 1.14105E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.1_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06258E+00  - Val Loss: 1.15422E+00\n",
      "Epoch 200/250  - Train Loss: 1.01302E+00  - Val Loss: 1.15415E+00\n",
      "Early stopping at epoch 206\n",
      "Best val loss: 1.15345E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.1_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 1.13498E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.1_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-03\n",
      "                                lambda_l2       =1e-03\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05979E+00  - Val Loss: 1.15329E+00\n",
      "Early stopping at epoch 105\n",
      "Best val loss: 1.15298E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.2_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06458E+00  - Val Loss: 1.15578E+00\n",
      "Early stopping at epoch 146\n",
      "Best val loss: 1.15478E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.2_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07818E+00  - Val Loss: 1.14968E+00\n",
      "Early stopping at epoch 174\n",
      "Best val loss: 1.14822E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.2_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06296E+00  - Val Loss: 1.15422E+00\n",
      "Epoch 200/250  - Train Loss: 1.01325E+00  - Val Loss: 1.15416E+00\n",
      "Early stopping at epoch 206\n",
      "Best val loss: 1.15346E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.2_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 1.13501E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.2_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-03\n",
      "                                lambda_l2       =1e-03\n",
      "                                dropout         =3e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06021E+00  - Val Loss: 1.15328E+00\n",
      "Early stopping at epoch 105\n",
      "Best val loss: 1.15305E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.3_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06505E+00  - Val Loss: 1.15598E+00\n",
      "Early stopping at epoch 183\n",
      "Best val loss: 1.15461E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.3_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07987E+00  - Val Loss: 1.15293E+00\n",
      "Early stopping at epoch 122\n",
      "Best val loss: 1.15288E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.3_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06328E+00  - Val Loss: 1.15411E+00\n",
      "Epoch 200/250  - Train Loss: 1.01346E+00  - Val Loss: 1.15413E+00\n",
      "Early stopping at epoch 206\n",
      "Best val loss: 1.15346E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.3_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 1.13503E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.3_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-03\n",
      "                                lambda_l2       =1e-02\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.11945E+00  - Val Loss: 1.15401E+00\n",
      "Early stopping at epoch 161\n",
      "Best val loss: 1.15351E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.0_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.12873E+00  - Val Loss: 1.15859E+00\n",
      "Epoch 200/250  - Train Loss: 1.03447E+00  - Val Loss: 1.15593E+00\n",
      "Early stopping at epoch 233\n",
      "Best val loss: 1.15577E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.0_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.13982E+00  - Val Loss: 1.15903E+00\n",
      "Epoch 200/250  - Train Loss: 1.03883E+00  - Val Loss: 1.15583E+00\n",
      "Best val loss: 1.15524E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.0_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.12520E+00  - Val Loss: 1.15653E+00\n",
      "Epoch 200/250  - Train Loss: 1.03048E+00  - Val Loss: 1.15521E+00\n",
      "Best val loss: 1.15446E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.0_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 1.13498E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.0_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-03\n",
      "                                lambda_l2       =1e-02\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.11947E+00  - Val Loss: 1.15404E+00\n",
      "Early stopping at epoch 161\n",
      "Best val loss: 1.15349E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.1_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.12875E+00  - Val Loss: 1.15870E+00\n",
      "Epoch 200/250  - Train Loss: 1.03448E+00  - Val Loss: 1.15593E+00\n",
      "Early stopping at epoch 233\n",
      "Best val loss: 1.15574E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.1_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.13988E+00  - Val Loss: 1.15931E+00\n",
      "Epoch 200/250  - Train Loss: 1.03848E+00  - Val Loss: 1.15571E+00\n",
      "Best val loss: 1.15516E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.1_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.12531E+00  - Val Loss: 1.15648E+00\n",
      "Epoch 200/250  - Train Loss: 1.03059E+00  - Val Loss: 1.15521E+00\n",
      "Best val loss: 1.15447E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.1_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 1.13500E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.1_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-03\n",
      "                                lambda_l2       =1e-02\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.11954E+00  - Val Loss: 1.15393E+00\n",
      "Early stopping at epoch 111\n",
      "Best val loss: 1.15377E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.2_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.12892E+00  - Val Loss: 1.15885E+00\n",
      "Epoch 200/250  - Train Loss: 1.03443E+00  - Val Loss: 1.15591E+00\n",
      "Early stopping at epoch 233\n",
      "Best val loss: 1.15574E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.2_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.13916E+00  - Val Loss: 1.15949E+00\n",
      "Epoch 200/250  - Train Loss: 1.03812E+00  - Val Loss: 1.15565E+00\n",
      "Best val loss: 1.15509E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.2_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.12552E+00  - Val Loss: 1.15647E+00\n",
      "Epoch 200/250  - Train Loss: 1.03074E+00  - Val Loss: 1.15523E+00\n",
      "Best val loss: 1.15448E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.2_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 1.13500E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.2_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-03\n",
      "                                lambda_l2       =1e-02\n",
      "                                dropout         =3e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.11964E+00  - Val Loss: 1.15393E+00\n",
      "Early stopping at epoch 143\n",
      "Best val loss: 1.15352E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.3_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.12915E+00  - Val Loss: 1.15905E+00\n",
      "Epoch 200/250  - Train Loss: 1.03434E+00  - Val Loss: 1.15591E+00\n",
      "Early stopping at epoch 233\n",
      "Best val loss: 1.15572E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.3_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.13837E+00  - Val Loss: 1.15967E+00\n",
      "Epoch 200/250  - Train Loss: 1.03759E+00  - Val Loss: 1.15556E+00\n",
      "Best val loss: 1.15498E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.3_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.12570E+00  - Val Loss: 1.15642E+00\n",
      "Epoch 200/250  - Train Loss: 1.03090E+00  - Val Loss: 1.15524E+00\n",
      "Best val loss: 1.15450E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.3_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 1.13501E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.3_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-02\n",
      "                                lambda_l2       =0e+00\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.49557E+00  - Val Loss: 1.15524E+00\n",
      "Epoch 200/250  - Train Loss: 1.05951E+00  - Val Loss: 1.15277E+00\n",
      "Early stopping at epoch 221\n",
      "Best val loss: 1.15265E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.0_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.52406E+00  - Val Loss: 1.16333E+00\n",
      "Epoch 200/250  - Train Loss: 1.08053E+00  - Val Loss: 1.15949E+00\n",
      "Best val loss: 1.15854E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.0_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.54868E+00  - Val Loss: 1.16560E+00\n",
      "Epoch 200/250  - Train Loss: 1.07939E+00  - Val Loss: 1.15709E+00\n",
      "Best val loss: 1.15620E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.0_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.51579E+00  - Val Loss: 1.16066E+00\n",
      "Epoch 200/250  - Train Loss: 1.06475E+00  - Val Loss: 1.15507E+00\n",
      "Best val loss: 1.15416E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.0_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 34\n",
      "Best val loss: 1.13498E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.0_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-02\n",
      "                                lambda_l2       =0e+00\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.49546E+00  - Val Loss: 1.15523E+00\n",
      "Epoch 200/250  - Train Loss: 1.05938E+00  - Val Loss: 1.15282E+00\n",
      "Early stopping at epoch 221\n",
      "Best val loss: 1.15268E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.1_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.52401E+00  - Val Loss: 1.16341E+00\n",
      "Epoch 200/250  - Train Loss: 1.08047E+00  - Val Loss: 1.15947E+00\n",
      "Best val loss: 1.15852E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.1_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.54887E+00  - Val Loss: 1.16538E+00\n",
      "Epoch 200/250  - Train Loss: 1.07949E+00  - Val Loss: 1.15706E+00\n",
      "Best val loss: 1.15614E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.1_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.51604E+00  - Val Loss: 1.16086E+00\n",
      "Epoch 200/250  - Train Loss: 1.06503E+00  - Val Loss: 1.15505E+00\n",
      "Best val loss: 1.15416E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.1_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 35\n",
      "Best val loss: 1.13498E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.1_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-02\n",
      "                                lambda_l2       =0e+00\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.49535E+00  - Val Loss: 1.15522E+00\n",
      "Epoch 200/250  - Train Loss: 1.05925E+00  - Val Loss: 1.15288E+00\n",
      "Early stopping at epoch 216\n",
      "Best val loss: 1.15267E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.2_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.52409E+00  - Val Loss: 1.16349E+00\n",
      "Epoch 200/250  - Train Loss: 1.08044E+00  - Val Loss: 1.15945E+00\n",
      "Best val loss: 1.15851E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.2_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.54813E+00  - Val Loss: 1.16557E+00\n",
      "Epoch 200/250  - Train Loss: 1.07937E+00  - Val Loss: 1.15704E+00\n",
      "Best val loss: 1.15605E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.2_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.51644E+00  - Val Loss: 1.16084E+00\n",
      "Epoch 200/250  - Train Loss: 1.06534E+00  - Val Loss: 1.15507E+00\n",
      "Best val loss: 1.15416E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.2_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 35\n",
      "Best val loss: 1.13498E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.2_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-02\n",
      "                                lambda_l2       =0e+00\n",
      "                                dropout         =3e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.49527E+00  - Val Loss: 1.15521E+00\n",
      "Epoch 200/250  - Train Loss: 1.05912E+00  - Val Loss: 1.15297E+00\n",
      "Early stopping at epoch 213\n",
      "Best val loss: 1.15269E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.3_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.52417E+00  - Val Loss: 1.16356E+00\n",
      "Epoch 200/250  - Train Loss: 1.08039E+00  - Val Loss: 1.15942E+00\n",
      "Best val loss: 1.15850E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.3_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.54670E+00  - Val Loss: 1.16540E+00\n",
      "Epoch 200/250  - Train Loss: 1.07916E+00  - Val Loss: 1.15696E+00\n",
      "Best val loss: 1.15595E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.3_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.51689E+00  - Val Loss: 1.16088E+00\n",
      "Epoch 200/250  - Train Loss: 1.06560E+00  - Val Loss: 1.15511E+00\n",
      "Best val loss: 1.15416E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.3_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 35\n",
      "Best val loss: 1.13498E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.3_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-02\n",
      "                                lambda_l2       =1e-05\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.49565E+00  - Val Loss: 1.15524E+00\n",
      "Epoch 200/250  - Train Loss: 1.05955E+00  - Val Loss: 1.15276E+00\n",
      "Early stopping at epoch 221\n",
      "Best val loss: 1.15265E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.0_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.52414E+00  - Val Loss: 1.16334E+00\n",
      "Epoch 200/250  - Train Loss: 1.08056E+00  - Val Loss: 1.15949E+00\n",
      "Best val loss: 1.15855E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.0_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.54877E+00  - Val Loss: 1.16561E+00\n",
      "Epoch 200/250  - Train Loss: 1.07942E+00  - Val Loss: 1.15710E+00\n",
      "Best val loss: 1.15620E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.0_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.51587E+00  - Val Loss: 1.16067E+00\n",
      "Epoch 200/250  - Train Loss: 1.06479E+00  - Val Loss: 1.15507E+00\n",
      "Best val loss: 1.15416E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.0_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 34\n",
      "Best val loss: 1.13498E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.0_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-02\n",
      "                                lambda_l2       =1e-05\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.49553E+00  - Val Loss: 1.15523E+00\n",
      "Epoch 200/250  - Train Loss: 1.05941E+00  - Val Loss: 1.15282E+00\n",
      "Early stopping at epoch 221\n",
      "Best val loss: 1.15268E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.1_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.52409E+00  - Val Loss: 1.16342E+00\n",
      "Epoch 200/250  - Train Loss: 1.08051E+00  - Val Loss: 1.15947E+00\n",
      "Best val loss: 1.15853E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.1_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.54894E+00  - Val Loss: 1.16539E+00\n",
      "Epoch 200/250  - Train Loss: 1.07952E+00  - Val Loss: 1.15707E+00\n",
      "Best val loss: 1.15614E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.1_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.51611E+00  - Val Loss: 1.16086E+00\n",
      "Epoch 200/250  - Train Loss: 1.06506E+00  - Val Loss: 1.15506E+00\n",
      "Best val loss: 1.15416E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.1_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 35\n",
      "Best val loss: 1.13498E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.1_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-02\n",
      "                                lambda_l2       =1e-05\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.49542E+00  - Val Loss: 1.15522E+00\n",
      "Epoch 200/250  - Train Loss: 1.05928E+00  - Val Loss: 1.15288E+00\n",
      "Early stopping at epoch 216\n",
      "Best val loss: 1.15267E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.2_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.52419E+00  - Val Loss: 1.16349E+00\n",
      "Epoch 200/250  - Train Loss: 1.08048E+00  - Val Loss: 1.15945E+00\n",
      "Best val loss: 1.15851E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.2_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.54821E+00  - Val Loss: 1.16558E+00\n",
      "Epoch 200/250  - Train Loss: 1.07940E+00  - Val Loss: 1.15704E+00\n",
      "Best val loss: 1.15604E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.2_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.51652E+00  - Val Loss: 1.16084E+00\n",
      "Epoch 200/250  - Train Loss: 1.06539E+00  - Val Loss: 1.15508E+00\n",
      "Best val loss: 1.15416E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.2_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 35\n",
      "Best val loss: 1.13498E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.2_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-02\n",
      "                                lambda_l2       =1e-05\n",
      "                                dropout         =3e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.49536E+00  - Val Loss: 1.15521E+00\n",
      "Epoch 200/250  - Train Loss: 1.05916E+00  - Val Loss: 1.15295E+00\n",
      "Early stopping at epoch 213\n",
      "Best val loss: 1.15269E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.3_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.52426E+00  - Val Loss: 1.16357E+00\n",
      "Epoch 200/250  - Train Loss: 1.08042E+00  - Val Loss: 1.15943E+00\n",
      "Best val loss: 1.15850E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.3_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.54678E+00  - Val Loss: 1.16540E+00\n",
      "Epoch 200/250  - Train Loss: 1.07919E+00  - Val Loss: 1.15696E+00\n",
      "Best val loss: 1.15595E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.3_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.51696E+00  - Val Loss: 1.16088E+00\n",
      "Epoch 200/250  - Train Loss: 1.06563E+00  - Val Loss: 1.15511E+00\n",
      "Best val loss: 1.15417E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.3_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 35\n",
      "Best val loss: 1.13498E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.3_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-02\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.49634E+00  - Val Loss: 1.15525E+00\n",
      "Epoch 200/250  - Train Loss: 1.05986E+00  - Val Loss: 1.15278E+00\n",
      "Early stopping at epoch 221\n",
      "Best val loss: 1.15265E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.0_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.52488E+00  - Val Loss: 1.16337E+00\n",
      "Epoch 200/250  - Train Loss: 1.08089E+00  - Val Loss: 1.15951E+00\n",
      "Best val loss: 1.15856E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.0_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.54946E+00  - Val Loss: 1.16565E+00\n",
      "Epoch 200/250  - Train Loss: 1.07969E+00  - Val Loss: 1.15711E+00\n",
      "Best val loss: 1.15620E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.0_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.51657E+00  - Val Loss: 1.16067E+00\n",
      "Epoch 200/250  - Train Loss: 1.06514E+00  - Val Loss: 1.15508E+00\n",
      "Best val loss: 1.15417E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.0_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 34\n",
      "Best val loss: 1.13498E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.0_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-02\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.49622E+00  - Val Loss: 1.15524E+00\n",
      "Epoch 200/250  - Train Loss: 1.05972E+00  - Val Loss: 1.15282E+00\n",
      "Early stopping at epoch 218\n",
      "Best val loss: 1.15269E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.1_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.52481E+00  - Val Loss: 1.16345E+00\n",
      "Epoch 200/250  - Train Loss: 1.08085E+00  - Val Loss: 1.15949E+00\n",
      "Best val loss: 1.15854E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.1_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.54964E+00  - Val Loss: 1.16543E+00\n",
      "Epoch 200/250  - Train Loss: 1.07980E+00  - Val Loss: 1.15708E+00\n",
      "Best val loss: 1.15614E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.1_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.51682E+00  - Val Loss: 1.16086E+00\n",
      "Epoch 200/250  - Train Loss: 1.06540E+00  - Val Loss: 1.15507E+00\n",
      "Best val loss: 1.15416E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.1_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 35\n",
      "Best val loss: 1.13498E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.1_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-02\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.49610E+00  - Val Loss: 1.15523E+00\n",
      "Epoch 200/250  - Train Loss: 1.05960E+00  - Val Loss: 1.15290E+00\n",
      "Early stopping at epoch 216\n",
      "Best val loss: 1.15267E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.2_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.52491E+00  - Val Loss: 1.16352E+00\n",
      "Epoch 200/250  - Train Loss: 1.08081E+00  - Val Loss: 1.15947E+00\n",
      "Best val loss: 1.15853E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.2_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.54891E+00  - Val Loss: 1.16563E+00\n",
      "Epoch 200/250  - Train Loss: 1.07966E+00  - Val Loss: 1.15705E+00\n",
      "Best val loss: 1.15604E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.2_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.51722E+00  - Val Loss: 1.16085E+00\n",
      "Epoch 200/250  - Train Loss: 1.06571E+00  - Val Loss: 1.15509E+00\n",
      "Best val loss: 1.15417E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.2_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 35\n",
      "Best val loss: 1.13498E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.2_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-02\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =3e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.49603E+00  - Val Loss: 1.15522E+00\n",
      "Epoch 200/250  - Train Loss: 1.05947E+00  - Val Loss: 1.15298E+00\n",
      "Early stopping at epoch 213\n",
      "Best val loss: 1.15270E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.3_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.52498E+00  - Val Loss: 1.16359E+00\n",
      "Epoch 200/250  - Train Loss: 1.08075E+00  - Val Loss: 1.15945E+00\n",
      "Best val loss: 1.15852E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.3_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.54747E+00  - Val Loss: 1.16546E+00\n",
      "Epoch 200/250  - Train Loss: 1.07946E+00  - Val Loss: 1.15697E+00\n",
      "Best val loss: 1.15593E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.3_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.51767E+00  - Val Loss: 1.16089E+00\n",
      "Epoch 200/250  - Train Loss: 1.06596E+00  - Val Loss: 1.15512E+00\n",
      "Best val loss: 1.15417E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.3_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 35\n",
      "Best val loss: 1.13498E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.3_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-02\n",
      "                                lambda_l2       =1e-03\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.50313E+00  - Val Loss: 1.15534E+00\n",
      "Epoch 200/250  - Train Loss: 1.06291E+00  - Val Loss: 1.15288E+00\n",
      "Early stopping at epoch 217\n",
      "Best val loss: 1.15269E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.0_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.53213E+00  - Val Loss: 1.16367E+00\n",
      "Epoch 200/250  - Train Loss: 1.08418E+00  - Val Loss: 1.15976E+00\n",
      "Best val loss: 1.15870E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.0_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.55643E+00  - Val Loss: 1.16615E+00\n",
      "Epoch 200/250  - Train Loss: 1.08231E+00  - Val Loss: 1.15725E+00\n",
      "Best val loss: 1.15617E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.0_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.52369E+00  - Val Loss: 1.16069E+00\n",
      "Epoch 200/250  - Train Loss: 1.06853E+00  - Val Loss: 1.15520E+00\n",
      "Best val loss: 1.15422E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.0_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 34\n",
      "Best val loss: 1.13498E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.0_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-02\n",
      "                                lambda_l2       =1e-03\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.50301E+00  - Val Loss: 1.15533E+00\n",
      "Epoch 200/250  - Train Loss: 1.06278E+00  - Val Loss: 1.15293E+00\n",
      "Early stopping at epoch 216\n",
      "Best val loss: 1.15269E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.1_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.53207E+00  - Val Loss: 1.16374E+00\n",
      "Epoch 200/250  - Train Loss: 1.08414E+00  - Val Loss: 1.15974E+00\n",
      "Best val loss: 1.15870E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.1_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.55657E+00  - Val Loss: 1.16593E+00\n",
      "Epoch 200/250  - Train Loss: 1.08242E+00  - Val Loss: 1.15720E+00\n",
      "Best val loss: 1.15608E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.1_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.52388E+00  - Val Loss: 1.16088E+00\n",
      "Epoch 200/250  - Train Loss: 1.06878E+00  - Val Loss: 1.15519E+00\n",
      "Best val loss: 1.15422E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.1_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 35\n",
      "Best val loss: 1.13498E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.1_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-02\n",
      "                                lambda_l2       =1e-03\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.50290E+00  - Val Loss: 1.15532E+00\n",
      "Epoch 200/250  - Train Loss: 1.06266E+00  - Val Loss: 1.15298E+00\n",
      "Early stopping at epoch 213\n",
      "Best val loss: 1.15271E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.2_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.53214E+00  - Val Loss: 1.16381E+00\n",
      "Epoch 200/250  - Train Loss: 1.08410E+00  - Val Loss: 1.15972E+00\n",
      "Best val loss: 1.15870E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.2_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.55584E+00  - Val Loss: 1.16617E+00\n",
      "Epoch 200/250  - Train Loss: 1.08224E+00  - Val Loss: 1.15715E+00\n",
      "Best val loss: 1.15590E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.2_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.52426E+00  - Val Loss: 1.16094E+00\n",
      "Epoch 200/250  - Train Loss: 1.06907E+00  - Val Loss: 1.15520E+00\n",
      "Best val loss: 1.15422E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.2_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 35\n",
      "Best val loss: 1.13498E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.2_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-02\n",
      "                                lambda_l2       =1e-03\n",
      "                                dropout         =3e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.50282E+00  - Val Loss: 1.15530E+00\n",
      "Epoch 200/250  - Train Loss: 1.06254E+00  - Val Loss: 1.15306E+00\n",
      "Early stopping at epoch 208\n",
      "Best val loss: 1.15275E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.3_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.53217E+00  - Val Loss: 1.16387E+00\n",
      "Epoch 200/250  - Train Loss: 1.08404E+00  - Val Loss: 1.15970E+00\n",
      "Best val loss: 1.15870E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.3_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.55439E+00  - Val Loss: 1.16604E+00\n",
      "Epoch 200/250  - Train Loss: 1.08201E+00  - Val Loss: 1.15703E+00\n",
      "Best val loss: 1.15574E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.3_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.52471E+00  - Val Loss: 1.16101E+00\n",
      "Epoch 200/250  - Train Loss: 1.06931E+00  - Val Loss: 1.15522E+00\n",
      "Best val loss: 1.15423E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.3_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 35\n",
      "Best val loss: 1.13498E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.3_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-02\n",
      "                                lambda_l2       =1e-02\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.56878E+00  - Val Loss: 1.15611E+00\n",
      "Early stopping at epoch 193\n",
      "Best val loss: 1.15312E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.0_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.60127E+00  - Val Loss: 1.16661E+00\n",
      "Epoch 200/250  - Train Loss: 1.11309E+00  - Val Loss: 1.16213E+00\n",
      "Best val loss: 1.16057E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.0_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.62082E+00  - Val Loss: 1.17130E+00\n",
      "Epoch 200/250  - Train Loss: 1.10454E+00  - Val Loss: 1.15716E+00\n",
      "Best val loss: 1.15527E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.0_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.59272E+00  - Val Loss: 1.16173E+00\n",
      "Epoch 200/250  - Train Loss: 1.09841E+00  - Val Loss: 1.15635E+00\n",
      "Best val loss: 1.15454E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.0_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 35\n",
      "Best val loss: 1.13498E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.0_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-02\n",
      "                                lambda_l2       =1e-02\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.56865E+00  - Val Loss: 1.15609E+00\n",
      "Early stopping at epoch 190\n",
      "Best val loss: 1.15316E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.1_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.60125E+00  - Val Loss: 1.16661E+00\n",
      "Epoch 200/250  - Train Loss: 1.11309E+00  - Val Loss: 1.16213E+00\n",
      "Best val loss: 1.16058E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.1_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.62169E+00  - Val Loss: 1.17126E+00\n",
      "Epoch 200/250  - Train Loss: 1.10509E+00  - Val Loss: 1.15722E+00\n",
      "Best val loss: 1.15536E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.1_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.59276E+00  - Val Loss: 1.16179E+00\n",
      "Epoch 200/250  - Train Loss: 1.09863E+00  - Val Loss: 1.15633E+00\n",
      "Best val loss: 1.15455E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.1_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 35\n",
      "Best val loss: 1.13498E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.1_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-02\n",
      "                                lambda_l2       =1e-02\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.56850E+00  - Val Loss: 1.15607E+00\n",
      "Early stopping at epoch 187\n",
      "Best val loss: 1.15316E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.2_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.60129E+00  - Val Loss: 1.16660E+00\n",
      "Epoch 200/250  - Train Loss: 1.11311E+00  - Val Loss: 1.16213E+00\n",
      "Best val loss: 1.16059E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.2_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.62124E+00  - Val Loss: 1.17116E+00\n",
      "Epoch 200/250  - Train Loss: 1.10517E+00  - Val Loss: 1.15712E+00\n",
      "Best val loss: 1.15540E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.2_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.59290E+00  - Val Loss: 1.16175E+00\n",
      "Epoch 200/250  - Train Loss: 1.09885E+00  - Val Loss: 1.15634E+00\n",
      "Best val loss: 1.15456E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.2_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 35\n",
      "Best val loss: 1.13498E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.2_lr0.0001_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-02\n",
      "                                lambda_l2       =1e-02\n",
      "                                dropout         =3e-01\n",
      "                                learning_rate   =1e-04\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.56837E+00  - Val Loss: 1.15603E+00\n",
      "Early stopping at epoch 186\n",
      "Best val loss: 1.15318E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.3_lr0.0001_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.60128E+00  - Val Loss: 1.16658E+00\n",
      "Epoch 200/250  - Train Loss: 1.11314E+00  - Val Loss: 1.16214E+00\n",
      "Best val loss: 1.16060E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.3_lr0.0001_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.61982E+00  - Val Loss: 1.17084E+00\n",
      "Epoch 200/250  - Train Loss: 1.10535E+00  - Val Loss: 1.15700E+00\n",
      "Best val loss: 1.15544E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.3_lr0.0001_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.59308E+00  - Val Loss: 1.16180E+00\n",
      "Epoch 200/250  - Train Loss: 1.09904E+00  - Val Loss: 1.15635E+00\n",
      "Best val loss: 1.15457E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.3_lr0.0001_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 35\n",
      "Best val loss: 1.13498E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.3_lr0.0001_w16_d5_run5.pth\n"
     ]
    }
   ],
   "source": [
    "# # current runtime: 9h 50\n",
    "# for lambda_l1 in l1_space:\n",
    "#     for lambda_l2 in l2_space:\n",
    "#         for dropout in dropout_space:\n",
    "#             for learning_rate in learning_rate_space:\n",
    "#                 for hidden_depth in depth_space:\n",
    "#                     for hidden_width in width_space:\n",
    "#                         print(f\"\"\"Training model for year '{year}...: \n",
    "#                                 lambda_l1       ={lambda_l1:.0e}\n",
    "#                                 lambda_l2       ={lambda_l2:.0e}\n",
    "#                                 dropout         ={dropout:.0e}\n",
    "#                                 learning_rate   ={learning_rate:.0e}\n",
    "#                                 hidden_depth    ={hidden_depth}\n",
    "#                                 hidden_width    ={hidden_width}\"\"\")\n",
    "#                         for run in range(n_runs):\n",
    "#                             print(f\"Run {run+1} of {n_runs}\")\n",
    "#                             seed = 42+run\n",
    "#                             np.random.seed(seed)\n",
    "#                             torch.manual_seed(seed)\n",
    "#                             # Initialize the model\n",
    "#                             input_dim = X_train[year].shape[1]\n",
    "#                             name = f'l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}'\n",
    "#                             models_21[name] = MLPModel(input_dim, depth=hidden_depth, width=hidden_width, dropout=dropout, activation=activation_fun).to(device)\n",
    "#                             optimizer = torch.optim.Adam(models_21[name].parameters(), lr=learning_rate)\n",
    "#                             train = MLPdataset(X_train[year], y_train[year])\n",
    "#                             val = MLPdataset(X_val[year], y_val[year])\n",
    "#                             best_models_reg[name], history_reg[name] = train_mlp(train,          \n",
    "#                                                             val,\n",
    "#                                                             models_21[name],\n",
    "#                                                             criterion,\n",
    "#                                                             epochs,\n",
    "#                                                             patience,\n",
    "#                                                             print_freq,\n",
    "#                                                             device,\n",
    "#                                                             optimizer,\n",
    "#                                                             lambda_l1=lambda_l1,\n",
    "#                                                             lambda_l2=lambda_l2,\n",
    "#                                                             batch_size=batch_size,\n",
    "#                                                             shuffle_train=True,\n",
    "#                                                             shuffle_val=False,\n",
    "#                                                             save_path=f'models/hyperparam_test/mlp_y{year}_l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}.pth'\n",
    "#                                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten_history(history_reg, \n",
    "#                 save_csv='models/hyperparam_test/history/history_reg.csv')\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training model with most data on multiple parameters\n",
    "# l1_space = [0.0, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "# l2_space = [0.0, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "# dropout_space = [0.0, 0.1, 0.2, 0.3] \n",
    "# learning_rate_space = [learning_rate] \n",
    "# depth_space = [5] \n",
    "# width_space = [16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3000x2400 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # DataLoader\n",
    "# val_ds     = MLPdataset(X_val[year], y_val[year])\n",
    "# val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # build (n_drop, n_l1, n_l2) mean-loss array\n",
    "# n_l1   = len(l1_space)\n",
    "# n_l2   = len(l2_space)\n",
    "# n_drop = len(dropout_space)\n",
    "\n",
    "# losses = np.zeros((n_drop, n_l1, n_l2), dtype=float)\n",
    "\n",
    "# for di, drop in enumerate(dropout_space):\n",
    "#     for i, l1 in enumerate(l1_space):\n",
    "#         for j, l2 in enumerate(l2_space):\n",
    "#             run_losses = []\n",
    "#             # for each seed, load & eval\n",
    "#             for run in range(n_runs):\n",
    "#                 # since depth_space & width_space each have one entry, \n",
    "#                 # we can just index 0 here — but this will generalize\n",
    "#                 tmp_losses = []\n",
    "#                 for d in depth_space:\n",
    "#                     for w in width_space:\n",
    "#                         m = load_model(w, d, run, l1, l2, drop, learning_rate, )\n",
    "#                         with torch.no_grad():\n",
    "#                             batch = [criterion(m(x.to(device)), y.to(device)).item()\n",
    "#                                      for x,y in val_loader]\n",
    "#                         tmp_losses.append(np.mean(batch))\n",
    "#                 run_losses.append(np.mean(tmp_losses))\n",
    "#             # now average over runs\n",
    "#             losses[di, i, j] = np.mean(run_losses)\n",
    "\n",
    "\n",
    "# # global color‐scale\n",
    "# vmin    = losses.min()\n",
    "# vmax    = losses.max()\n",
    "# vcenter = vmin + 0.5*(vmax - vmin)\n",
    "# norm    = TwoSlopeNorm(vmin=vmin, vcenter=vcenter, vmax=vmax)\n",
    "# cmap    = 'viridis'\n",
    "\n",
    "# # layout: up to 2 columns\n",
    "# ncols = min(2, n_drop)\n",
    "# nrows = math.ceil(n_drop / ncols)\n",
    "\n",
    "# fig, axes = plt.subplots(nrows, ncols,\n",
    "#                          figsize=(5*ncols, 4*nrows),\n",
    "#                          squeeze=False)\n",
    "# axes_flat = axes.flatten()\n",
    "\n",
    "# for idx, drop in enumerate(dropout_space):\n",
    "#     ax = axes_flat[idx]\n",
    "#     im = ax.imshow(\n",
    "#         losses[idx],\n",
    "#         aspect='auto',\n",
    "#         cmap=cmap,\n",
    "#         norm=norm,\n",
    "#         origin='lower',\n",
    "#     )\n",
    "\n",
    "#     # y = l1, x = l2\n",
    "#     ax.set_yticks(np.arange(n_l1))\n",
    "#     ax.set_yticklabels(l1_space)\n",
    "#     ax.set_xticks(np.arange(n_l2))\n",
    "#     ax.set_xticklabels(l2_space)\n",
    "\n",
    "#     # only bottom row shows x‐labels\n",
    "#     if idx < (nrows-1)*ncols:\n",
    "#         ax.tick_params(labelbottom=False)\n",
    "#     else:\n",
    "#         ax.set_xlabel(r'$\\ell_2$')\n",
    "\n",
    "#     # only first‐column shows y‐labels\n",
    "#     if idx % ncols != 0:\n",
    "#         ax.tick_params(labelleft=False)\n",
    "#     else:\n",
    "#         ax.set_ylabel(r'$\\ell_1$')\n",
    "\n",
    "#     ax.set_title(f\"dropout = {drop:.2f}\")\n",
    "\n",
    "# # clear unused\n",
    "# for ax in axes_flat[n_drop:]:\n",
    "#     fig.delaxes(ax)\n",
    "\n",
    "# # colorbar at the right edge\n",
    "# fig.subplots_adjust(right=0.88)\n",
    "# cax = fig.add_axes([0.90, 0.15, 0.02, 0.7])  # [left, bottom, width, height]\n",
    "# cbar = fig.colorbar(im, cax=cax)\n",
    "# cbar.set_label('Average Validation Loss (MSE)')\n",
    "\n",
    "# plt.savefig('figs/l1_l2_dropout_loss.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_4_'></a>[Pyramid](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training model with most data on multiple parameters\n",
    "# l1_space = [0.0, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "# l2_space = [0.0, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "# dropout_space = [0.0, 0.1, 0.2, 0.3] \n",
    "# learning_rate_space = [learning_rate]\n",
    "# depth_space = None\n",
    "# width_space = [[32, 16, 8,4]]\n",
    "\n",
    "# best_models_reg_pyramid = {}\n",
    "# history_reg_pyramid = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for year '21...: \n",
      "                            lambda_l1       =0e+00\n",
      "                            lambda_l2       =0e+00\n",
      "                            dropout         =0e+00\n",
      "                            learning_rate   =1e-04\n",
      "                            hidden_depth    =4\n",
      "                            hidden_width    =[32, 16, 8, 4]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.23853E-01  - Val Loss: 1.12864E+00\n",
      "Epoch 200/250  - Train Loss: 8.78993E-01  - Val Loss: 1.11410E+00\n",
      "Early stopping at epoch 215\n",
      "Best val loss: 1.11345E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.0_lr0.0001_w[32, 16, 8, 4]_d4_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.52443E-01  - Val Loss: 1.14036E+00\n",
      "Epoch 200/250  - Train Loss: 9.22673E-01  - Val Loss: 1.11758E+00\n",
      "Best val loss: 1.11152E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.0_lr0.0001_w[32, 16, 8, 4]_d4_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.32442E-01  - Val Loss: 1.13936E+00\n",
      "Epoch 200/250  - Train Loss: 8.83052E-01  - Val Loss: 1.12720E+00\n",
      "Early stopping at epoch 201\n",
      "Best val loss: 1.12416E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.0_lr0.0001_w[32, 16, 8, 4]_d4_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 61\n",
      "Best val loss: 1.13196E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.0_lr0.0001_w[32, 16, 8, 4]_d4_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.29060E-01  - Val Loss: 1.12753E+00\n",
      "Early stopping at epoch 187\n",
      "Best val loss: 1.11924E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.0_lr0.0001_w[32, 16, 8, 4]_d4_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =0e+00\n",
      "                            lambda_l2       =0e+00\n",
      "                            dropout         =1e-01\n",
      "                            learning_rate   =1e-04\n",
      "                            hidden_depth    =4\n",
      "                            hidden_width    =[32, 16, 8, 4]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.50324E-01  - Val Loss: 1.13645E+00\n",
      "Epoch 200/250  - Train Loss: 9.30144E-01  - Val Loss: 1.12315E+00\n",
      "Best val loss: 1.11877E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.1_lr0.0001_w[32, 16, 8, 4]_d4_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.67817E-01  - Val Loss: 1.13741E+00\n",
      "Epoch 200/250  - Train Loss: 9.51398E-01  - Val Loss: 1.12815E+00\n",
      "Best val loss: 1.12435E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.1_lr0.0001_w[32, 16, 8, 4]_d4_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.53952E-01  - Val Loss: 1.13935E+00\n",
      "Epoch 200/250  - Train Loss: 9.37936E-01  - Val Loss: 1.12243E+00\n",
      "Best val loss: 1.11942E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.1_lr0.0001_w[32, 16, 8, 4]_d4_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 61\n",
      "Best val loss: 1.13188E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.1_lr0.0001_w[32, 16, 8, 4]_d4_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.54152E-01  - Val Loss: 1.13688E+00\n",
      "Epoch 200/250  - Train Loss: 9.33232E-01  - Val Loss: 1.12422E+00\n",
      "Best val loss: 1.12273E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.1_lr0.0001_w[32, 16, 8, 4]_d4_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =0e+00\n",
      "                            lambda_l2       =0e+00\n",
      "                            dropout         =2e-01\n",
      "                            learning_rate   =1e-04\n",
      "                            hidden_depth    =4\n",
      "                            hidden_width    =[32, 16, 8, 4]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.63191E-01  - Val Loss: 1.14088E+00\n",
      "Epoch 200/250  - Train Loss: 9.48092E-01  - Val Loss: 1.13549E+00\n",
      "Best val loss: 1.13206E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.2_lr0.0001_w[32, 16, 8, 4]_d4_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.73608E-01  - Val Loss: 1.13785E+00\n",
      "Early stopping at epoch 186\n",
      "Best val loss: 1.13186E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.2_lr0.0001_w[32, 16, 8, 4]_d4_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.63810E-01  - Val Loss: 1.13961E+00\n",
      "Epoch 200/250  - Train Loss: 9.54773E-01  - Val Loss: 1.12809E+00\n",
      "Best val loss: 1.12456E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.2_lr0.0001_w[32, 16, 8, 4]_d4_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 62\n",
      "Best val loss: 1.13198E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.2_lr0.0001_w[32, 16, 8, 4]_d4_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.64864E-01  - Val Loss: 1.14225E+00\n",
      "Epoch 200/250  - Train Loss: 9.51076E-01  - Val Loss: 1.13245E+00\n",
      "Best val loss: 1.13059E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.2_lr0.0001_w[32, 16, 8, 4]_d4_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =0e+00\n",
      "                            lambda_l2       =0e+00\n",
      "                            dropout         =3e-01\n",
      "                            learning_rate   =1e-04\n",
      "                            hidden_depth    =4\n",
      "                            hidden_width    =[32, 16, 8, 4]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.74640E-01  - Val Loss: 1.14298E+00\n",
      "Epoch 200/250  - Train Loss: 9.62969E-01  - Val Loss: 1.14016E+00\n",
      "Early stopping at epoch 212\n",
      "Best val loss: 1.14000E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.3_lr0.0001_w[32, 16, 8, 4]_d4_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.84943E-01  - Val Loss: 1.14146E+00\n",
      "Early stopping at epoch 187\n",
      "Best val loss: 1.13576E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.3_lr0.0001_w[32, 16, 8, 4]_d4_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.70864E-01  - Val Loss: 1.14111E+00\n",
      "Epoch 200/250  - Train Loss: 9.63999E-01  - Val Loss: 1.13468E+00\n",
      "Best val loss: 1.13255E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.3_lr0.0001_w[32, 16, 8, 4]_d4_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 66\n",
      "Best val loss: 1.13215E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.3_lr0.0001_w[32, 16, 8, 4]_d4_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.74070E-01  - Val Loss: 1.14687E+00\n",
      "Epoch 200/250  - Train Loss: 9.63955E-01  - Val Loss: 1.13936E+00\n",
      "Best val loss: 1.13729E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.3_lr0.0001_w[32, 16, 8, 4]_d4_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =0e+00\n",
      "                            lambda_l2       =1e-05\n",
      "                            dropout         =0e+00\n",
      "                            learning_rate   =1e-04\n",
      "                            hidden_depth    =4\n",
      "                            hidden_width    =[32, 16, 8, 4]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.25030E-01  - Val Loss: 1.12919E+00\n",
      "Epoch 200/250  - Train Loss: 8.81537E-01  - Val Loss: 1.11456E+00\n",
      "Early stopping at epoch 206\n",
      "Best val loss: 1.11399E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.0_lr0.0001_w[32, 16, 8, 4]_d4_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.53031E-01  - Val Loss: 1.14002E+00\n",
      "Epoch 200/250  - Train Loss: 9.08782E-01  - Val Loss: 1.11917E+00\n",
      "Best val loss: 1.11133E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.0_lr0.0001_w[32, 16, 8, 4]_d4_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.33197E-01  - Val Loss: 1.13897E+00\n",
      "Epoch 200/250  - Train Loss: 8.83664E-01  - Val Loss: 1.12184E+00\n",
      "Early stopping at epoch 206\n",
      "Best val loss: 1.11958E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.0_lr0.0001_w[32, 16, 8, 4]_d4_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 61\n",
      "Best val loss: 1.13195E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.0_lr0.0001_w[32, 16, 8, 4]_d4_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.29214E-01  - Val Loss: 1.12617E+00\n",
      "Epoch 200/250  - Train Loss: 8.76311E-01  - Val Loss: 1.11514E+00\n",
      "Early stopping at epoch 204\n",
      "Best val loss: 1.11201E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.0_lr0.0001_w[32, 16, 8, 4]_d4_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =0e+00\n",
      "                            lambda_l2       =1e-05\n",
      "                            dropout         =1e-01\n",
      "                            learning_rate   =1e-04\n",
      "                            hidden_depth    =4\n",
      "                            hidden_width    =[32, 16, 8, 4]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.50727E-01  - Val Loss: 1.13587E+00\n",
      "Epoch 200/250  - Train Loss: 9.30560E-01  - Val Loss: 1.12163E+00\n",
      "Best val loss: 1.11727E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.1_lr0.0001_w[32, 16, 8, 4]_d4_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.67876E-01  - Val Loss: 1.13720E+00\n",
      "Epoch 200/250  - Train Loss: 9.51454E-01  - Val Loss: 1.12737E+00\n",
      "Best val loss: 1.12344E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.1_lr0.0001_w[32, 16, 8, 4]_d4_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.54409E-01  - Val Loss: 1.13940E+00\n",
      "Epoch 200/250  - Train Loss: 9.38612E-01  - Val Loss: 1.12358E+00\n",
      "Best val loss: 1.11995E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.1_lr0.0001_w[32, 16, 8, 4]_d4_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 61\n",
      "Best val loss: 1.13189E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.1_lr0.0001_w[32, 16, 8, 4]_d4_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.54274E-01  - Val Loss: 1.13666E+00\n",
      "Epoch 200/250  - Train Loss: 9.33362E-01  - Val Loss: 1.12357E+00\n",
      "Best val loss: 1.12169E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.1_lr0.0001_w[32, 16, 8, 4]_d4_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =0e+00\n",
      "                            lambda_l2       =1e-05\n",
      "                            dropout         =2e-01\n",
      "                            learning_rate   =1e-04\n",
      "                            hidden_depth    =4\n",
      "                            hidden_width    =[32, 16, 8, 4]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.63477E-01  - Val Loss: 1.14077E+00\n",
      "Epoch 200/250  - Train Loss: 9.48398E-01  - Val Loss: 1.13558E+00\n",
      "Best val loss: 1.13182E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.2_lr0.0001_w[32, 16, 8, 4]_d4_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.73642E-01  - Val Loss: 1.13771E+00\n",
      "Early stopping at epoch 186\n",
      "Best val loss: 1.13190E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.2_lr0.0001_w[32, 16, 8, 4]_d4_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.63996E-01  - Val Loss: 1.13951E+00\n",
      "Epoch 200/250  - Train Loss: 9.55092E-01  - Val Loss: 1.12793E+00\n",
      "Best val loss: 1.12475E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.2_lr0.0001_w[32, 16, 8, 4]_d4_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 62\n",
      "Best val loss: 1.13198E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.2_lr0.0001_w[32, 16, 8, 4]_d4_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.65117E-01  - Val Loss: 1.14244E+00\n",
      "Epoch 200/250  - Train Loss: 9.51273E-01  - Val Loss: 1.13197E+00\n",
      "Best val loss: 1.13027E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.2_lr0.0001_w[32, 16, 8, 4]_d4_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =0e+00\n",
      "                            lambda_l2       =1e-05\n",
      "                            dropout         =3e-01\n",
      "                            learning_rate   =1e-04\n",
      "                            hidden_depth    =4\n",
      "                            hidden_width    =[32, 16, 8, 4]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.74824E-01  - Val Loss: 1.14294E+00\n",
      "Epoch 200/250  - Train Loss: 9.63033E-01  - Val Loss: 1.13993E+00\n",
      "Early stopping at epoch 212\n",
      "Best val loss: 1.13977E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.3_lr0.0001_w[32, 16, 8, 4]_d4_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.84996E-01  - Val Loss: 1.14121E+00\n",
      "Early stopping at epoch 187\n",
      "Best val loss: 1.13550E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.3_lr0.0001_w[32, 16, 8, 4]_d4_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.70999E-01  - Val Loss: 1.14098E+00\n",
      "Epoch 200/250  - Train Loss: 9.64185E-01  - Val Loss: 1.13449E+00\n",
      "Best val loss: 1.13256E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.3_lr0.0001_w[32, 16, 8, 4]_d4_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 66\n",
      "Best val loss: 1.13215E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.3_lr0.0001_w[32, 16, 8, 4]_d4_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.74303E-01  - Val Loss: 1.14687E+00\n",
      "Epoch 200/250  - Train Loss: 9.64164E-01  - Val Loss: 1.13919E+00\n",
      "Best val loss: 1.13718E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.3_lr0.0001_w[32, 16, 8, 4]_d4_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =0e+00\n",
      "                            lambda_l2       =1e-04\n",
      "                            dropout         =0e+00\n",
      "                            learning_rate   =1e-04\n",
      "                            hidden_depth    =4\n",
      "                            hidden_width    =[32, 16, 8, 4]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.30366E-01  - Val Loss: 1.12894E+00\n",
      "Epoch 200/250  - Train Loss: 8.86738E-01  - Val Loss: 1.11226E+00\n",
      "Early stopping at epoch 206\n",
      "Best val loss: 1.11108E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.0_lr0.0001_w[32, 16, 8, 4]_d4_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.55734E-01  - Val Loss: 1.14071E+00\n",
      "Epoch 200/250  - Train Loss: 9.13669E-01  - Val Loss: 1.11983E+00\n",
      "Best val loss: 1.11168E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.0_lr0.0001_w[32, 16, 8, 4]_d4_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.36387E-01  - Val Loss: 1.14004E+00\n",
      "Epoch 200/250  - Train Loss: 8.89070E-01  - Val Loss: 1.12451E+00\n",
      "Early stopping at epoch 245\n",
      "Best val loss: 1.12184E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.0_lr0.0001_w[32, 16, 8, 4]_d4_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 62\n",
      "Best val loss: 1.13189E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.0_lr0.0001_w[32, 16, 8, 4]_d4_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.30763E-01  - Val Loss: 1.12576E+00\n",
      "Early stopping at epoch 187\n",
      "Best val loss: 1.11348E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.0_lr0.0001_w[32, 16, 8, 4]_d4_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =0e+00\n",
      "                            lambda_l2       =1e-04\n",
      "                            dropout         =1e-01\n",
      "                            learning_rate   =1e-04\n",
      "                            hidden_depth    =4\n",
      "                            hidden_width    =[32, 16, 8, 4]\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 80\n",
      "Best val loss: 1.13808E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.1_lr0.0001_w[32, 16, 8, 4]_d4_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.70293E-01  - Val Loss: 1.13711E+00\n",
      "Epoch 200/250  - Train Loss: 9.54591E-01  - Val Loss: 1.12728E+00\n",
      "Best val loss: 1.12332E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.1_lr0.0001_w[32, 16, 8, 4]_d4_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.56390E-01  - Val Loss: 1.13987E+00\n",
      "Epoch 200/250  - Train Loss: 9.40435E-01  - Val Loss: 1.12438E+00\n",
      "Best val loss: 1.12095E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.1_lr0.0001_w[32, 16, 8, 4]_d4_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 62\n",
      "Best val loss: 1.13185E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.1_lr0.0001_w[32, 16, 8, 4]_d4_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.56343E-01  - Val Loss: 1.13560E+00\n",
      "Epoch 200/250  - Train Loss: 9.35054E-01  - Val Loss: 1.12281E+00\n",
      "Best val loss: 1.12165E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.1_lr0.0001_w[32, 16, 8, 4]_d4_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =0e+00\n",
      "                            lambda_l2       =1e-04\n",
      "                            dropout         =2e-01\n",
      "                            learning_rate   =1e-04\n",
      "                            hidden_depth    =4\n",
      "                            hidden_width    =[32, 16, 8, 4]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.65446E-01  - Val Loss: 1.14060E+00\n",
      "Epoch 200/250  - Train Loss: 9.50439E-01  - Val Loss: 1.13369E+00\n",
      "Early stopping at epoch 239\n",
      "Best val loss: 1.13172E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.2_lr0.0001_w[32, 16, 8, 4]_d4_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.75965E-01  - Val Loss: 1.13782E+00\n",
      "Early stopping at epoch 186\n",
      "Best val loss: 1.13166E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.2_lr0.0001_w[32, 16, 8, 4]_d4_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.65815E-01  - Val Loss: 1.13984E+00\n",
      "Epoch 200/250  - Train Loss: 9.56516E-01  - Val Loss: 1.12822E+00\n",
      "Best val loss: 1.12501E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.2_lr0.0001_w[32, 16, 8, 4]_d4_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 64\n",
      "Best val loss: 1.13195E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.2_lr0.0001_w[32, 16, 8, 4]_d4_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.66885E-01  - Val Loss: 1.14227E+00\n",
      "Epoch 200/250  - Train Loss: 9.52730E-01  - Val Loss: 1.13101E+00\n",
      "Best val loss: 1.12918E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.2_lr0.0001_w[32, 16, 8, 4]_d4_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =0e+00\n",
      "                            lambda_l2       =1e-04\n",
      "                            dropout         =3e-01\n",
      "                            learning_rate   =1e-04\n",
      "                            hidden_depth    =4\n",
      "                            hidden_width    =[32, 16, 8, 4]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.76409E-01  - Val Loss: 1.14250E+00\n",
      "Epoch 200/250  - Train Loss: 9.64347E-01  - Val Loss: 1.13969E+00\n",
      "Early stopping at epoch 240\n",
      "Best val loss: 1.13869E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.3_lr0.0001_w[32, 16, 8, 4]_d4_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.86263E-01  - Val Loss: 1.14074E+00\n",
      "Early stopping at epoch 188\n",
      "Best val loss: 1.13512E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.3_lr0.0001_w[32, 16, 8, 4]_d4_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.72827E-01  - Val Loss: 1.14131E+00\n",
      "Epoch 200/250  - Train Loss: 9.65985E-01  - Val Loss: 1.13475E+00\n",
      "Best val loss: 1.13263E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.3_lr0.0001_w[32, 16, 8, 4]_d4_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 66\n",
      "Best val loss: 1.13213E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.3_lr0.0001_w[32, 16, 8, 4]_d4_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.75952E-01  - Val Loss: 1.14676E+00\n",
      "Epoch 200/250  - Train Loss: 9.65571E-01  - Val Loss: 1.13862E+00\n",
      "Best val loss: 1.13676E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.3_lr0.0001_w[32, 16, 8, 4]_d4_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =0e+00\n",
      "                            lambda_l2       =1e-03\n",
      "                            dropout         =0e+00\n",
      "                            learning_rate   =1e-04\n",
      "                            hidden_depth    =4\n",
      "                            hidden_width    =[32, 16, 8, 4]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.65298E-01  - Val Loss: 1.14783E+00\n",
      "Epoch 200/250  - Train Loss: 9.17078E-01  - Val Loss: 1.11805E+00\n",
      "Best val loss: 1.11287E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.0_lr0.0001_w[32, 16, 8, 4]_d4_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.74680E-01  - Val Loss: 1.14174E+00\n",
      "Epoch 200/250  - Train Loss: 9.42046E-01  - Val Loss: 1.11857E+00\n",
      "Best val loss: 1.10997E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.0_lr0.0001_w[32, 16, 8, 4]_d4_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.53416E-01  - Val Loss: 1.13927E+00\n",
      "Epoch 200/250  - Train Loss: 9.08885E-01  - Val Loss: 1.11942E+00\n",
      "Early stopping at epoch 250\n",
      "Best val loss: 1.11663E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.0_lr0.0001_w[32, 16, 8, 4]_d4_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 67\n",
      "Best val loss: 1.13137E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.0_lr0.0001_w[32, 16, 8, 4]_d4_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.49486E-01  - Val Loss: 1.12358E+00\n",
      "Early stopping at epoch 187\n",
      "Best val loss: 1.10579E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.0_lr0.0001_w[32, 16, 8, 4]_d4_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =0e+00\n",
      "                            lambda_l2       =1e-03\n",
      "                            dropout         =1e-01\n",
      "                            learning_rate   =1e-04\n",
      "                            hidden_depth    =4\n",
      "                            hidden_width    =[32, 16, 8, 4]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.73264E-01  - Val Loss: 1.14680E+00\n",
      "Epoch 200/250  - Train Loss: 9.52058E-01  - Val Loss: 1.13212E+00\n",
      "Best val loss: 1.11458E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.1_lr0.0001_w[32, 16, 8, 4]_d4_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.83873E-01  - Val Loss: 1.14016E+00\n",
      "Epoch 200/250  - Train Loss: 9.63244E-01  - Val Loss: 1.12378E+00\n",
      "Best val loss: 1.11916E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.1_lr0.0001_w[32, 16, 8, 4]_d4_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.67275E-01  - Val Loss: 1.13995E+00\n",
      "Epoch 200/250  - Train Loss: 9.48539E-01  - Val Loss: 1.12267E+00\n",
      "Best val loss: 1.11739E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.1_lr0.0001_w[32, 16, 8, 4]_d4_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 67\n",
      "Best val loss: 1.13130E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.1_lr0.0001_w[32, 16, 8, 4]_d4_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.68280E-01  - Val Loss: 1.13000E+00\n",
      "Epoch 200/250  - Train Loss: 9.44453E-01  - Val Loss: 1.11621E+00\n",
      "Early stopping at epoch 223\n",
      "Best val loss: 1.11485E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.1_lr0.0001_w[32, 16, 8, 4]_d4_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =0e+00\n",
      "                            lambda_l2       =1e-03\n",
      "                            dropout         =2e-01\n",
      "                            learning_rate   =1e-04\n",
      "                            hidden_depth    =4\n",
      "                            hidden_width    =[32, 16, 8, 4]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.76952E-01  - Val Loss: 1.14287E+00\n",
      "Epoch 200/250  - Train Loss: 9.61172E-01  - Val Loss: 1.13068E+00\n",
      "Best val loss: 1.12478E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.2_lr0.0001_w[32, 16, 8, 4]_d4_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.90591E-01  - Val Loss: 1.13970E+00\n",
      "Epoch 200/250  - Train Loss: 9.72508E-01  - Val Loss: 1.12910E+00\n",
      "Best val loss: 1.12556E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.2_lr0.0001_w[32, 16, 8, 4]_d4_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.75419E-01  - Val Loss: 1.13975E+00\n",
      "Epoch 200/250  - Train Loss: 9.63257E-01  - Val Loss: 1.12865E+00\n",
      "Early stopping at epoch 245\n",
      "Best val loss: 1.12550E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.2_lr0.0001_w[32, 16, 8, 4]_d4_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 70\n",
      "Best val loss: 1.13132E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.2_lr0.0001_w[32, 16, 8, 4]_d4_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.76660E-01  - Val Loss: 1.13915E+00\n",
      "Epoch 200/250  - Train Loss: 9.59952E-01  - Val Loss: 1.12857E+00\n",
      "Best val loss: 1.12591E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.2_lr0.0001_w[32, 16, 8, 4]_d4_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =0e+00\n",
      "                            lambda_l2       =1e-03\n",
      "                            dropout         =3e-01\n",
      "                            learning_rate   =1e-04\n",
      "                            hidden_depth    =4\n",
      "                            hidden_width    =[32, 16, 8, 4]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.84081E-01  - Val Loss: 1.14281E+00\n",
      "Epoch 200/250  - Train Loss: 9.70180E-01  - Val Loss: 1.13598E+00\n",
      "Best val loss: 1.13189E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.3_lr0.0001_w[32, 16, 8, 4]_d4_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.98624E-01  - Val Loss: 1.14140E+00\n",
      "Epoch 200/250  - Train Loss: 9.81955E-01  - Val Loss: 1.13287E+00\n",
      "Best val loss: 1.13147E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.3_lr0.0001_w[32, 16, 8, 4]_d4_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.82179E-01  - Val Loss: 1.14262E+00\n",
      "Epoch 200/250  - Train Loss: 9.73596E-01  - Val Loss: 1.13503E+00\n",
      "Best val loss: 1.13280E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.3_lr0.0001_w[32, 16, 8, 4]_d4_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 71\n",
      "Best val loss: 1.13200E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.3_lr0.0001_w[32, 16, 8, 4]_d4_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.84426E-01  - Val Loss: 1.14607E+00\n",
      "Epoch 200/250  - Train Loss: 9.71560E-01  - Val Loss: 1.13653E+00\n",
      "Best val loss: 1.13328E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.3_lr0.0001_w[32, 16, 8, 4]_d4_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =0e+00\n",
      "                            lambda_l2       =1e-02\n",
      "                            dropout         =0e+00\n",
      "                            learning_rate   =1e-04\n",
      "                            hidden_depth    =4\n",
      "                            hidden_width    =[32, 16, 8, 4]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.02127E+00  - Val Loss: 1.15523E+00\n",
      "Epoch 200/250  - Train Loss: 1.00407E+00  - Val Loss: 1.15262E+00\n",
      "Early stopping at epoch 209\n",
      "Best val loss: 1.15250E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.0_lr0.0001_w[32, 16, 8, 4]_d4_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.03286E+00  - Val Loss: 1.15021E+00\n",
      "Epoch 200/250  - Train Loss: 1.00295E+00  - Val Loss: 1.13199E+00\n",
      "Best val loss: 1.12660E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.0_lr0.0001_w[32, 16, 8, 4]_d4_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.02648E+00  - Val Loss: 1.15581E+00\n",
      "Epoch 200/250  - Train Loss: 1.00692E+00  - Val Loss: 1.15353E+00\n",
      "Early stopping at epoch 213\n",
      "Best val loss: 1.15330E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.0_lr0.0001_w[32, 16, 8, 4]_d4_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 80\n",
      "Best val loss: 1.13486E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.0_lr0.0001_w[32, 16, 8, 4]_d4_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.02459E+00  - Val Loss: 1.16055E+00\n",
      "Epoch 200/250  - Train Loss: 1.00487E+00  - Val Loss: 1.15346E+00\n",
      "Best val loss: 1.15288E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.0_lr0.0001_w[32, 16, 8, 4]_d4_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =0e+00\n",
      "                            lambda_l2       =1e-02\n",
      "                            dropout         =1e-01\n",
      "                            learning_rate   =1e-04\n",
      "                            hidden_depth    =4\n",
      "                            hidden_width    =[32, 16, 8, 4]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.02130E+00  - Val Loss: 1.15523E+00\n",
      "Epoch 200/250  - Train Loss: 1.00408E+00  - Val Loss: 1.15262E+00\n",
      "Early stopping at epoch 209\n",
      "Best val loss: 1.15250E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.1_lr0.0001_w[32, 16, 8, 4]_d4_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.03659E+00  - Val Loss: 1.15284E+00\n",
      "Epoch 200/250  - Train Loss: 1.00734E+00  - Val Loss: 1.13860E+00\n",
      "Best val loss: 1.13351E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.1_lr0.0001_w[32, 16, 8, 4]_d4_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.02613E+00  - Val Loss: 1.15600E+00\n",
      "Epoch 200/250  - Train Loss: 1.00648E+00  - Val Loss: 1.15340E+00\n",
      "Early stopping at epoch 213\n",
      "Best val loss: 1.15320E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.1_lr0.0001_w[32, 16, 8, 4]_d4_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 81\n",
      "Best val loss: 1.13487E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.1_lr0.0001_w[32, 16, 8, 4]_d4_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.02457E+00  - Val Loss: 1.16062E+00\n",
      "Epoch 200/250  - Train Loss: 1.00486E+00  - Val Loss: 1.15345E+00\n",
      "Best val loss: 1.15287E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.1_lr0.0001_w[32, 16, 8, 4]_d4_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =0e+00\n",
      "                            lambda_l2       =1e-02\n",
      "                            dropout         =2e-01\n",
      "                            learning_rate   =1e-04\n",
      "                            hidden_depth    =4\n",
      "                            hidden_width    =[32, 16, 8, 4]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.02132E+00  - Val Loss: 1.15524E+00\n",
      "Epoch 200/250  - Train Loss: 1.00409E+00  - Val Loss: 1.15262E+00\n",
      "Early stopping at epoch 209\n",
      "Best val loss: 1.15250E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.2_lr0.0001_w[32, 16, 8, 4]_d4_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.03893E+00  - Val Loss: 1.15885E+00\n",
      "Epoch 200/250  - Train Loss: 1.01031E+00  - Val Loss: 1.14347E+00\n",
      "Best val loss: 1.13936E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.2_lr0.0001_w[32, 16, 8, 4]_d4_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.02584E+00  - Val Loss: 1.15606E+00\n",
      "Epoch 200/250  - Train Loss: 1.00634E+00  - Val Loss: 1.15337E+00\n",
      "Early stopping at epoch 213\n",
      "Best val loss: 1.15317E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.2_lr0.0001_w[32, 16, 8, 4]_d4_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 81\n",
      "Best val loss: 1.13487E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.2_lr0.0001_w[32, 16, 8, 4]_d4_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.02452E+00  - Val Loss: 1.16065E+00\n",
      "Epoch 200/250  - Train Loss: 1.00485E+00  - Val Loss: 1.15344E+00\n",
      "Best val loss: 1.15286E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.2_lr0.0001_w[32, 16, 8, 4]_d4_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =0e+00\n",
      "                            lambda_l2       =1e-02\n",
      "                            dropout         =3e-01\n",
      "                            learning_rate   =1e-04\n",
      "                            hidden_depth    =4\n",
      "                            hidden_width    =[32, 16, 8, 4]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.02139E+00  - Val Loss: 1.15525E+00\n",
      "Epoch 200/250  - Train Loss: 1.00411E+00  - Val Loss: 1.15262E+00\n",
      "Early stopping at epoch 209\n",
      "Best val loss: 1.15250E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.3_lr0.0001_w[32, 16, 8, 4]_d4_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.04313E+00  - Val Loss: 1.16563E+00\n",
      "Epoch 200/250  - Train Loss: 1.01300E+00  - Val Loss: 1.14883E+00\n",
      "Best val loss: 1.14482E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.3_lr0.0001_w[32, 16, 8, 4]_d4_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.02583E+00  - Val Loss: 1.15616E+00\n",
      "Epoch 200/250  - Train Loss: 1.00625E+00  - Val Loss: 1.15336E+00\n",
      "Early stopping at epoch 213\n",
      "Best val loss: 1.15316E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.3_lr0.0001_w[32, 16, 8, 4]_d4_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 81\n",
      "Best val loss: 1.13484E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.3_lr0.0001_w[32, 16, 8, 4]_d4_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.02455E+00  - Val Loss: 1.16067E+00\n",
      "Epoch 200/250  - Train Loss: 1.00486E+00  - Val Loss: 1.15343E+00\n",
      "Best val loss: 1.15286E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.3_lr0.0001_w[32, 16, 8, 4]_d4_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-05\n",
      "                            lambda_l2       =0e+00\n",
      "                            dropout         =0e+00\n",
      "                            learning_rate   =1e-04\n",
      "                            hidden_depth    =4\n",
      "                            hidden_width    =[32, 16, 8, 4]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.30273E-01  - Val Loss: 1.12842E+00\n",
      "Epoch 200/250  - Train Loss: 8.86669E-01  - Val Loss: 1.11256E+00\n",
      "Early stopping at epoch 206\n",
      "Best val loss: 1.11136E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.0_lr0.0001_w[32, 16, 8, 4]_d4_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.57002E-01  - Val Loss: 1.14120E+00\n",
      "Epoch 200/250  - Train Loss: 9.13408E-01  - Val Loss: 1.11815E+00\n",
      "Best val loss: 1.10972E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.0_lr0.0001_w[32, 16, 8, 4]_d4_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.37374E-01  - Val Loss: 1.13978E+00\n",
      "Epoch 200/250  - Train Loss: 8.89953E-01  - Val Loss: 1.12127E+00\n",
      "Early stopping at epoch 245\n",
      "Best val loss: 1.11909E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.0_lr0.0001_w[32, 16, 8, 4]_d4_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 61\n",
      "Best val loss: 1.13197E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.0_lr0.0001_w[32, 16, 8, 4]_d4_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.31533E-01  - Val Loss: 1.12572E+00\n",
      "Early stopping at epoch 173\n",
      "Best val loss: 1.11383E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.0_lr0.0001_w[32, 16, 8, 4]_d4_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-05\n",
      "                            lambda_l2       =0e+00\n",
      "                            dropout         =1e-01\n",
      "                            learning_rate   =1e-04\n",
      "                            hidden_depth    =4\n",
      "                            hidden_width    =[32, 16, 8, 4]\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 80\n",
      "Best val loss: 1.13841E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.1_lr0.0001_w[32, 16, 8, 4]_d4_run1.pth\n",
      "Run 2 of 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m train \u001b[38;5;241m=\u001b[39m MLPdataset(X_train[year], y_train[year])\n\u001b[1;32m     26\u001b[0m val \u001b[38;5;241m=\u001b[39m MLPdataset(X_val[year], y_val[year])\n\u001b[0;32m---> 27\u001b[0m best_models_reg_pyramid[name], history_reg_pyramid[name] \u001b[38;5;241m=\u001b[39m train_mlp(train,          \n\u001b[1;32m     28\u001b[0m                                 val,\n\u001b[1;32m     29\u001b[0m                                 models_21[name],\n\u001b[1;32m     30\u001b[0m                                 criterion,\n\u001b[1;32m     31\u001b[0m                                 epochs,\n\u001b[1;32m     32\u001b[0m                                 patience,\n\u001b[1;32m     33\u001b[0m                                 print_freq,\n\u001b[1;32m     34\u001b[0m                                 device,\n\u001b[1;32m     35\u001b[0m                                 optimizer,\n\u001b[1;32m     36\u001b[0m                                 lambda_l1\u001b[38;5;241m=\u001b[39mlambda_l1,\n\u001b[1;32m     37\u001b[0m                                 lambda_l2\u001b[38;5;241m=\u001b[39mlambda_l2,\n\u001b[1;32m     38\u001b[0m                                 batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m     39\u001b[0m                                 shuffle_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     40\u001b[0m                                 shuffle_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     41\u001b[0m                                 save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/hyperparam_test/mlp_y\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_l1\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlambda_l1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_l2\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlambda_l2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_drop\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdropout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_lr\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlearning_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_w\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_width\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_d\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_depth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_run\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     42\u001b[0m                                 )\n",
      "File \u001b[0;32m~/Documents/04 Uni/10 Thesis/git/MastersThesis/libs/models.py:160\u001b[0m, in \u001b[0;36mtrain_mlp\u001b[0;34m(train_dataset, val_dataset, model, criterion, epochs, patience, print_freq, device, optimizer, learning_rate, lambda_l1, lambda_l2, batch_size, shuffle_train, shuffle_val, save_path, timing)\u001b[0m\n\u001b[1;32m    157\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    158\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m--> 160\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m    161\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    163\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Documents/04 Uni/10 Thesis/git/MastersThesis/libs/models.py:26\u001b[0m, in \u001b[0;36mMLPdataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     24\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures[idx]\n\u001b[1;32m     25\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets[idx]\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(X, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32), torch\u001b[38;5;241m.\u001b[39mtensor(y, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # current runtime: 15h\n",
    "# for lambda_l1 in l1_space:\n",
    "#     for lambda_l2 in l2_space:\n",
    "#         for dropout in dropout_space:\n",
    "#             for learning_rate in learning_rate_space:\n",
    "#                 for hidden_width in width_space:\n",
    "#                     hidden_depth = len(hidden_width)\n",
    "#                     print(f\"\"\"Training model for year '{year}...: \n",
    "#                             lambda_l1       ={lambda_l1:.0e}\n",
    "#                             lambda_l2       ={lambda_l2:.0e}\n",
    "#                             dropout         ={dropout:.0e}\n",
    "#                             learning_rate   ={learning_rate:.0e}\n",
    "#                             hidden_depth    ={hidden_depth}\n",
    "#                             hidden_width    ={hidden_width}\"\"\")\n",
    "#                     for run in range(n_runs):\n",
    "#                         print(f\"Run {run+1} of {n_runs}\")\n",
    "#                         seed = 42+run\n",
    "#                         np.random.seed(seed)\n",
    "#                         torch.manual_seed(seed)\n",
    "#                         # Initialize the model\n",
    "#                         input_dim = X_train[year].shape[1]\n",
    "#                         name = f'l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}'\n",
    "#                         models_21[name] = MLPModel(input_dim, depth=hidden_depth, width=hidden_width, dropout=dropout, activation=activation_fun).to(device)\n",
    "#                         optimizer = torch.optim.Adam(models_21[name].parameters(), lr=learning_rate)\n",
    "#                         train = MLPdataset(X_train[year], y_train[year])\n",
    "#                         val = MLPdataset(X_val[year], y_val[year])\n",
    "#                         best_models_reg_pyramid[name], history_reg_pyramid[name] = train_mlp(train,          \n",
    "#                                                         val,\n",
    "#                                                         models_21[name],\n",
    "#                                                         criterion,\n",
    "#                                                         epochs,\n",
    "#                                                         patience,\n",
    "#                                                         print_freq,\n",
    "#                                                         device,\n",
    "#                                                         optimizer,\n",
    "#                                                         lambda_l1=lambda_l1,\n",
    "#                                                         lambda_l2=lambda_l2,\n",
    "#                                                         batch_size=batch_size,\n",
    "#                                                         shuffle_train=True,\n",
    "#                                                         shuffle_val=False,\n",
    "#                                                         save_path=f'models/hyperparam_test/mlp_y{year}_l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}.pth'\n",
    "#                                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DataLoader\n",
    "# val_ds     = MLPdataset(X_val[year], y_val[year])\n",
    "# val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # build (n_drop, n_l1, n_l2) mean-loss array\n",
    "# n_l1   = len(l1_space)\n",
    "# n_l2   = len(l2_space)\n",
    "# n_drop = len(dropout_space)\n",
    "\n",
    "# losses = np.zeros((n_drop, n_l1, n_l2), dtype=float)\n",
    "\n",
    "# for di, drop in enumerate(dropout_space):\n",
    "#     for i, l1 in enumerate(l1_space):\n",
    "#         for j, l2 in enumerate(l2_space):\n",
    "#             run_losses = []\n",
    "#             # for each seed, load & eval\n",
    "#             for run in range(n_runs):\n",
    "#                 # since depth_space & width_space each have one entry, \n",
    "#                 # we can just index 0 here — but this will generalize\n",
    "#                 tmp_losses = []\n",
    "#                 for w in width_space:\n",
    "#                     d = len(w)  # since w is a list, we can use its length\n",
    "#                     m = load_model(w, d, run, l1, l2, drop, learning_rate, )\n",
    "#                     with torch.no_grad():\n",
    "#                         batch = [criterion(m(x.to(device)), y.to(device)).item()\n",
    "#                                     for x,y in val_loader]\n",
    "#                     tmp_losses.append(np.mean(batch))\n",
    "#                 run_losses.append(np.mean(tmp_losses))\n",
    "#             # now average over runs\n",
    "#             losses[di, i, j] = np.mean(run_losses)\n",
    "\n",
    "\n",
    "# # global color‐scale\n",
    "# vmin    = losses.min()\n",
    "# vmax    = losses.max()\n",
    "# vcenter = vmin + 0.5*(vmax - vmin)\n",
    "# norm    = TwoSlopeNorm(vmin=vmin, vcenter=vcenter, vmax=vmax)\n",
    "# cmap    = 'viridis'\n",
    "\n",
    "# # layout: up to 2 columns\n",
    "# ncols = min(2, n_drop)\n",
    "# nrows = math.ceil(n_drop / ncols)\n",
    "\n",
    "# fig, axes = plt.subplots(nrows, ncols,\n",
    "#                          figsize=(5*ncols, 4*nrows),\n",
    "#                          squeeze=False)\n",
    "# axes_flat = axes.flatten()\n",
    "\n",
    "# for idx, drop in enumerate(dropout_space):\n",
    "#     ax = axes_flat[idx]\n",
    "#     im = ax.imshow(\n",
    "#         losses[idx],\n",
    "#         aspect='auto',\n",
    "#         cmap=cmap,\n",
    "#         norm=norm,\n",
    "#         origin='lower',\n",
    "#     )\n",
    "\n",
    "#     # y = l1, x = l2\n",
    "#     ax.set_yticks(np.arange(n_l1))\n",
    "#     ax.set_yticklabels(l1_space)\n",
    "#     ax.set_xticks(np.arange(n_l2))\n",
    "#     ax.set_xticklabels(l2_space)\n",
    "\n",
    "#     # only bottom row shows x‐labels\n",
    "#     if idx < (nrows-1)*ncols:\n",
    "#         ax.tick_params(labelbottom=False)\n",
    "#     else:\n",
    "#         ax.set_xlabel(r'$\\ell_2$')\n",
    "\n",
    "#     # only first‐column shows y‐labels\n",
    "#     if idx % ncols != 0:\n",
    "#         ax.tick_params(labelleft=False)\n",
    "#     else:\n",
    "#         ax.set_ylabel(r'$\\ell_1$')\n",
    "\n",
    "#     ax.set_title(f\"dropout = {drop:.2f}\")\n",
    "\n",
    "# # clear unused\n",
    "# for ax in axes_flat[n_drop:]:\n",
    "#     fig.delaxes(ax)\n",
    "\n",
    "# # colorbar at the right edge\n",
    "# fig.subplots_adjust(right=0.88)\n",
    "# cax = fig.add_axes([0.90, 0.15, 0.02, 0.7])  # [left, bottom, width, height]\n",
    "# cbar = fig.colorbar(im, cax=cax)\n",
    "# cbar.set_label('Average Validation Loss (MSE)')\n",
    "\n",
    "# plt.savefig('figs/l1_l2_dropout_loss_pyramid.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[Test of activation functions](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general hyperparameters\n",
    "hidden_depth = 4 \n",
    "hidden_width = 16 \n",
    "learning_rate = 1e-4 \n",
    "\n",
    "# general critereon and regularization parameters\n",
    "criterion = nn.MSELoss()\n",
    "lambda_l1 = 1e-5 # baseline l1 regularization\n",
    "lambda_l2 = 1e-4 # baseline l2 regularization\n",
    "dropout = 0.0\n",
    "\n",
    "activation_space = {'ReLU':nn.ReLU, 'LeakyReLU':nn.LeakyReLU, 'Tanh':nn.Tanh, 'Sigmoid':nn.Sigmoid}\n",
    "\n",
    "best_models_activ = {}\n",
    "history_activ = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for activ_name, activ_fun in activation_space.items():\n",
    "    run_time = []\n",
    "    start_time = time.time()\n",
    "    print(f\"\"\"Training model for year '{year}...: \n",
    "            activation      ={activ_name}\n",
    "            lambda_l1       ={lambda_l1:.0e}\n",
    "            lambda_l2       ={lambda_l2:.0e}\n",
    "            dropout         ={dropout:.0e}\n",
    "            learning_rate   ={learning_rate:.0e}\n",
    "            hidden_depth    ={hidden_depth}\n",
    "            hidden_width    ={hidden_width}\"\"\")\n",
    "    for run in range(n_runs):\n",
    "        print(f\"Run {run+1} of {n_runs}\")\n",
    "        seed = 42+run\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        # Initialize the model\n",
    "        input_dim = X_train[year].shape[1]\n",
    "        name = f'l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}_activ{activ_name}'\n",
    "        models_21[name] = MLPModel(input_dim, depth=hidden_depth, width=hidden_width, dropout=dropout, activation=activ_fun).to(device)\n",
    "        optimizer = torch.optim.Adam(models_21[name].parameters(), lr=learning_rate)\n",
    "        train = MLPdataset(X_train[year], y_train[year])\n",
    "        val = MLPdataset(X_val[year], y_val[year])\n",
    "        best_models_activ[name], history_activ[name], time_ = train_mlp(train,          \n",
    "                                        val,\n",
    "                                        models_21[name],\n",
    "                                        criterion,\n",
    "                                        epochs,\n",
    "                                        patience,\n",
    "                                        print_freq,\n",
    "                                        device,\n",
    "                                        optimizer,\n",
    "                                        lambda_l1=lambda_l1,\n",
    "                                        lambda_l2=lambda_l2,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle_train=True,\n",
    "                                        shuffle_val=False,\n",
    "                                        save_path=f'models/hyperparam_test/mlp_y{year}_l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}_activ{activ_name}.pth',\n",
    "                                        timing = True\n",
    "                                        )\n",
    "        run_time.append(time_)\n",
    "    end_time = time.time()\n",
    "    print(f\"Training with activation nn.{activ_name} took {end_time - start_time:.2f} seconds.\")\n",
    "    print(f\"Average time per epoch: {np.concatenate(run_time).mean()} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_history_activ(history_activ, \n",
    "                save_csv='models/hyperparam_test/history/history_activ.csv')\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_activ = pd.read_csv('models/hyperparam_test/history/history_activ.csv')\n",
    "\n",
    "activations = sorted(df_activ['activ_name'].unique())\n",
    "n_acts = len(activations)+1\n",
    "cmap = plt.get_cmap('viridis', n_acts)\n",
    "color_map = {act: cmap(i) for i, act in enumerate(activations)}\n",
    "\n",
    "# create 2x2 grid\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "# plot\n",
    "for i, act in enumerate(activations):\n",
    "    ax = axes[i]\n",
    "    df_act = df_activ[df_activ['activ_name'] == act]\n",
    "    # Plot each run for this activation\n",
    "    for run_id, grp in df_act.groupby('run'):\n",
    "        ax.plot(\n",
    "            grp['epoch'],\n",
    "            grp['val_loss'],\n",
    "            color=color_map[act],\n",
    "            alpha=0.5,\n",
    "            linewidth=2,\n",
    "        )\n",
    "    ax.set_title(act)\n",
    "    # show x-label on bottom row \n",
    "    if i // 2 == 1:\n",
    "        ax.set_xlabel('Epoch', fontsize=12)\n",
    "    else:\n",
    "        ax.tick_params(labelbottom=False)\n",
    "    # show y-label on left column\n",
    "    if i % 2 == 0:\n",
    "        ax.set_ylabel('Validation Loss', fontsize=12)\n",
    "    else:\n",
    "        ax.tick_params(labelleft=False)\n",
    "    ax.set_ylim(0.019, 0.03)\n",
    "\n",
    "# remove any unused subplots\n",
    "for j in range(len(activations), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('figs/activation_fun_history.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model for each activation function for each run\n",
    "preds_activ = {}\n",
    "\n",
    "for activ_name, activ_fun in activation_space.items():\n",
    "    print(f\"Loading models for activation {activ_name}...\")\n",
    "    all_preds = []\n",
    "    activation_fun = activ_fun\n",
    "    for run in range(n_runs):\n",
    "        best_models_activ[f'activ{activ_name}_run{run+1}'] = load_model(\n",
    "            hidden_width, hidden_depth, run, \n",
    "            lambda_l1, lambda_l2, dropout, learning_rate, activ=activ_name\n",
    "        )\n",
    "        preds = predict_mlp(\n",
    "            best_models_activ[f'activ{activ_name}_run{run+1}'], \n",
    "            X_val[year], \n",
    "            y_val[year], \n",
    "            batch_size=batch_size,\n",
    "            device=device,\n",
    "        )\n",
    "        all_preds.append(preds)\n",
    "    np.stack(all_preds, axis=0)\n",
    "    preds_activ[activ_name] = np.mean(all_preds, axis=0)\n",
    "\n",
    "df_preds_activ = pd.DataFrame(preds_activ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_full = {}\n",
    "results_full = {}\n",
    "for activ_name in activation_space.keys():\n",
    "    y_true = df_preds_activ['y_true'].values\n",
    "    y_pred = df_preds_activ[activ_name].values\n",
    "    results_full[activ_name] = {\n",
    "        'RMSE': rmse_fun(y_true, y_pred),\n",
    "        'MAE': mae_fun(y_true, y_pred),\n",
    "        'AMADL': amadl_fun(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "for metric in ['RMSE', 'MAE', 'AMADL']:\n",
    "        vals = [\n",
    "        results_full['ReLU'][metric],\n",
    "        results_full['LeakyReLU'][metric],\n",
    "        results_full['Tanh'][metric],\n",
    "        results_full['Sigmoid'][metric],\n",
    "        ]\n",
    "        metrics_full[metric] = vals\n",
    "\n",
    "tab_activ = latex_table(list(activation_space.keys()),metrics_full)\n",
    "with open('tabs/activ_fun_perf.tex', 'w') as f:\n",
    "    f.write(tab_activ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import t as student_t\n",
    "\n",
    "# define plot limits\n",
    "x_min, x_max = -0.5, 0.5\n",
    "bins = 100\n",
    "\n",
    "# 2×2 grid with shared axes\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 6),\n",
    "                         sharex=True, sharey=True)\n",
    "\n",
    "for ax, act in zip(axes.flat, activation_space):\n",
    "    res = df_preds_activ['y_true'] - df_preds_activ[act]\n",
    "\n",
    "    # density=True makes the histogram area = 1\n",
    "    ax.hist(res, bins=bins, density=True, alpha=0.7)\n",
    "\n",
    "    x = np.linspace(x_min, x_max, 500)\n",
    "    mu, sigma = res.mean(), res.std(ddof=0)\n",
    "    mu_lap    = np.median(res)\n",
    "    b_lap     = np.mean(np.abs(res - mu_lap))\n",
    "    nu        = 1\n",
    "\n",
    "    # Normal pdf\n",
    "    pdf_gauss = (1/(sigma*np.sqrt(2*np.pi))) * np.exp(-0.5*((x - mu)/sigma)**2)\n",
    "    ax.plot(x, pdf_gauss, \"C1--\", lw=2,\n",
    "            label=rf\"Normal($\\mu$={mu:.3f}, $\\sigma$={sigma:.3f})\")\n",
    "\n",
    "    # Laplace pdf\n",
    "    pdf_lap = (1/(2*b_lap)) * np.exp(-np.abs(x - mu_lap)/b_lap)\n",
    "    ax.plot(x, pdf_lap, \"C3:\", lw=2,\n",
    "            label=rf\"Laplace($\\mu_L$={mu_lap:.3f}, b={b_lap:.3f})\")\n",
    "\n",
    "    # Student-t pdf\n",
    "    pdf_t = student_t.pdf(x, df=nu, loc=mu, scale=sigma)\n",
    "    ax.plot(x, pdf_t, \"C6-.\", lw=2,\n",
    "            label=rf\"Student-$t_{{{nu}}}$($\\mu$={mu:.3f}, $\\sigma$={sigma:.3f})\")\n",
    "\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_title(act)\n",
    "    ax.legend(fontsize=\"small\")\n",
    "\n",
    "# Only label the outer axes\n",
    "axes[0, 0].set_ylabel('Density')\n",
    "axes[1, 0].set_ylabel('Density')\n",
    "axes[1, 0].set_xlabel('Residual')\n",
    "axes[1, 1].set_xlabel('Residual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figs/residuals_activations_density.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc7_'></a>[Test of criterion functions](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general hyperparameters\n",
    "hidden_depth = 4 \n",
    "hidden_width = 16 \n",
    "learning_rate = 1e-4 \n",
    "\n",
    "# general critereon and regularization parameters\n",
    "lambda_l1 = 1e-5 # baseline l1 regularization\n",
    "lambda_l2 = 1e-4 # baseline l2 regularization\n",
    "dropout = 0.0\n",
    "activation_fun = nn.ReLU\n",
    "\n",
    "criterion_space = {'MSE':nn.MSELoss(),'MAE':nn.L1Loss(),'Huber':nn.HuberLoss(delta=0.5)}\n",
    "\n",
    "best_models_crit = {}\n",
    "history_crit = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for crit, crit_fun in criterion_space.items():\n",
    "    run_time = []\n",
    "    start_time = time.time()\n",
    "    print(f\"\"\"Training model for year '{year}...: \n",
    "            criterion       ={crit}\n",
    "            lambda_l1       ={lambda_l1:.0e}\n",
    "            lambda_l2       ={lambda_l2:.0e}\n",
    "            dropout         ={dropout:.0e}\n",
    "            learning_rate   ={learning_rate:.0e}\n",
    "            hidden_depth    ={hidden_depth}\n",
    "            hidden_width    ={hidden_width}\"\"\")\n",
    "    for run in range(n_runs):\n",
    "        print(f\"Run {run+1} of {n_runs}\")\n",
    "        seed = 42+run\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        # Initialize the model\n",
    "        input_dim = X_train[year].shape[1]\n",
    "        name = f'l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}_crit{crit}'\n",
    "        models_21[name] = MLPModel(input_dim, depth=hidden_depth, width=hidden_width, dropout=dropout, activation=activation_fun).to(device)\n",
    "        optimizer = torch.optim.Adam(models_21[name].parameters(), lr=learning_rate)\n",
    "        train = MLPdataset(X_train[year], y_train[year])\n",
    "        val = MLPdataset(X_val[year], y_val[year])\n",
    "        best_models_crit[name], history_crit[name], time_ = train_mlp(train,          \n",
    "                                        val,\n",
    "                                        models_21[name],\n",
    "                                        crit_fun,\n",
    "                                        epochs,\n",
    "                                        patience,\n",
    "                                        print_freq,\n",
    "                                        device,\n",
    "                                        optimizer,\n",
    "                                        lambda_l1=lambda_l1,\n",
    "                                        lambda_l2=lambda_l2,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle_train=True,\n",
    "                                        shuffle_val=False,\n",
    "                                        save_path=f'models/hyperparam_test/mlp_y{year}_l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}_crit{crit}.pth',\n",
    "                                        timing = True\n",
    "                                        )\n",
    "        run_time.append(time_)\n",
    "    end_time = time.time()\n",
    "    print(f\"Training with criterion {crit_fun} took {end_time - start_time:.2f} seconds.\")\n",
    "    print(f\"Average time per epoch: {np.concatenate(run_time).mean()} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_history_crit(history_crit, \n",
    "                save_csv='models/hyperparam_test/history/history_crit.csv')\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crit = pd.read_csv('models/hyperparam_test/history/history_crit.csv')\n",
    "\n",
    "criterions = sorted(df_crit['crit_name'].unique(), reverse=True)\n",
    "n_crits = len(criterions)+1\n",
    "cmap = plt.get_cmap('viridis', n_crits)\n",
    "color_map = {crit: cmap(i) for i, crit in enumerate(criterions)}\n",
    "\n",
    "\n",
    "# 2×2 grid with shared axes\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "gs  = gridspec.GridSpec(2, 4, height_ratios=[1, 1])\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0,0:2])\n",
    "ax2 = fig.add_subplot(gs[0,2:4])\n",
    "ax3 = fig.add_subplot(gs[1,1:3])\n",
    "axes = [ax1, ax2, ax3]\n",
    "\n",
    "# plot\n",
    "for i, crit in enumerate(criterions):\n",
    "    ax = axes[i]\n",
    "    df_cr = df_crit[df_crit['crit_name'] == crit]\n",
    "    # Plot each run for this criterion\n",
    "    for run_id, grp in df_cr.groupby('run'):\n",
    "        ax.plot(\n",
    "            grp['epoch'],\n",
    "            grp['val_loss'],\n",
    "            color=color_map[crit],\n",
    "            alpha=0.5,\n",
    "            linewidth=2,\n",
    "        )\n",
    "    ax.set_title(crit)\n",
    "    ax.set_ylabel(f'Validation Loss ({crit})', fontsize=12)\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "\n",
    "ax1.set_ylim(0.0161, 0.03)\n",
    "ax2.set_ylim(0.061, 0.20)\n",
    "ax3.set_ylim(0.0065, 0.020)\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('figs/criterion_fun_history.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model for each criterion function for each run\n",
    "preds_crit = {}\n",
    "\n",
    "for crit_name, crit_fun in criterion_space.items():\n",
    "    print(f\"Loading models for criterion {crit_name}...\")\n",
    "    all_preds = []\n",
    "    for run in range(n_runs):\n",
    "        best_models_crit[f'crit{crit_name}_run{run+1}'] = load_model(\n",
    "            hidden_width, hidden_depth, run, \n",
    "            lambda_l1, lambda_l2, dropout, learning_rate, crit=crit_name\n",
    "        )\n",
    "        preds = predict_mlp(\n",
    "            best_models_crit[f'crit{crit_name}_run{run+1}'], \n",
    "            X_val[year], \n",
    "            y_val[year], \n",
    "            batch_size=batch_size,\n",
    "            device=device,\n",
    "        )\n",
    "        all_preds.append(preds)\n",
    "    np.stack(all_preds, axis=0)\n",
    "    preds_crit[crit_name] = np.mean(all_preds, axis=0)\n",
    "\n",
    "df_preds_crit = pd.DataFrame(preds_crit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_full = {}\n",
    "results_full = {}\n",
    "for crit_name in criterion_space.keys():\n",
    "    y_true = df_preds_crit['y_true'].values\n",
    "    y_pred = df_preds_crit[crit_name].values\n",
    "    results_full[crit_name] = {\n",
    "        'RMSE': rmse_fun(y_true, y_pred),\n",
    "        'MAE': mae_fun(y_true, y_pred),\n",
    "        'AMADL': amadl_fun(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "for metric in ['RMSE', 'MAE', 'AMADL']:\n",
    "        vals = [\n",
    "        results_full['MSE'][metric],\n",
    "        results_full['MAE'][metric],\n",
    "        results_full['Huber'][metric],\n",
    "        ]\n",
    "        metrics_full[metric] = vals\n",
    "\n",
    "tab_crit = latex_table(list(criterion_space.keys()),metrics_full)\n",
    "with open('tabs/crit_fun_perf.tex', 'w') as f:\n",
    "    f.write(tab_crit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define plot limits\n",
    "x_min, x_max = -0.5, 0.5\n",
    "bins = 100\n",
    "\n",
    "# 2×2 grid with shared axes\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "gs  = gridspec.GridSpec(2, 4, height_ratios=[1, 1])\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0,0:2])\n",
    "ax2 = fig.add_subplot(gs[0,2:4],  sharex=ax1, sharey=ax1)\n",
    "ax2.tick_params(axis='y', labelleft=False)\n",
    "ax3 = fig.add_subplot(gs[1,1:3],  sharex=ax1, sharey=ax1)\n",
    "axes = [ax1, ax2, ax3]\n",
    "\n",
    "for ax, act in zip(axes, criterion_space):\n",
    "    res = df_preds_crit['y_true'] - df_preds_crit[act]\n",
    "\n",
    "    # density=True makes the histogram area = 1\n",
    "    ax.hist(res, bins=bins, density=True, alpha=0.7)\n",
    "\n",
    "    x = np.linspace(x_min, x_max, 500)\n",
    "    mu, sigma = res.mean(), res.std(ddof=0)\n",
    "    mu_lap    = np.median(res)\n",
    "    b_lap     = np.mean(np.abs(res - mu_lap))\n",
    "    nu        = 1\n",
    "\n",
    "    # Normal pdf\n",
    "    pdf_gauss = (1/(sigma*np.sqrt(2*np.pi))) * np.exp(-0.5*((x - mu)/sigma)**2)\n",
    "    ax.plot(x, pdf_gauss, \"C1--\", lw=2,\n",
    "            label=rf\"Normal($\\mu$={mu:.3f}, $\\sigma$={sigma:.3f})\")\n",
    "\n",
    "    # Laplace pdf\n",
    "    pdf_lap = (1/(2*b_lap)) * np.exp(-np.abs(x - mu_lap)/b_lap)\n",
    "    ax.plot(x, pdf_lap, \"C3:\", lw=2,\n",
    "            label=rf\"Laplace($\\mu_L$={mu_lap:.3f}, b={b_lap:.3f})\")\n",
    "\n",
    "    # Student-t pdf\n",
    "    pdf_t = student_t.pdf(x, df=nu, loc=mu, scale=sigma)\n",
    "    ax.plot(x, pdf_t, \"C6-.\", lw=2,\n",
    "            label=rf\"Student-$t_{{{nu}}}$($\\mu$={mu:.3f}, $\\sigma$={sigma:.3f})\")\n",
    "\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_title(act)\n",
    "    ax.legend(fontsize=\"small\")\n",
    "    ax.set_xlabel('Residual')\n",
    "\n",
    "# Only label the outer axes\n",
    "ax1.set_ylabel('Density')\n",
    "ax3.set_ylabel('Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figs/residuals_criterion_density.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

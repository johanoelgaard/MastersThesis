{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from models import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df = pd.read_csv('data/data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [col for col in df.columns if col not in ['timestamp','ticker','target']]\n",
    "df_norm = df.copy(deep=True)\n",
    "df_norm = df_norm.dropna(subset=feature_cols) # should not drop any rows\n",
    "\n",
    "df_norm['timestamp'] = pd.to_datetime(df_norm['timestamp'])\n",
    "\n",
    "# drop data from 2025\n",
    "df_norm = df_norm[df_norm['timestamp'] < '2025-01-01']\n",
    "\n",
    "# normalize inputs\n",
    "X_values = df_norm[feature_cols].values.astype('float64')\n",
    "\n",
    "# # simple normalization: subtract mean, divide std\n",
    "# X_mean, X_std = X_values.mean(axis=0), X_values.std(axis=0)\n",
    "# X_values = (X_values - X_mean) / (X_std + 1e-8) # adding a small epsilon to avoid division by zero\n",
    "\n",
    "y_values = df_norm['target'].values.astype('float64')\n",
    "\n",
    "periods = { '20' : '2020-01-01', \n",
    "            '21' : '2021-01-01', \n",
    "            '22' : '2022-01-01', \n",
    "            '23' : '2023-01-01' \n",
    "            } # year used for validation i.e. 2020-01-01 will use 2001-2019 for training and 2020 for validation\n",
    "\n",
    "X_train, y_train = {}, {}\n",
    "X_val, y_val = {}, {}\n",
    "X_test, y_test = {}, {}\n",
    "X_scalars, y_scalars = {}, {}\n",
    "\n",
    "\n",
    "for name, period in periods.items():\n",
    "    tr_mask = df_norm['timestamp'] < period\n",
    "    va_mask = (df_norm['timestamp'] >= period) & (df_norm['timestamp'] - pd.DateOffset(years=1) < period)\n",
    "    te_mask = (df_norm['timestamp'] - pd.DateOffset(years=1) >= period) & (df_norm['timestamp'] - pd.DateOffset(years=2) < period)\n",
    "\n",
    "    assert (df_norm.loc[tr_mask, 'timestamp'].max() <\n",
    "        df_norm.loc[va_mask, 'timestamp'].min()), \"train leaks into val\"\n",
    "\n",
    "    assert (df_norm.loc[va_mask, 'timestamp'].max() <\n",
    "            df_norm.loc[te_mask, 'timestamp'].min()), \"val leaks into test\"\n",
    "\n",
    "    X_tr, y_tr = X_values[tr_mask], y_values[tr_mask]\n",
    "    X_va, y_va = X_values[va_mask], y_values[va_mask]\n",
    "    X_te, y_te = X_values[te_mask], y_values[te_mask]\n",
    "\n",
    "    X_scalars[name], y_scalars[name] = StandardScaler(), StandardScaler()\n",
    "\n",
    "    X_scalars[name].fit(X_tr)\n",
    "    X_train[name] = X_scalars[name].transform(X_tr)\n",
    "    X_val[name] = X_scalars[name].transform(X_va)\n",
    "    X_test[name] = X_scalars[name].transform(X_te)\n",
    "\n",
    "    # ignoring the target normalization\n",
    "    # y_scalars[name].fit(y_tr.reshape(-1, 1))\n",
    "    # y_train[name] = y_scalars[name].transform(y_tr.reshape(-1, 1))\n",
    "    # y_val[name] = y_scalars[name].transform(y_va.reshape(-1, 1))\n",
    "    # y_test[name] = y_scalars[name].transform(y_te.reshape(-1, 1))\n",
    "\n",
    "    y_train[name] = y_tr.reshape(-1, 1)\n",
    "    y_val[name] = y_va.reshape(-1, 1)\n",
    "    y_test[name] = y_te.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# moving to metal or CUDA GPU if available\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# general hyperparameters\n",
    "# input_dim = x_train_mlp.shape[1]\n",
    "hidden_depth = 2 # only hidden, excluding in- and output layers\n",
    "hidden_width = 32 # int for all being equal width; list for different widths\n",
    "learning_rate = 3e-4 # increased from 1e-3 as we have full batching\n",
    "activation_fun = nn.ReLU # nn.ReLU nn.Tanh nn.Sigmoid nn.LeakyReLU nn.Identity\n",
    "\n",
    "\n",
    "# general critereon and regularization parameters\n",
    "criterion = nn.MSELoss()\n",
    "lambda_l1 = 0.0 # 1e-3 currently 0 \n",
    "lambda_l2 = 0.0 # 1e-3 0 because of weight decay\n",
    "\n",
    "\n",
    "weight_decay = 1e-3\n",
    "dropout = 0.2\n",
    "\n",
    "# general parmeters\n",
    "patience = 5000\n",
    "print_freq = 250\n",
    "epochs = 500\n",
    "batch_size = 5096 # adjust to your GPU memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for period 20...\n",
      "Epoch 200/1000  - Train Loss: 2.49249E-02  - Val Loss: 2.03898E-02\n",
      "Epoch 400/1000  - Train Loss: 2.43964E-02  - Val Loss: 2.05460E-02\n",
      "Epoch 600/1000  - Train Loss: 2.39270E-02  - Val Loss: 2.05829E-02\n",
      "Epoch 800/1000  - Train Loss: 2.33728E-02  - Val Loss: 2.03666E-02\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m train \u001b[38;5;241m=\u001b[39m MLPdataset(X_train[name], y_train[name])\n\u001b[1;32m     13\u001b[0m val \u001b[38;5;241m=\u001b[39m MLPdataset(X_val[name], y_val[name])\n\u001b[0;32m---> 14\u001b[0m best_models[name], history[name] \u001b[38;5;241m=\u001b[39m train_mlp(train,          \n\u001b[1;32m     15\u001b[0m                                             val,\n\u001b[1;32m     16\u001b[0m                                             models[name],\n\u001b[1;32m     17\u001b[0m                                             criterion,\n\u001b[1;32m     18\u001b[0m                                             epochs,\n\u001b[1;32m     19\u001b[0m                                             patience,\n\u001b[1;32m     20\u001b[0m                                             print_freq,\n\u001b[1;32m     21\u001b[0m                                             device,\n\u001b[1;32m     22\u001b[0m                                             optimizer,\n\u001b[1;32m     23\u001b[0m                                             lambda_l1\u001b[38;5;241m=\u001b[39mlambda_l1,\n\u001b[1;32m     24\u001b[0m                                             lambda_l2\u001b[38;5;241m=\u001b[39mlambda_l2,\n\u001b[1;32m     25\u001b[0m                                             batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m     26\u001b[0m                                             shuffle_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     27\u001b[0m                                             shuffle_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     28\u001b[0m                                             save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/mlp_y\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_l1\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlambda_l1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_l2\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_decay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_drop\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdropout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_lr\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlearning_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_w\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_width\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_d\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_depth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/04 Uni/10 Thesis/git/MastersThesis/models.py:158\u001b[0m, in \u001b[0;36mtrain_mlp\u001b[0;34m(train_dataset, val_dataset, model, criterion, epochs, patience, print_freq, device, optimizer, learning_rate, lambda_l1, lambda_l2, batch_size, shuffle_train, shuffle_val, save_path)\u001b[0m\n\u001b[1;32m    155\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    156\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m    159\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    161\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Documents/04 Uni/10 Thesis/git/MastersThesis/models.py:26\u001b[0m, in \u001b[0;36mMLPdataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     24\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures[idx]\n\u001b[1;32m     25\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets[idx]\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(X, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32), torch\u001b[38;5;241m.\u001b[39mtensor(y, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_models = {}\n",
    "history = {}\n",
    "models = {}\n",
    "\n",
    "for name, period in periods.items():\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    print(f\"Training model for period {name}...\")\n",
    "    input_dim = X_train[name].shape[1]\n",
    "    models[name] = MLPModel(input_dim,depth=hidden_depth,width=hidden_width, dropout=dropout, activation=activation_fun).to(device)\n",
    "    optimizer = torch.optim.AdamW(models[name].parameters(), weight_decay=weight_decay, lr=learning_rate)\n",
    "    train = MLPdataset(X_train[name], y_train[name])\n",
    "    val = MLPdataset(X_val[name], y_val[name])\n",
    "    best_models[name], history[name] = train_mlp(train,          \n",
    "                                                val,\n",
    "                                                models[name],\n",
    "                                                criterion,\n",
    "                                                epochs,\n",
    "                                                patience,\n",
    "                                                print_freq,\n",
    "                                                device,\n",
    "                                                optimizer,\n",
    "                                                lambda_l1=lambda_l1,\n",
    "                                                lambda_l2=lambda_l2,\n",
    "                                                batch_size=batch_size,\n",
    "                                                shuffle_train=True,\n",
    "                                                shuffle_val=False,\n",
    "                                                save_path=f'models/mlp_y{name}_l1{lambda_l1}_l2{weight_decay}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for period _20...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m train \u001b[38;5;241m=\u001b[39m MLPdataset(X_train[name], y_train[name])\n\u001b[1;32m     13\u001b[0m val \u001b[38;5;241m=\u001b[39m MLPdataset(X_val[name], y_val[name])\n\u001b[0;32m---> 14\u001b[0m best_models[name], history[name] \u001b[38;5;241m=\u001b[39m train_mlp(train,          \n\u001b[1;32m     15\u001b[0m                                              val,\n\u001b[1;32m     16\u001b[0m                                              models[name],\n\u001b[1;32m     17\u001b[0m                                              criterion,\n\u001b[1;32m     18\u001b[0m                                              epochs,\n\u001b[1;32m     19\u001b[0m                                              patience,\n\u001b[1;32m     20\u001b[0m                                              print_freq,\n\u001b[1;32m     21\u001b[0m                                              device,\n\u001b[1;32m     22\u001b[0m                                              optimizer,\n\u001b[1;32m     23\u001b[0m                                              lambda_l1\u001b[38;5;241m=\u001b[39mlambda_l1,\n\u001b[1;32m     24\u001b[0m                                              lambda_l2\u001b[38;5;241m=\u001b[39mlambda_l2,\n\u001b[1;32m     25\u001b[0m                                              batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m     26\u001b[0m                                              shuffle_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     27\u001b[0m                                              shuffle_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     28\u001b[0m                                              save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/mlp_model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/04 Uni/10 Thesis/git/MastersThesis/models.py:167\u001b[0m, in \u001b[0;36mtrain_mlp\u001b[0;34m(train_dataset, val_dataset, model, criterion, epochs, patience, print_freq, device, optimizer, learning_rate, lambda_l1, lambda_l2, batch_size, shuffle_train, shuffle_val, save_path)\u001b[0m\n\u001b[1;32m    165\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m l2_regularization(model, lambda_l2) \u001b[38;5;241m/\u001b[39m X\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    166\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 167\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    169\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m X\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    171\u001b[0m train_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/optim/adamw.py:188\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    175\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    177\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    178\u001b[0m         group,\n\u001b[1;32m    179\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m         state_steps,\n\u001b[1;32m    186\u001b[0m     )\n\u001b[0;32m--> 188\u001b[0m     adamw(\n\u001b[1;32m    189\u001b[0m         params_with_grad,\n\u001b[1;32m    190\u001b[0m         grads,\n\u001b[1;32m    191\u001b[0m         exp_avgs,\n\u001b[1;32m    192\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    193\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    194\u001b[0m         state_steps,\n\u001b[1;32m    195\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m    196\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    197\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    198\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    199\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    200\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    201\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    202\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    203\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    204\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    205\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    206\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    207\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    208\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    209\u001b[0m     )\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/optim/adamw.py:340\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 340\u001b[0m func(\n\u001b[1;32m    341\u001b[0m     params,\n\u001b[1;32m    342\u001b[0m     grads,\n\u001b[1;32m    343\u001b[0m     exp_avgs,\n\u001b[1;32m    344\u001b[0m     exp_avg_sqs,\n\u001b[1;32m    345\u001b[0m     max_exp_avg_sqs,\n\u001b[1;32m    346\u001b[0m     state_steps,\n\u001b[1;32m    347\u001b[0m     amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m    348\u001b[0m     beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    349\u001b[0m     beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    350\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m    351\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[1;32m    352\u001b[0m     eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m    353\u001b[0m     maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[1;32m    354\u001b[0m     capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[1;32m    355\u001b[0m     differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[1;32m    356\u001b[0m     grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[1;32m    357\u001b[0m     found_inf\u001b[38;5;241m=\u001b[39mfound_inf,\n\u001b[1;32m    358\u001b[0m     has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    359\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/optim/adamw.py:420\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    419\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m--> 420\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[1;32m    423\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# best_models = {}\n",
    "# history = {}\n",
    "# models = {}\n",
    "\n",
    "# for name, period in periods.items():\n",
    "#     np.random.seed(42)\n",
    "#     torch.manual_seed(42)\n",
    "#     print(f\"Training model for period {name}...\")\n",
    "#     input_dim = X_train[name].shape[1]\n",
    "#     models[name] = MLPModel(input_dim,depth=hidden_depth,width=hidden_width, dropout=dropout, activation=activation_fun).to(device)\n",
    "#     optimizer = torch.optim.AdamW(models[name].parameters(), weight_decay=weight_decay, lr=learning_rate)\n",
    "#     train = MLPdataset(X_train[name], y_train[name])\n",
    "#     val = MLPdataset(X_val[name], y_val[name])\n",
    "#     best_models[name], history[name] = train_mlp(train,          \n",
    "#                                                  val,\n",
    "#                                                  models[name],\n",
    "#                                                  criterion,\n",
    "#                                                  epochs,\n",
    "#                                                  patience,\n",
    "#                                                  print_freq,\n",
    "#                                                  device,\n",
    "#                                                  optimizer,\n",
    "#                                                  lambda_l1=lambda_l1,\n",
    "#                                                  lambda_l2=lambda_l2,\n",
    "#                                                  batch_size=batch_size,\n",
    "#                                                  shuffle_train=True,\n",
    "#                                                  shuffle_val=False,\n",
    "#                                                  save_path=f'models/mlp_model_{name}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'_20'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, period \u001b[38;5;129;01min\u001b[39;00m periods\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# plot training history\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m----> 4\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(history[name][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(history[name][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m     plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpochs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: '_20'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for name, period in periods.items():\n",
    "    # plot training history\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history[name]['train_loss'], label='Train Loss')\n",
    "    plt.plot(history[name]['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Training History for Model {name}')\n",
    "    plt.legend()\n",
    "    # plt.ylim(0, 1)\n",
    "    # plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "    # plot importance in best model\n",
    "    # ensure the model is in evaluation mode\n",
    "    best_model = best_models[name].eval()\n",
    "    val_ = DataLoader(MLPdataset(X_val[name], y_val[name]), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # initialize a tensor to accumulate importance for each feature\n",
    "    n_features = len(feature_cols)\n",
    "    feature_importance = torch.zeros(n_features).to(device)\n",
    "    n_samples = 0\n",
    "\n",
    "    # loop over the validation dataloader\n",
    "    for batch_X, batch_y in val_:\n",
    "        batch_X = batch_X.to(device)\n",
    "        # enable gradient tracking on inputs\n",
    "        batch_X.requires_grad_()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = best_model(batch_X)\n",
    "        # to get a scalar output for gradient calculation, sum over the batch\n",
    "        output_sum = outputs.sum()\n",
    "        \n",
    "        # clear previous gradients and compute gradients of the output with respect to inputs\n",
    "        best_model.zero_grad()\n",
    "        output_sum.backward()\n",
    "        \n",
    "        # accumulate the absolute gradients summed across each sample in the batch\n",
    "        # (batch_X.grad has shape [batch_size, n_features])\n",
    "        feature_importance += batch_X.grad.abs().sum(dim=0)\n",
    "        n_samples += batch_X.size(0)\n",
    "\n",
    "    # average the gradients over all samples\n",
    "    feature_importance /= n_samples\n",
    "\n",
    "    # move to CPU for plotting\n",
    "    feature_importance = feature_importance.cpu().detach().numpy()\n",
    "\n",
    "    # normalize the importance scores to sum to 1\n",
    "    importance_norm = feature_importance / feature_importance.sum()\n",
    "\n",
    "    # sort features by importance in descending order\n",
    "    sorted_idx = np.argsort(importance_norm)[::-1]\n",
    "    sorted_importance = importance_norm[sorted_idx]\n",
    "\n",
    "    # # select the top 20 features\n",
    "    # top_n = 20\n",
    "    # if len(sorted_importance) > top_n:\n",
    "    #     sorted_importance = sorted_importance[:top_n]\n",
    "    #     sorted_idx = sorted_idx[:top_n]\n",
    "    #     feature_cols = feature_cols[:top_n]\n",
    "    sorted_features = [feature_cols[i] for i in sorted_idx]\n",
    "\n",
    "    # bar chart of feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(len(sorted_features)), sorted_importance, align='center')\n",
    "    plt.xticks(range(len(sorted_features)), sorted_features, rotation=90)\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Normalized Importance')\n",
    "    plt.title('Feature Importance Based on Average Absolute Gradients')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model with most data on multiple parameters\n",
    "# geomspace for weight decay\n",
    "weight_decay_space = [1e-4, 1e-3, 5e-3, 1e-2] # np.geomspace(1e-4, 1e-2, num=4)\n",
    "dropout_space = [0.0, 0.1, 0.2, 0.3]# np.geomspace(0.0, 0.3, num=4)\n",
    "learning_rate_space = learning_rate # np.geomspace(1e-4, 5e-3, num=4)\n",
    "depth_space = [1, 2, 3]\n",
    "width_space = [32, 64, 128]\n",
    "\n",
    "best_models_23 = {}\n",
    "history_23 = {}\n",
    "models_23 = {}\n",
    "\n",
    "for weight_decay in weight_decay_space:\n",
    "    for dropout in dropout_space:\n",
    "        for learning_rate in learning_rate_space:\n",
    "            for hidden_depth in depth_space:\n",
    "                for hidden_width in width_space:\n",
    "                    np.random.seed(42)\n",
    "                    torch.manual_seed(42)\n",
    "                    print(f\"Training model for year 2023 with weight_decay={weight_decay}, dropout={dropout}, learning_rate={learning_rate}, hidden_depth={hidden_depth}, hidden_width={hidden_width}...\")\n",
    "                    # Initialize the model\n",
    "                    input_dim = X_train['23'].shape[1]\n",
    "                    name = f'l1{lambda_l1}_l2{weight_decay}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}'\n",
    "                    models_23[name] = MLPModel(input_dim, depth=hidden_depth, width=hidden_width, dropout=dropout, activation=activation_fun).to(device)\n",
    "                    optimizer = torch.optim.AdamW(models_23[name].parameters(), weight_decay=weight_decay, lr=learning_rate)\n",
    "                    train = MLPdataset(X_train['23'], y_train['23'])\n",
    "                    val = MLPdataset(X_val['23'], y_val['23'])\n",
    "                    best_models_23[name], history_23[name] = train_mlp(train,          \n",
    "                                                    val,\n",
    "                                                    models_23[name],\n",
    "                                                    criterion,\n",
    "                                                    epochs,\n",
    "                                                    patience,\n",
    "                                                    print_freq,\n",
    "                                                    device,\n",
    "                                                    optimizer,\n",
    "                                                    lambda_l1=lambda_l1,\n",
    "                                                    lambda_l2=lambda_l2,\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    shuffle_train=True,\n",
    "                                                    shuffle_val=False,\n",
    "                                                    save_path=f'models/mlp_y23_l1{lambda_l1}_l2{weight_decay}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

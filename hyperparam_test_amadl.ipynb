{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Notebook for hyperparameter tuning of the MLP model with AMADL Loss Function](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Notebook for hyperparameter tuning of the MLP model with AMADL Loss Function](#toc1_)    \n",
    "- [Import libraries](#toc2_)    \n",
    "- [Import data](#toc3_)    \n",
    "- [Prepare data for training](#toc4_)    \n",
    "- [Test hyperparameters](#toc5_)    \n",
    "  - [Constant scheme](#toc5_1_)    \n",
    "  - [Pyramid scheme](#toc5_2_)    \n",
    "    - [Depth and width comparison](#toc5_2_1_)    \n",
    "    - [Model convergence](#toc5_2_2_)    \n",
    "  - [Regularization strength](#toc5_3_)    \n",
    "  - [Pyramid](#toc5_4_)    \n",
    "- [Test of activation functions](#toc6_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Import libraries](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "import matplotlib.gridspec as gridspec\n",
    "import math\n",
    "import ast\n",
    "import gc\n",
    "import time\n",
    "from scipy.stats import t as student_t\n",
    "\n",
    "from libs.models import *\n",
    "from libs.functions import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "plt.rcParams.update({'font.size': 12, 'figure.figsize': (10, 4), 'figure.dpi': 300})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Import data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df = pd.read_csv('data/data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Prepare data for training](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare expanding window splits\n",
    "periods = {\n",
    "    '21' : '2019-12-31', # 2021 is the test set\n",
    "    # '22' : '2020-12-31', # 2022 is the test set\n",
    "    # '23' : '2021-12-31', # 2023 is the test set\n",
    "    # '24': '2022-12-31' # 2024 is the test set\n",
    "}\n",
    "\n",
    "# identify dummy vs. numeric columns\n",
    "feature_cols = [col for col in df.columns if col not in ['timestamp', 'ticker', 'target']]\n",
    "nace_cols = [c for c in feature_cols if c.startswith('NACE_')]\n",
    "dummy_cols = ['divi','divo'] # sin removed\n",
    "macro_cols = ['discount', 'tms', 'dp', 'ep', 'svar'] # 'bm_macro'\n",
    "\n",
    "# nummeric cols = cols not in cat and macro cols\n",
    "numeric_cols = [c for c in feature_cols if c not in dummy_cols and c not in nace_cols and c not in macro_cols]\n",
    "\n",
    "df_raw = df.copy(deep=True)\n",
    "df_raw['timestamp'] = pd.to_datetime(df_raw['timestamp'])\n",
    "\n",
    "# drop data from 2025\n",
    "df_raw = df_raw[df_raw['timestamp'] < '2025-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create interaction features between numeric and macro features\n",
    "C = df[numeric_cols].values         # shape = (n_rows, P_c)\n",
    "X = df[macro_cols].values           # shape = (n_rows, P_x)\n",
    "\n",
    "# compute all pairwise products with broadcasting:\n",
    "K = C[:,:,None] * X[:,None,:]\n",
    "\n",
    "# reshape to (n_rows, P_c * P_x)\n",
    "Z = K.reshape(len(df), -1)\n",
    "\n",
    "# build the column names in the same order\n",
    "xc_names = [\n",
    "    f\"{c}_x_{m}\"\n",
    "    for c in numeric_cols\n",
    "    for m in macro_cols\n",
    "]\n",
    "\n",
    "# wrap back into a DataFrame\n",
    "df_xc = pd.DataFrame(Z, columns=xc_names, index=df.index)\n",
    "\n",
    "feature_cols = numeric_cols + xc_names + dummy_cols + nace_cols\n",
    "numeric_cols = numeric_cols + xc_names\n",
    "cat_cols = dummy_cols + nace_cols\n",
    "df_z = df_raw.merge(df_xc, left_index=True, right_index=True)\n",
    "# drop macro_cols\n",
    "df_z = df_z.drop(columns=macro_cols)\n",
    "# sort columns by feature_cols\n",
    "df_norm = df_z[['timestamp', 'ticker', 'target'] + feature_cols]\n",
    "\n",
    "y_values = df_norm['target'].values.astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare containers\n",
    "X_train, X_val, X_test = {}, {}, {}\n",
    "y_train, y_val, y_test = {}, {}, {}\n",
    "preprocessors = {}\n",
    "y_scalers = {}\n",
    "\n",
    "for y, period in periods.items():\n",
    "    period = pd.to_datetime(period)\n",
    "\n",
    "    # split masks\n",
    "    tr_mask = df_norm['timestamp'] < period\n",
    "    va_mask = (df_norm['timestamp'] >= period) & \\\n",
    "              (df_norm['timestamp'] - pd.DateOffset(years=1) < period)\n",
    "    te_mask = (df_norm['timestamp'] - pd.DateOffset(years=1) >= period) & \\\n",
    "              (df_norm['timestamp'] - pd.DateOffset(years=2) < period)\n",
    "\n",
    "    # extract raw feature DataFrames\n",
    "    X_tr_df = df_norm.loc[tr_mask, feature_cols].copy()\n",
    "    X_va_df = df_norm.loc[va_mask, feature_cols].copy()\n",
    "    X_te_df = df_norm.loc[te_mask, feature_cols].copy()\n",
    "    y_tr    = y_values[tr_mask]\n",
    "    y_va    = y_values[va_mask]\n",
    "    y_te    = y_values[te_mask]\n",
    "\n",
    "    # compute winsorization bounds on train\n",
    "    lower = X_tr_df[numeric_cols].quantile(0.01)\n",
    "    upper = X_tr_df[numeric_cols].quantile(0.99)\n",
    "\n",
    "    # apply clipping to train, val, test\n",
    "    X_tr_df[numeric_cols] = X_tr_df[numeric_cols].clip(lower=lower, upper=upper, axis=1)\n",
    "    X_va_df[numeric_cols] = X_va_df[numeric_cols].clip(lower=lower, upper=upper, axis=1)\n",
    "    X_te_df[numeric_cols] = X_te_df[numeric_cols].clip(lower=lower, upper=upper, axis=1)\n",
    "\n",
    "    # now fit scaler on numeric only\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', StandardScaler(), numeric_cols),\n",
    "        ('cat', 'passthrough',  cat_cols)\n",
    "    ])\n",
    "    preprocessor.fit(X_tr_df)\n",
    "    preprocessors[y] = preprocessor\n",
    "\n",
    "    # transform all splits\n",
    "    X_train[y] = preprocessor.transform(X_tr_df).astype('float32')\n",
    "    X_val[y]   = preprocessor.transform(X_va_df).astype('float32')\n",
    "    X_test[y]  = preprocessor.transform(X_te_df).astype('float32')\n",
    "\n",
    "    # fit standard scaler on y values\n",
    "    y_scaler = StandardScaler()\n",
    "    y_scaler.fit(y_tr.reshape(-1, 1))\n",
    "    y_scalers[y] = y_scaler\n",
    "    y_tr = y_scaler.transform(y_tr.reshape(-1, 1)).flatten()\n",
    "    y_va = y_scaler.transform(y_va.reshape(-1, 1)).flatten()\n",
    "    y_te = y_scaler.transform(y_te.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "    # store targets as before\n",
    "    y_train[y] = y_tr.reshape(-1, 1)\n",
    "    y_val[y]   = y_va.reshape(-1, 1)\n",
    "    y_test[y]  = y_te.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# moving to metal or CUDA GPU if available\n",
    "device = torch.device((\"cuda\" if torch.cuda.is_available() \n",
    "                       else \"mps\" if torch.backends.mps.is_available() \n",
    "                       else \"cpu\"))\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# general hyperparameters\n",
    "hidden_depth = None \n",
    "hidden_width = None \n",
    "learning_rate = 5e-5 # 1e-4 # 1e-5 \n",
    "activation_fun = nn.ReLU # nn.ReLU, nn.Tanh, nn.Sigmoid, nn.LeakyReLU\n",
    "\n",
    "# general critereon and regularization parameters\n",
    "criterion = AMADLTanhLoss(delta=0.5, k=20.0)\n",
    "lambda_l1 = 1e-4 # baseline l1 regularization # 1e-5\n",
    "lambda_l2 = 1e-4 # baseline l2 regularization\n",
    "drop = 0.0\n",
    "\n",
    "# general parmeters\n",
    "patience = 50\n",
    "print_freq = 100\n",
    "epochs = 250\n",
    "batch_size = 4096 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = '21'\n",
    "models_21 = {}\n",
    "n_runs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model loader fun\n",
    "def load_model(w, d, run, lambda_l1, lambda_l2, drop, lr, activ = None, crit = None):\n",
    "    m = MLPModel(\n",
    "        input_dim  = X_train[year].shape[1],\n",
    "        depth      = d,\n",
    "        width      = w,\n",
    "        dropout    = drop,\n",
    "        activation = activation_fun,\n",
    "    ).to(device)\n",
    "    path = (\n",
    "        f\"models/amadl/hyperparam_test/mlp_y{year}\"\n",
    "        f\"_l1{lambda_l1}_l2{lambda_l2}\"\n",
    "        f\"_drop{drop}_lr{lr}\"\n",
    "        f\"_w{w}_d{d}_run{run+1}.pth\"\n",
    "    )\n",
    "    if activ is not None:\n",
    "        path = path.replace('.pth', f'_activ{activ}.pth')\n",
    "    if crit is not None:\n",
    "        path = path.replace('.pth', f'_crit{crit}.pth')\n",
    "    m.load_state_dict(torch.load(path, map_location=device))\n",
    "    m.eval()\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Test hyperparameters](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_1_'></a>[Constant scheme](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model with most data on multiple parameters\n",
    "l1_space = [lambda_l1]\n",
    "l2_space = [lambda_l2]\n",
    "dropout_space = [drop]\n",
    "learning_rate_space = [learning_rate]\n",
    "depth_space = [1, 2, 3, 4, 5, 6, 7]\n",
    "width_space = [8, 16, 32, 64, 128]\n",
    "best_models_size = {}\n",
    "history_size = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =1\n",
      "                                hidden_width    =8\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 4.26892E-01  - Val Loss: 5.39480E-01\n",
      "Epoch 200/250  - Train Loss: 4.10426E-01  - Val Loss: 5.26012E-01\n",
      "Best val loss: 5.20427E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w8_d1_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 4.38129E-01  - Val Loss: 5.54602E-01\n",
      "Early stopping at epoch 101\n",
      "Best val loss: 5.53342E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w8_d1_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 4.41540E-01  - Val Loss: 6.09969E-01\n",
      "Epoch 200/250  - Train Loss: 4.21061E-01  - Val Loss: 5.92798E-01\n",
      "Best val loss: 5.91717E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w8_d1_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 4.29425E-01  - Val Loss: 5.27743E-01\n",
      "Early stopping at epoch 107\n",
      "Best val loss: 5.22880E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w8_d1_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 4.33080E-01  - Val Loss: 5.34997E-01\n",
      "Early stopping at epoch 137\n",
      "Best val loss: 5.31109E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w8_d1_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =1\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 65\n",
      "Best val loss: 5.06508E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w16_d1_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 4.29210E-01  - Val Loss: 5.06759E-01\n",
      "Early stopping at epoch 155\n",
      "Best val loss: 5.06617E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w16_d1_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 4.21533E-01  - Val Loss: 5.16464E-01\n",
      "Epoch 200/250  - Train Loss: 4.03208E-01  - Val Loss: 5.17228E-01\n",
      "Early stopping at epoch 205\n",
      "Best val loss: 5.12237E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w16_d1_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 4.20619E-01  - Val Loss: 5.39887E-01\n",
      "Epoch 200/250  - Train Loss: 4.04089E-01  - Val Loss: 5.40951E-01\n",
      "Early stopping at epoch 219\n",
      "Best val loss: 5.36643E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w16_d1_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 4.24404E-01  - Val Loss: 5.41757E-01\n",
      "Epoch 200/250  - Train Loss: 4.05276E-01  - Val Loss: 5.40599E-01\n",
      "Early stopping at epoch 201\n",
      "Best val loss: 5.36305E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w16_d1_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =1\n",
      "                                hidden_width    =32\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 4.29597E-01  - Val Loss: 5.48311E-01\n",
      "Early stopping at epoch 101\n",
      "Best val loss: 5.37046E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w32_d1_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 78\n",
      "Best val loss: 5.16618E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w32_d1_run2.pth\n",
      "Run 3 of 5\n",
      "Early stopping at epoch 69\n",
      "Best val loss: 4.98536E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w32_d1_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 4.23549E-01  - Val Loss: 5.74440E-01\n",
      "Early stopping at epoch 109\n",
      "Best val loss: 5.69917E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w32_d1_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 4.23076E-01  - Val Loss: 5.60299E-01\n",
      "Early stopping at epoch 143\n",
      "Best val loss: 5.46497E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w32_d1_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =1\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 72\n",
      "Best val loss: 5.07354E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w64_d1_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 78\n",
      "Best val loss: 5.09152E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w64_d1_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 4.40069E-01  - Val Loss: 5.36431E-01\n",
      "Early stopping at epoch 112\n",
      "Best val loss: 5.28863E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w64_d1_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 91\n",
      "Best val loss: 5.00194E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w64_d1_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 80\n",
      "Best val loss: 4.90196E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w64_d1_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =1\n",
      "                                hidden_width    =128\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 4.72808E-01  - Val Loss: 5.26790E-01\n",
      "Early stopping at epoch 114\n",
      "Best val loss: 5.16612E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w128_d1_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 93\n",
      "Best val loss: 5.33894E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w128_d1_run2.pth\n",
      "Run 3 of 5\n",
      "Early stopping at epoch 75\n",
      "Best val loss: 5.03635E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w128_d1_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 4.71251E-01  - Val Loss: 5.43090E-01\n",
      "Early stopping at epoch 192\n",
      "Best val loss: 5.35334E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w128_d1_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 60\n",
      "Best val loss: 5.16665E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w128_d1_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =8\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 4.29906E-01  - Val Loss: 5.72375E-01\n",
      "Early stopping at epoch 115\n",
      "Best val loss: 5.67582E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w8_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 4.38352E-01  - Val Loss: 5.02813E-01\n",
      "Early stopping at epoch 111\n",
      "Best val loss: 4.98595E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w8_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 4.33399E-01  - Val Loss: 5.48794E-01\n",
      "Epoch 200/250  - Train Loss: 4.21230E-01  - Val Loss: 5.22642E-01\n",
      "Best val loss: 5.16913E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w8_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 4.33560E-01  - Val Loss: 5.31680E-01\n",
      "Early stopping at epoch 121\n",
      "Best val loss: 5.20998E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w8_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 4.35489E-01  - Val Loss: 5.47734E-01\n",
      "Early stopping at epoch 190\n",
      "Best val loss: 5.40968E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w8_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 86\n",
      "Best val loss: 5.06285E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w16_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 4.30457E-01  - Val Loss: 5.25227E-01\n",
      "Early stopping at epoch 153\n",
      "Best val loss: 5.24497E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w16_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 4.38635E-01  - Val Loss: 5.55313E-01\n",
      "Early stopping at epoch 193\n",
      "Best val loss: 5.47500E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w16_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 4.30822E-01  - Val Loss: 5.52262E-01\n",
      "Epoch 200/250  - Train Loss: 4.09900E-01  - Val Loss: 5.31212E-01\n",
      "Best val loss: 5.29669E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w16_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 4.34346E-01  - Val Loss: 5.57554E-01\n",
      "Epoch 200/250  - Train Loss: 4.12805E-01  - Val Loss: 5.40661E-01\n",
      "Early stopping at epoch 210\n",
      "Best val loss: 5.37522E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w16_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =32\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 4.44892E-01  - Val Loss: 5.14604E-01\n",
      "Early stopping at epoch 131\n",
      "Best val loss: 5.13585E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w32_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 4.41576E-01  - Val Loss: 5.23278E-01\n",
      "Early stopping at epoch 154\n",
      "Best val loss: 5.21538E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w32_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 4.43612E-01  - Val Loss: 5.30430E-01\n",
      "Early stopping at epoch 158\n",
      "Best val loss: 5.29198E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w32_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 4.33314E-01  - Val Loss: 5.47265E-01\n",
      "Epoch 200/250  - Train Loss: 4.08758E-01  - Val Loss: 5.37773E-01\n",
      "Best val loss: 5.31538E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w32_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 75\n",
      "Best val loss: 5.00072E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w32_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 4.64898E-01  - Val Loss: 5.34997E-01\n",
      "Early stopping at epoch 110\n",
      "Best val loss: 5.31649E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 51\n",
      "Best val loss: 4.95409E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Early stopping at epoch 84\n",
      "Best val loss: 5.15365E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 51\n",
      "Best val loss: 5.03402E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 4.60404E-01  - Val Loss: 5.01798E-01\n",
      "Early stopping at epoch 163\n",
      "Best val loss: 4.99593E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =128\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 98\n",
      "Best val loss: 5.08598E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w128_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 58\n",
      "Best val loss: 5.03484E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w128_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 5.32114E-01  - Val Loss: 5.45223E-01\n",
      "Epoch 200/250  - Train Loss: 4.64805E-01  - Val Loss: 5.28498E-01\n",
      "Best val loss: 5.23579E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w128_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 86\n",
      "Best val loss: 5.10526E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w128_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 67\n",
      "Best val loss: 5.12726E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w128_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =3\n",
      "                                hidden_width    =8\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 4.46799E-01  - Val Loss: 4.95224E-01\n",
      "Early stopping at epoch 157\n",
      "Best val loss: 4.95092E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w8_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 4.48838E-01  - Val Loss: 5.88799E-01\n",
      "Epoch 200/250  - Train Loss: 4.38632E-01  - Val Loss: 5.80682E-01\n",
      "Early stopping at epoch 238\n",
      "Best val loss: 5.80269E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w8_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 4.42880E-01  - Val Loss: 5.35943E-01\n",
      "Early stopping at epoch 134\n",
      "Best val loss: 5.35617E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w8_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 72\n",
      "Best val loss: 5.02389E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w8_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 4.41240E-01  - Val Loss: 5.10876E-01\n",
      "Early stopping at epoch 144\n",
      "Best val loss: 5.10241E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w8_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =3\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 4.42521E-01  - Val Loss: 5.04190E-01\n",
      "Early stopping at epoch 103\n",
      "Best val loss: 4.95868E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w16_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 4.33824E-01  - Val Loss: 5.17111E-01\n",
      "Early stopping at epoch 154\n",
      "Best val loss: 5.15825E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w16_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 4.40515E-01  - Val Loss: 5.06916E-01\n",
      "Early stopping at epoch 152\n",
      "Best val loss: 5.06569E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w16_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 4.39891E-01  - Val Loss: 5.30734E-01\n",
      "Epoch 200/250  - Train Loss: 4.19292E-01  - Val Loss: 5.26075E-01\n",
      "Best val loss: 5.24084E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w16_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 4.44011E-01  - Val Loss: 5.10130E-01\n",
      "Early stopping at epoch 138\n",
      "Best val loss: 5.09645E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w16_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =3\n",
      "                                hidden_width    =32\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 4.53471E-01  - Val Loss: 5.11193E-01\n",
      "Early stopping at epoch 102\n",
      "Best val loss: 4.99586E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w32_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 63\n",
      "Best val loss: 4.99611E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w32_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Early stopping at epoch 51\n",
      "Best val loss: 5.00668E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w32_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 4.52481E-01  - Val Loss: 5.25409E-01\n",
      "Early stopping at epoch 158\n",
      "Best val loss: 5.25323E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w32_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 4.43012E-01  - Val Loss: 5.14580E-01\n",
      "Early stopping at epoch 137\n",
      "Best val loss: 5.13348E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w32_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =3\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 51\n",
      "Best val loss: 5.09761E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w64_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 51\n",
      "Best val loss: 5.00177E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w64_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 4.82573E-01  - Val Loss: 5.31887E-01\n",
      "Early stopping at epoch 110\n",
      "Best val loss: 5.18404E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w64_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 70\n",
      "Best val loss: 5.07776E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w64_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 51\n",
      "Best val loss: 4.96198E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w64_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =3\n",
      "                                hidden_width    =128\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 69\n",
      "Best val loss: 5.30174E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w128_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 51\n",
      "Best val loss: 5.07864E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w128_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Early stopping at epoch 68\n",
      "Best val loss: 5.13240E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w128_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 51\n",
      "Best val loss: 5.01579E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w128_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 51\n",
      "Best val loss: 5.09376E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w128_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =4\n",
      "                                hidden_width    =8\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 88\n",
      "Best val loss: 5.00221E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w8_d4_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 4.41420E-01  - Val Loss: 4.98626E-01\n",
      "Early stopping at epoch 138\n",
      "Best val loss: 4.98453E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w8_d4_run2.pth\n",
      "Run 3 of 5\n",
      "Early stopping at epoch 51\n",
      "Best val loss: 4.99902E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w8_d4_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 5.37919E-01  - Val Loss: 7.08376E-01\n",
      "Epoch 200/250  - Train Loss: 5.25487E-01  - Val Loss: 6.92621E-01\n",
      "Best val loss: 6.85474E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w8_d4_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 4.47331E-01  - Val Loss: 5.58529E-01\n",
      "Early stopping at epoch 174\n",
      "Best val loss: 5.55512E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w8_d4_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =4\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 4.47223E-01  - Val Loss: 5.29134E-01\n",
      "Early stopping at epoch 179\n",
      "Best val loss: 5.27147E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w16_d4_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 53\n",
      "Best val loss: 4.99600E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w16_d4_run2.pth\n",
      "Run 3 of 5\n",
      "Early stopping at epoch 51\n",
      "Best val loss: 5.05167E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w16_d4_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 4.39487E-01  - Val Loss: 5.32116E-01\n",
      "Early stopping at epoch 198\n",
      "Best val loss: 5.28010E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w16_d4_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 4.42355E-01  - Val Loss: 5.25438E-01\n",
      "Early stopping at epoch 152\n",
      "Best val loss: 5.24553E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w16_d4_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =4\n",
      "                                hidden_width    =32\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 4.51501E-01  - Val Loss: 5.30319E-01\n",
      "Early stopping at epoch 143\n",
      "Best val loss: 5.28020E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w32_d4_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 68\n",
      "Best val loss: 4.99700E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w32_d4_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 4.58253E-01  - Val Loss: 5.22030E-01\n",
      "Early stopping at epoch 101\n",
      "Best val loss: 5.09350E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w32_d4_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 4.46464E-01  - Val Loss: 5.30518E-01\n",
      "Early stopping at epoch 180\n",
      "Best val loss: 5.26834E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w32_d4_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 4.56011E-01  - Val Loss: 5.08933E-01\n",
      "Early stopping at epoch 120\n",
      "Best val loss: 5.07450E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w32_d4_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =4\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 51\n",
      "Best val loss: 5.00766E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w64_d4_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 84\n",
      "Best val loss: 5.06448E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w64_d4_run2.pth\n",
      "Run 3 of 5\n",
      "Early stopping at epoch 51\n",
      "Best val loss: 5.00640E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w64_d4_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 51\n",
      "Best val loss: 5.01531E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w64_d4_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 4.78709E-01  - Val Loss: 5.10633E-01\n",
      "Epoch 200/250  - Train Loss: 4.15503E-01  - Val Loss: 5.10231E-01\n",
      "Early stopping at epoch 205\n",
      "Best val loss: 5.05867E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w64_d4_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =4\n",
      "                                hidden_width    =128\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 73\n",
      "Best val loss: 5.04428E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w128_d4_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 85\n",
      "Best val loss: 5.09452E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w128_d4_run2.pth\n",
      "Run 3 of 5\n",
      "Early stopping at epoch 67\n",
      "Best val loss: 5.04558E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w128_d4_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 61\n",
      "Best val loss: 5.13918E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w128_d4_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 76\n",
      "Best val loss: 5.08620E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w128_d4_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =8\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 4.51656E-01  - Val Loss: 5.34557E-01\n",
      "Epoch 200/250  - Train Loss: 4.23936E-01  - Val Loss: 5.26576E-01\n",
      "Early stopping at epoch 224\n",
      "Best val loss: 5.24688E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w8_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 4.60728E-01  - Val Loss: 5.25792E-01\n",
      "Epoch 200/250  - Train Loss: 4.30262E-01  - Val Loss: 5.15181E-01\n",
      "Best val loss: 5.12783E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w8_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 4.48469E-01  - Val Loss: 5.09783E-01\n",
      "Early stopping at epoch 175\n",
      "Best val loss: 5.08853E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w8_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 4.56668E-01  - Val Loss: 5.29490E-01\n",
      "Early stopping at epoch 149\n",
      "Best val loss: 5.28576E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w8_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 98\n",
      "Best val loss: 4.99520E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w8_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 4.46256E-01  - Val Loss: 5.07855E-01\n",
      "Early stopping at epoch 113\n",
      "Best val loss: 5.05472E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w16_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 4.49069E-01  - Val Loss: 5.51413E-01\n",
      "Epoch 200/250  - Train Loss: 4.19759E-01  - Val Loss: 5.34041E-01\n",
      "Best val loss: 5.29127E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w16_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Early stopping at epoch 51\n",
      "Best val loss: 6.42598E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w16_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 83\n",
      "Best val loss: 5.00010E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w16_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 4.51727E-01  - Val Loss: 5.51009E-01\n",
      "Epoch 200/250  - Train Loss: 4.27371E-01  - Val Loss: 5.39855E-01\n",
      "Early stopping at epoch 229\n",
      "Best val loss: 5.32336E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w16_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =32\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 4.59881E-01  - Val Loss: 5.27813E-01\n",
      "Epoch 200/250  - Train Loss: 4.14284E-01  - Val Loss: 5.14630E-01\n",
      "Best val loss: 5.11565E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w32_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 4.52505E-01  - Val Loss: 5.24453E-01\n",
      "Early stopping at epoch 120\n",
      "Best val loss: 5.16964E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w32_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 4.52003E-01  - Val Loss: 5.31549E-01\n",
      "Early stopping at epoch 120\n",
      "Best val loss: 5.20235E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w32_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 4.53055E-01  - Val Loss: 5.09029E-01\n",
      "Early stopping at epoch 126\n",
      "Best val loss: 5.02567E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w32_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 51\n",
      "Best val loss: 5.00599E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w32_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 5.02402E-01  - Val Loss: 5.37381E-01\n",
      "Early stopping at epoch 108\n",
      "Best val loss: 5.03877E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w64_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 51\n",
      "Best val loss: 5.00133E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w64_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Early stopping at epoch 79\n",
      "Best val loss: 5.10683E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w64_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 51\n",
      "Best val loss: 5.02898E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w64_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 89\n",
      "Best val loss: 5.11351E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w64_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =5\n",
      "                                hidden_width    =128\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 81\n",
      "Best val loss: 5.05280E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w128_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 51\n",
      "Best val loss: 5.09777E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w128_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Early stopping at epoch 51\n",
      "Best val loss: 5.10088E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w128_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 69\n",
      "Best val loss: 5.08298E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w128_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 85\n",
      "Best val loss: 4.99923E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w128_d5_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =6\n",
      "                                hidden_width    =8\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 51\n",
      "Best val loss: 5.00625E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w8_d6_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 95\n",
      "Best val loss: 4.99970E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w8_d6_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 5.15568E-01  - Val Loss: 6.62202E-01\n",
      "Epoch 200/250  - Train Loss: 4.45000E-01  - Val Loss: 5.13421E-01\n",
      "Best val loss: 5.13343E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w8_d6_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 51\n",
      "Best val loss: 5.20321E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w8_d6_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 5.44952E-01  - Val Loss: 5.10902E-01\n",
      "Epoch 200/250  - Train Loss: 5.07949E-01  - Val Loss: 5.00033E-01\n",
      "Early stopping at epoch 245\n",
      "Best val loss: 4.99985E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w8_d6_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =6\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 51\n",
      "Best val loss: 5.06326E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w16_d6_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 51\n",
      "Best val loss: 5.00099E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w16_d6_run2.pth\n",
      "Run 3 of 5\n",
      "Early stopping at epoch 51\n",
      "Best val loss: 5.13670E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w16_d6_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 4.57290E-01  - Val Loss: 5.26160E-01\n",
      "Early stopping at epoch 149\n",
      "Best val loss: 5.25142E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w16_d6_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 51\n",
      "Best val loss: 5.03537E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w16_d6_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =6\n",
      "                                hidden_width    =32\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 4.57158E-01  - Val Loss: 5.19377E-01\n",
      "Early stopping at epoch 103\n",
      "Best val loss: 5.08630E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w32_d6_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 4.59080E-01  - Val Loss: 5.18387E-01\n",
      "Early stopping at epoch 119\n",
      "Best val loss: 5.14450E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w32_d6_run2.pth\n",
      "Run 3 of 5\n",
      "Early stopping at epoch 60\n",
      "Best val loss: 4.99949E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w32_d6_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 4.71541E-01  - Val Loss: 5.15059E-01\n",
      "Early stopping at epoch 155\n",
      "Best val loss: 5.11953E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w32_d6_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 4.54670E-01  - Val Loss: 5.24099E-01\n",
      "Early stopping at epoch 109\n",
      "Best val loss: 5.13502E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w32_d6_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =6\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 97\n",
      "Best val loss: 5.02914E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w64_d6_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 99\n",
      "Best val loss: 5.02682E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w64_d6_run2.pth\n",
      "Run 3 of 5\n",
      "Early stopping at epoch 51\n",
      "Best val loss: 5.00059E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w64_d6_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 86\n",
      "Best val loss: 5.13027E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w64_d6_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 51\n",
      "Best val loss: 5.00318E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w64_d6_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =6\n",
      "                                hidden_width    =128\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 70\n",
      "Best val loss: 5.10601E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w128_d6_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 51\n",
      "Best val loss: 5.02531E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w128_d6_run2.pth\n",
      "Run 3 of 5\n",
      "Early stopping at epoch 78\n",
      "Best val loss: 5.08369E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w128_d6_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 77\n",
      "Best val loss: 5.07910E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w128_d6_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 51\n",
      "Best val loss: 6.51805E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w128_d6_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =7\n",
      "                                hidden_width    =8\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 5.15356E-01  - Val Loss: 5.00011E-01\n",
      "Early stopping at epoch 162\n",
      "Best val loss: 4.99949E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w8_d7_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 51\n",
      "Best val loss: 6.45222E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w8_d7_run2.pth\n",
      "Run 3 of 5\n",
      "Early stopping at epoch 51\n",
      "Best val loss: 6.37473E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w8_d7_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 5.14927E-01  - Val Loss: 6.55503E-01\n",
      "Early stopping at epoch 182\n",
      "Best val loss: 6.54639E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w8_d7_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 51\n",
      "Best val loss: 5.01147E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w8_d7_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =7\n",
      "                                hidden_width    =16\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 96\n",
      "Best val loss: 4.99952E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w16_d7_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 5.15370E-01  - Val Loss: 6.29049E-01\n",
      "Early stopping at epoch 184\n",
      "Best val loss: 5.13111E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w16_d7_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 4.56715E-01  - Val Loss: 5.17345E-01\n",
      "Early stopping at epoch 129\n",
      "Best val loss: 5.14980E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w16_d7_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 5.26212E-01  - Val Loss: 6.63762E-01\n",
      "Epoch 200/250  - Train Loss: 5.18790E-01  - Val Loss: 6.55080E-01\n",
      "Best val loss: 6.54607E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w16_d7_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 5.23438E-01  - Val Loss: 6.51392E-01\n",
      "Early stopping at epoch 177\n",
      "Best val loss: 5.06248E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w16_d7_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =7\n",
      "                                hidden_width    =32\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 4.64917E-01  - Val Loss: 5.17729E-01\n",
      "Early stopping at epoch 107\n",
      "Best val loss: 5.10195E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w32_d7_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 5.42259E-01  - Val Loss: 6.54818E-01\n",
      "Early stopping at epoch 120\n",
      "Best val loss: 6.54590E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w32_d7_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 4.66520E-01  - Val Loss: 5.07371E-01\n",
      "Early stopping at epoch 106\n",
      "Best val loss: 5.05065E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w32_d7_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 51\n",
      "Best val loss: 5.01554E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w32_d7_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 5.41486E-01  - Val Loss: 6.54896E-01\n",
      "Early stopping at epoch 118\n",
      "Best val loss: 6.54546E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w32_d7_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =7\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 51\n",
      "Best val loss: 5.04758E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w64_d7_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 51\n",
      "Best val loss: 6.41955E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w64_d7_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 5.71847E-01  - Val Loss: 6.54854E-01\n",
      "Early stopping at epoch 112\n",
      "Best val loss: 6.54407E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w64_d7_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 78\n",
      "Best val loss: 5.10414E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w64_d7_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 52\n",
      "Best val loss: 4.99988E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w64_d7_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =0e+00\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =7\n",
      "                                hidden_width    =128\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 84\n",
      "Best val loss: 5.04467E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w128_d7_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 70\n",
      "Best val loss: 6.53585E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w128_d7_run2.pth\n",
      "Run 3 of 5\n",
      "Early stopping at epoch 51\n",
      "Best val loss: 5.02703E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w128_d7_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 80\n",
      "Best val loss: 5.06966E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w128_d7_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 83\n",
      "Best val loss: 5.04585E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w128_d7_run5.pth\n"
     ]
    }
   ],
   "source": [
    "# # current runtime: 2h 15m at 5 runs per model\n",
    "# for lambda_l1 in l1_space:\n",
    "#     for lambda_l2 in l2_space:\n",
    "#         for dropout in dropout_space:\n",
    "#             for learning_rate in learning_rate_space:\n",
    "#                 for hidden_depth in depth_space:\n",
    "#                     for hidden_width in width_space:\n",
    "#                         print(f\"\"\"Training model for year '{year}...: \n",
    "#                                 lambda_l1       ={lambda_l1:.0e}\n",
    "#                                 lambda_l2       ={lambda_l2:.0e}\n",
    "#                                 dropout         ={dropout:.0e}\n",
    "#                                 learning_rate   ={learning_rate:.0e}\n",
    "#                                 hidden_depth    ={hidden_depth}\n",
    "#                                 hidden_width    ={hidden_width}\"\"\")\n",
    "#                         for run in range(n_runs):\n",
    "#                             print(f\"Run {run+1} of {n_runs}\")\n",
    "#                             seed = 42+run   \n",
    "#                             np.random.seed(seed)\n",
    "#                             torch.manual_seed(seed)\n",
    "#                             # Initialize the model\n",
    "#                             input_dim = X_train[year].shape[1]\n",
    "#                             name = f'l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}'\n",
    "#                             models_21[name] = MLPModel(input_dim, depth=hidden_depth, width=hidden_width, dropout=dropout, activation=activation_fun).to(device)\n",
    "#                             optimizer = torch.optim.Adam(models_21[name].parameters(), lr=learning_rate)\n",
    "#                             train = MLPdataset(X_train[year], y_train[year])\n",
    "#                             val = MLPdataset(X_val[year], y_val[year])\n",
    "#                             best_models_size[name], history_size[name] = train_mlp(train,          \n",
    "#                                                             val,\n",
    "#                                                             models_21[name],\n",
    "#                                                             criterion,\n",
    "#                                                             epochs,\n",
    "#                                                             patience,\n",
    "#                                                             print_freq,\n",
    "#                                                             device,\n",
    "#                                                             optimizer,\n",
    "#                                                             lambda_l1=lambda_l1,\n",
    "#                                                             lambda_l2=lambda_l2,\n",
    "#                                                             batch_size=batch_size,\n",
    "#                                                             shuffle_train=True,\n",
    "#                                                             shuffle_val=False,\n",
    "#                                                             save_path=f'models/amadl/hyperparam_test/mlp_y{year}_l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}.pth'\n",
    "#                                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten_history(history_size, \n",
    "#                 save_csv='models/amadl/hyperparam_test/history/history_size_critAMADL.csv')\n",
    "# gc.collect()\n",
    "# torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_2_'></a>[Pyramid scheme](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model with most data on multiple parameters\n",
    "l1_space = [lambda_l1]\n",
    "l2_space = [lambda_l2]\n",
    "dropout_space = [drop]\n",
    "learning_rate_space = [learning_rate]\n",
    "depth_space = None\n",
    "width_space = [[32], \n",
    "               [32, 16], \n",
    "               [32, 16, 8], \n",
    "               [32, 16, 8, 4], \n",
    "               [32, 16, 8, 4, 2]]\n",
    "best_models_pyramid = {}\n",
    "history_pyramid = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-04\n",
      "                            lambda_l2       =1e-04\n",
      "                            dropout         =0e+00\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =1\n",
      "                            hidden_width    =[32]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 4.29597E-01  - Val Loss: 5.48311E-01\n",
      "Early stopping at epoch 101\n",
      "Best val loss: 5.37046E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w[32]_d1_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 78\n",
      "Best val loss: 5.16618E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w[32]_d1_run2.pth\n",
      "Run 3 of 5\n",
      "Early stopping at epoch 69\n",
      "Best val loss: 4.98536E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w[32]_d1_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 4.23549E-01  - Val Loss: 5.74440E-01\n",
      "Early stopping at epoch 109\n",
      "Best val loss: 5.69917E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w[32]_d1_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 4.23076E-01  - Val Loss: 5.60299E-01\n",
      "Early stopping at epoch 143\n",
      "Best val loss: 5.46497E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w[32]_d1_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-04\n",
      "                            lambda_l2       =1e-04\n",
      "                            dropout         =0e+00\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =2\n",
      "                            hidden_width    =[32, 16]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 4.36830E-01  - Val Loss: 5.33510E-01\n",
      "Early stopping at epoch 131\n",
      "Best val loss: 5.30310E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w[32, 16]_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 97\n",
      "Best val loss: 5.30187E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w[32, 16]_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 4.42571E-01  - Val Loss: 5.28452E-01\n",
      "Early stopping at epoch 115\n",
      "Best val loss: 5.18578E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w[32, 16]_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 87\n",
      "Best val loss: 4.92665E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w[32, 16]_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 4.36847E-01  - Val Loss: 5.89838E-01\n",
      "Epoch 200/250  - Train Loss: 4.09681E-01  - Val Loss: 5.70692E-01\n",
      "Best val loss: 5.63951E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w[32, 16]_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-04\n",
      "                            lambda_l2       =1e-04\n",
      "                            dropout         =0e+00\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 4.45941E-01  - Val Loss: 4.95903E-01\n",
      "Early stopping at epoch 119\n",
      "Best val loss: 4.94819E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 4.37219E-01  - Val Loss: 5.11219E-01\n",
      "Early stopping at epoch 128\n",
      "Best val loss: 5.08822E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 4.36196E-01  - Val Loss: 5.38655E-01\n",
      "Epoch 200/250  - Train Loss: 4.09832E-01  - Val Loss: 5.41656E-01\n",
      "Early stopping at epoch 204\n",
      "Best val loss: 5.36628E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 4.42324E-01  - Val Loss: 5.65994E-01\n",
      "Epoch 200/250  - Train Loss: 4.10154E-01  - Val Loss: 5.51202E-01\n",
      "Best val loss: 5.40971E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 4.50513E-01  - Val Loss: 4.96973E-01\n",
      "Epoch 200/250  - Train Loss: 4.19921E-01  - Val Loss: 4.92574E-01\n",
      "Early stopping at epoch 242\n",
      "Best val loss: 4.91572E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-04\n",
      "                            lambda_l2       =1e-04\n",
      "                            dropout         =0e+00\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =4\n",
      "                            hidden_width    =[32, 16, 8, 4]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 4.57828E-01  - Val Loss: 5.75556E-01\n",
      "Epoch 200/250  - Train Loss: 4.29324E-01  - Val Loss: 5.48298E-01\n",
      "Best val loss: 5.36562E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w[32, 16, 8, 4]_d4_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 77\n",
      "Best val loss: 4.98470E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w[32, 16, 8, 4]_d4_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 4.63816E-01  - Val Loss: 5.67472E-01\n",
      "Epoch 200/250  - Train Loss: 4.31321E-01  - Val Loss: 5.45891E-01\n",
      "Early stopping at epoch 221\n",
      "Best val loss: 5.42790E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w[32, 16, 8, 4]_d4_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 5.52313E-01  - Val Loss: 5.13572E-01\n",
      "Epoch 200/250  - Train Loss: 5.26996E-01  - Val Loss: 5.04390E-01\n",
      "Best val loss: 5.00240E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w[32, 16, 8, 4]_d4_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 4.50218E-01  - Val Loss: 5.14487E-01\n",
      "Early stopping at epoch 148\n",
      "Best val loss: 5.13745E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w[32, 16, 8, 4]_d4_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-04\n",
      "                            lambda_l2       =1e-04\n",
      "                            dropout         =0e+00\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =5\n",
      "                            hidden_width    =[32, 16, 8, 4, 2]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 5.19058E-01  - Val Loss: 5.00391E-01\n",
      "Early stopping at epoch 175\n",
      "Best val loss: 4.99883E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w[32, 16, 8, 4, 2]_d5_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 51\n",
      "Best val loss: 5.00275E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w[32, 16, 8, 4, 2]_d5_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 5.92227E-01  - Val Loss: 5.32963E-01\n",
      "Epoch 200/250  - Train Loss: 5.68149E-01  - Val Loss: 5.22943E-01\n",
      "Best val loss: 5.18462E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w[32, 16, 8, 4, 2]_d5_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 5.29294E-01  - Val Loss: 5.03854E-01\n",
      "Epoch 200/250  - Train Loss: 5.11740E-01  - Val Loss: 5.00024E-01\n",
      "Best val loss: 4.99948E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w[32, 16, 8, 4, 2]_d5_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 6.36357E-01  - Val Loss: 8.42375E-01\n",
      "Epoch 200/250  - Train Loss: 6.10734E-01  - Val Loss: 8.13006E-01\n",
      "Best val loss: 7.99459E-01\n",
      "Model saved to models/amadl/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.0_lr5e-05_w[32, 16, 8, 4, 2]_d5_run5.pth\n"
     ]
    }
   ],
   "source": [
    "# # current runtime: 30m\n",
    "# for lambda_l1 in l1_space:\n",
    "#     for lambda_l2 in l2_space:\n",
    "#         for dropout in dropout_space:\n",
    "#             for learning_rate in learning_rate_space:\n",
    "#                 for hidden_width in width_space:\n",
    "#                     hidden_depth = len(hidden_width)\n",
    "#                     print(f\"\"\"Training model for year '{year}...: \n",
    "#                             lambda_l1       ={lambda_l1:.0e}\n",
    "#                             lambda_l2       ={lambda_l2:.0e}\n",
    "#                             dropout         ={dropout:.0e}\n",
    "#                             learning_rate   ={learning_rate:.0e}\n",
    "#                             hidden_depth    ={hidden_depth}\n",
    "#                             hidden_width    ={hidden_width}\"\"\")\n",
    "#                     for run in range(n_runs):\n",
    "#                         print(f\"Run {run+1} of {n_runs}\")\n",
    "#                         seed = 42+run\n",
    "#                         np.random.seed(seed)\n",
    "#                         torch.manual_seed(seed)\n",
    "#                         # Initialize the model\n",
    "#                         input_dim = X_train[year].shape[1]\n",
    "#                         name = f'l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}'\n",
    "#                         models_21[name] = MLPModel(input_dim, depth=hidden_depth, width=hidden_width, dropout=dropout, activation=activation_fun).to(device)\n",
    "#                         optimizer = torch.optim.Adam(models_21[name].parameters(), lr=learning_rate)\n",
    "#                         train = MLPdataset(X_train[year], y_train[year])\n",
    "#                         val = MLPdataset(X_val[year], y_val[year])\n",
    "#                         best_models_pyramid[name], history_pyramid[name] = train_mlp(train,          \n",
    "#                                                         val,\n",
    "#                                                         models_21[name],\n",
    "#                                                         criterion,\n",
    "#                                                         epochs,\n",
    "#                                                         patience,\n",
    "#                                                         print_freq,\n",
    "#                                                         device,\n",
    "#                                                         optimizer,\n",
    "#                                                         lambda_l1=lambda_l1,\n",
    "#                                                         lambda_l2=lambda_l2,\n",
    "#                                                         batch_size=batch_size,\n",
    "#                                                         shuffle_train=True,\n",
    "#                                                         shuffle_val=False,\n",
    "#                                                         save_path=f'models/amadl/hyperparam_test/mlp_y{year}_l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}.pth'\n",
    "#                                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten_history(history_pyramid, \n",
    "#                 save_csv='models/amadl/hyperparam_test/history/history_pyramid_critAMADL.csv')\n",
    "# gc.collect()\n",
    "# torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_2_1_'></a>[Depth and width comparison](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant-width grid\n",
    "const_widths = [8, 16, 32, 64, 128]\n",
    "const_depths = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "# pyramid sequences (each column of the right heatmap)\n",
    "pyr_seqs = [\n",
    "    [32],\n",
    "    [32,16],\n",
    "    [32,16,8],\n",
    "    [32,16,8,4],\n",
    "    [32,16,8,4,2],\n",
    "]\n",
    "Wc = len(const_widths)\n",
    "Dc = len(const_depths)\n",
    "Wp = len(pyr_seqs)\n",
    "\n",
    "# DataLoader\n",
    "val_ds     = MLPdataset(X_val[year], y_val[year])\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# constant-width loss matrix\n",
    "loss_const = np.zeros((Dc, Wc))\n",
    "for di, d in enumerate(const_depths):\n",
    "    for wj, w in enumerate(const_widths):\n",
    "        run_losses = []\n",
    "        for run in range(n_runs):\n",
    "            m = load_model(w, d, run, lambda_l1, lambda_l2, drop, learning_rate)\n",
    "            with torch.no_grad():\n",
    "                batch = [criterion(m(x.to(device)), y.to(device)).item()\n",
    "                         for x,y in val_loader]\n",
    "            run_losses.append(np.mean(batch))\n",
    "        loss_const[di, wj] = np.mean(run_losses)\n",
    "\n",
    "# pyramid loss matrix\n",
    "loss_pyr = np.full((Wp, Wp), np.nan)\n",
    "for sj, seq in enumerate(pyr_seqs):\n",
    "    depth = len(seq)\n",
    "    run_losses = []\n",
    "    for run in range(n_runs):\n",
    "        # we “flatten” seq into width param for loader\n",
    "        m = load_model(seq, depth, run, lambda_l1, lambda_l2, drop, learning_rate)\n",
    "        with torch.no_grad():\n",
    "            batch = [criterion(m(x.to(device)), y.to(device)).item()\n",
    "                     for x,y in val_loader]\n",
    "        run_losses.append(np.mean(batch))\n",
    "    loss_pyr[depth-1, sj] = np.mean(run_losses)\n",
    "\n",
    "# plot\n",
    "fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(12,5),\n",
    "                               gridspec_kw={'width_ratios':[4,1]})\n",
    "\n",
    "# color‐scale\n",
    "all_vals = loss_const.ravel()\n",
    "vmin    = all_vals.min()\n",
    "vmax    = all_vals.max()+0.005\n",
    "vcenter = vmin + 0.5*(vmax - vmin)\n",
    "norm    = TwoSlopeNorm(vmin=vmin, vcenter=vcenter, vmax=vmax)\n",
    "cmap    = 'viridis'\n",
    "\n",
    "# left: constant‐width heatmap\n",
    "im0 = ax0.imshow(loss_const, aspect='auto', cmap=cmap, norm=norm)\n",
    "im0 = ax0.imshow(loss_const, aspect='auto', cmap=cmap, norm=norm)\n",
    "ax0.set_xticks(range(Wc))\n",
    "ax0.set_xticklabels(const_widths)\n",
    "ax0.set_yticks(range(Dc))\n",
    "ax0.set_yticklabels(const_depths)\n",
    "ax0.set_xlabel('Hidden Width')\n",
    "ax0.set_ylabel('Hidden Depth')\n",
    "ax0.set_title(f'Constant Width')\n",
    "\n",
    "# right: collapsed‐x pyramid heatmap\n",
    "diag_pyr = np.diag(loss_pyr)\n",
    "diag_mat = diag_pyr[:, np.newaxis]\n",
    "\n",
    "im1 = ax1.imshow(diag_mat, aspect='auto', cmap=cmap, norm=norm)\n",
    "ax1.set_xticks([0])\n",
    "ax1.set_xticklabels(['Geometric\\npyramid'])\n",
    "ax1.set_yticks(np.arange(Wp))\n",
    "ax1.set_yticklabels(np.arange(1, Wp+1))\n",
    "ax1.set_ylabel('Hidden Depth')\n",
    "ax1.set_title('Pyramid Scheme')\n",
    "\n",
    "# shared colorbar\n",
    "cbar = fig.colorbar(im0, ax=[ax0,ax1], shrink=0.9, pad=0.02)\n",
    "cbar.set_label('Average Validation Loss (MSE)')\n",
    "\n",
    "plt.savefig('figs/width_depth_amadl.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre‐compute global vmin/vmax across all runs so color‐scale is consistent\n",
    "all_vals = []\n",
    "for run in range(n_runs):\n",
    "    # constant‐width for this run\n",
    "    tmp = []\n",
    "    for d in const_depths:\n",
    "        for w in const_widths:\n",
    "            m = load_model(w, d, run, lambda_l1, lambda_l2, drop, learning_rate)\n",
    "            with torch.no_grad():\n",
    "                batch = [criterion(m(x.to(device)), y.to(device)).item()\n",
    "                         for x,y in val_loader]\n",
    "            tmp.append(np.mean(batch))\n",
    "    all_vals.extend(tmp)\n",
    "\n",
    "    # pyramid diag for this run\n",
    "    diag = []\n",
    "    for seq in pyr_seqs:\n",
    "        depth = len(seq)\n",
    "        m = load_model(seq, depth, run, lambda_l1, lambda_l2, drop, learning_rate)\n",
    "        with torch.no_grad():\n",
    "            batch = [criterion(m(x.to(device)), y.to(device)).item()\n",
    "                     for x,y in val_loader]\n",
    "        diag.append(np.mean(batch))\n",
    "    all_vals.extend(diag)\n",
    "\n",
    "runs = list(range(n_runs))\n",
    "# split into pages of 4 runs each\n",
    "pages = [runs[i:i+4] for i in range(0, len(runs), 4)]\n",
    "\n",
    "for p, page_runs in enumerate(pages, start=1):\n",
    "    nrows = len(page_runs)\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows, 2,\n",
    "        figsize=(10, 3*nrows),\n",
    "        gridspec_kw={'width_ratios': [4, 1]},\n",
    "        constrained_layout=False  # turn off for manual colorbar placement\n",
    "    )\n",
    "\n",
    "    # if only one row, wrap axes\n",
    "    if nrows == 1:\n",
    "        axes = np.expand_dims(axes, 0)\n",
    "\n",
    "    for i, run in enumerate(page_runs):\n",
    "        ax0, ax1 = axes[i]\n",
    "\n",
    "        # ---- left: constant-width heatmap for this run ----\n",
    "        loss_const = np.zeros((Dc, Wc))\n",
    "        for di, d in enumerate(const_depths):\n",
    "            for wj, w in enumerate(const_widths):\n",
    "                m = load_model(w, d, run, lambda_l1, lambda_l2, drop, learning_rate)\n",
    "                with torch.no_grad():\n",
    "                    batch = [criterion(m(x.to(device)), y.to(device)).item()\n",
    "                             for x,y in val_loader]\n",
    "                loss_const[di, wj] = np.mean(batch)\n",
    "\n",
    "        im0 = ax0.imshow(loss_const, aspect='auto', cmap=cmap, norm=norm)\n",
    "        if i == 0:\n",
    "            ax0.set_title('Constant Width')\n",
    "        ax0.set_ylabel(f'Run {run+1}\\nHidden Depth')\n",
    "        ax0.set_xticks(range(Wc))\n",
    "        ax0.set_xticklabels(const_widths)\n",
    "        ax0.set_yticks(range(Dc))\n",
    "        ax0.set_yticklabels(const_depths)\n",
    "        if i == nrows - 1:\n",
    "            ax0.set_xlabel('Hidden Width')\n",
    "        else:\n",
    "            ax0.set_xlabel('')\n",
    "            ax0.tick_params(labelbottom=False)\n",
    "\n",
    "        # collapsed pyramid for this run\n",
    "        diag = []\n",
    "        for seq in pyr_seqs:\n",
    "            depth = len(seq)\n",
    "            m = load_model(seq, depth, run, lambda_l1, lambda_l2, drop, learning_rate)\n",
    "            with torch.no_grad():\n",
    "                batch = [criterion(m(x.to(device)), y.to(device)).item()\n",
    "                         for x,y in val_loader]\n",
    "            diag.append(np.mean(batch))\n",
    "        diag_mat = np.array(diag)[:, None]\n",
    "\n",
    "        im1 = ax1.imshow(diag_mat, aspect='auto', cmap=cmap, norm=norm)\n",
    "        if i == 0:\n",
    "            ax1.set_title('Pyramid Scheme')\n",
    "        ax1.set_xticks([0])\n",
    "        # only label the bottom subplot\n",
    "        if i == nrows - 1:\n",
    "            ax1.set_xticklabels(['Geometric\\npyramid'])\n",
    "        else:\n",
    "            ax1.set_xticklabels([])\n",
    "        ax1.set_yticks(range(Wp))\n",
    "        ax1.set_yticklabels(range(1, Wp+1))\n",
    "        ax1.set_ylabel('Hidden Depth')\n",
    "\n",
    "    # shared colorbar on the right of this page\n",
    "    cax = fig.add_axes([0.92,  # 92% from left\n",
    "                        0.11,  # 10% from bottom\n",
    "                        0.02,  # 2% figure‐width\n",
    "                        0.77   # 80% figure‐height\n",
    "                       ])\n",
    "    cbar = fig.colorbar(im0, cax=cax)\n",
    "    cbar.set_label('Validation Loss (MSE)')\n",
    "\n",
    "    fig.savefig(f'figs/width_depth_page{p}_amadl.png', dpi=300, bbox_inches='tight')\n",
    "    # plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_2_2_'></a>[Model convergence](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSVs\n",
    "df_size = pd.read_csv('models/amadl/hyperparam_test/history/history_size_critAMADL.csv')\n",
    "df_pyr  = pd.read_csv('models/amadl/hyperparam_test/history/history_pyramid_critAMADL.csv')\n",
    "\n",
    "# extract name\n",
    "df_size['widths'] = df_size['widths'].apply(ast.literal_eval)\n",
    "df_pyr ['widths'] = df_pyr ['widths'].apply(ast.literal_eval)\n",
    "\n",
    "df_size['width'] = df_size['widths'].apply(lambda w: w[0])\n",
    "df_pyr ['width'] = df_pyr ['widths'].apply(lambda w: w[0])\n",
    "\n",
    "df_size['name'] = df_size['width'].apply(\n",
    "    lambda w: f\"Constant width = {w}\"\n",
    ")\n",
    "\n",
    "width_to_str = (\n",
    "    df_pyr\n",
    "    .groupby('width')['widths_str']\n",
    "    .agg(lambda ss: max(ss, key=len))\n",
    "    .to_dict()\n",
    ")\n",
    "df_pyr['name'] = df_pyr['width'].map(\n",
    "    lambda w: f\"Pyramid width = {width_to_str[w]}\"\n",
    ")\n",
    "\n",
    "# combine\n",
    "df = pd.concat([df_size, df_pyr], ignore_index=True)\n",
    "\n",
    "# sort by width\n",
    "panel_df = (\n",
    "    df[['name','width']]            # pick only the two columns we care about\n",
    "      .drop_duplicates()            # one row per unique panel\n",
    "      .assign(is_pyr=lambda x: x['name'].str.startswith('Pyramid'))\n",
    "      .sort_values(['is_pyr','width'])  # constants (is_pyr=False) first, then pyramids; each by ascending width\n",
    ")\n",
    "\n",
    "plot_names = panel_df['name'].tolist()\n",
    "n = len(plot_names)\n",
    "n_cols = min(3, n)\n",
    "n_rows = math.ceil(n / n_cols)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols,\n",
    "                         figsize=(4*n_cols, 4*n_rows),\n",
    "                         sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "# build a unified color map over depths\n",
    "depths = sorted(df['depth'].unique())\n",
    "cmap   = plt.get_cmap('viridis')\n",
    "colors = {d: cmap(i/(len(depths)-1)) for i, d in enumerate(depths)}\n",
    "\n",
    "for idx, name in enumerate(plot_names):\n",
    "    ax = axes[idx]\n",
    "    sub_df = df[df['name'] == name]\n",
    "    for d in depths:\n",
    "        sd = sub_df[sub_df['depth'] == d]\n",
    "        for run in sd['run'].unique():\n",
    "            run_df = sd[sd['run'] == run].sort_values('epoch')\n",
    "            ax.plot(\n",
    "                run_df['epoch'],\n",
    "                run_df['val_loss'],\n",
    "                color=colors[d],\n",
    "                alpha=0.8,\n",
    "                linewidth=2,\n",
    "                label=(f\"d={d}\" if run == sd['run'].min() else None)\n",
    "            )\n",
    "\n",
    "    ax.set_title(name)\n",
    "    # ax.set_ylim(1.75e-2, 3e-2)\n",
    "    ax.set_ylim(0.5, 0.6)\n",
    "    ax.set_xlim(0, 250)\n",
    "    ax.set_xticks(np.linspace(0, 250, 6, dtype=int))\n",
    "\n",
    "    # only bottom row: x-label\n",
    "    if idx // n_cols == n_rows - 1:\n",
    "        ax.set_xlabel(\"Epoch\", fontsize=12)\n",
    "    else:\n",
    "        ax.set_xticklabels([])\n",
    "\n",
    "    # only leftmost column: y-label\n",
    "    if idx % n_cols == 0:\n",
    "        ax.set_ylabel(\"Validation Loss\", fontsize=12)\n",
    "\n",
    "# remove any unused axes\n",
    "for j in range(len(plot_names), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# global legend on right\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels,\n",
    "           title=\"Hidden Depth\",\n",
    "           loc='center left',\n",
    "           bbox_to_anchor=(1, 0.5),\n",
    "           fontsize=12)\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.savefig('figs/val_loss_history_amadl.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_3_'></a>[Regularization strength](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model with most data on multiple parameters\n",
    "l1_space = [0.0, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "l2_space = [0.0, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "dropout_space = [0.0, 0.05, 0.1, 0.2]\n",
    "learning_rate_space = [learning_rate] \n",
    "depth_space = [4] \n",
    "width_space = [64]\n",
    "best_models_reg = {}\n",
    "history_reg = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for lambda_l1 in l1_space:\n",
    "#     for lambda_l2 in l2_space:\n",
    "#         for dropout in dropout_space:\n",
    "#             for learning_rate in learning_rate_space:\n",
    "#                 for hidden_depth in depth_space:\n",
    "#                     for hidden_width in width_space:\n",
    "#                         print(f\"\"\"Training model for year '{year}...: \n",
    "#                                 lambda_l1       ={lambda_l1:.0e}\n",
    "#                                 lambda_l2       ={lambda_l2:.0e}\n",
    "#                                 dropout         ={dropout:.0e}\n",
    "#                                 learning_rate   ={learning_rate:.0e}\n",
    "#                                 hidden_depth    ={hidden_depth}\n",
    "#                                 hidden_width    ={hidden_width}\"\"\")\n",
    "#                         for run in range(n_runs):\n",
    "#                             print(f\"Run {run+1} of {n_runs}\")\n",
    "#                             seed = 42+run\n",
    "#                             np.random.seed(seed)\n",
    "#                             torch.manual_seed(seed)\n",
    "#                             # Initialize the model\n",
    "#                             input_dim = X_train[year].shape[1]\n",
    "#                             name = f'l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}'\n",
    "#                             models_21[name] = MLPModel(input_dim, depth=hidden_depth, width=hidden_width, dropout=dropout, activation=activation_fun).to(device)\n",
    "#                             optimizer = torch.optim.Adam(models_21[name].parameters(), lr=learning_rate)\n",
    "#                             train = MLPdataset(X_train[year], y_train[year])\n",
    "#                             val = MLPdataset(X_val[year], y_val[year])\n",
    "#                             best_models_reg[name], history_reg[name] = train_mlp(train,          \n",
    "#                                                             val,\n",
    "#                                                             models_21[name],\n",
    "#                                                             criterion,\n",
    "#                                                             epochs,\n",
    "#                                                             patience,\n",
    "#                                                             print_freq,\n",
    "#                                                             device,\n",
    "#                                                             optimizer,\n",
    "#                                                             lambda_l1=lambda_l1,\n",
    "#                                                             lambda_l2=lambda_l2,\n",
    "#                                                             batch_size=batch_size,\n",
    "#                                                             shuffle_train=True,\n",
    "#                                                             shuffle_val=False,\n",
    "#                                                             save_path=f'models/amadl/hyperparam_test/mlp_y{year}_l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}.pth'\n",
    "#                                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten_history(history_reg, \n",
    "#                 save_csv='models/amadl/hyperparam_test/history/history_reg_critAMADL.csv')\n",
    "# gc.collect()\n",
    "# torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model with most data on multiple parameters\n",
    "l1_space = [0.0, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "l2_space = [0.0, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "dropout_space = [0.0, 0.05 , 0.1, 0.2] \n",
    "learning_rate_space = [learning_rate] \n",
    "depth_space = [4] \n",
    "width_space = [64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "val_ds     = MLPdataset(X_val[year], y_val[year])\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# build (n_drop, n_l1, n_l2) mean-loss array\n",
    "n_l1   = len(l1_space)\n",
    "n_l2   = len(l2_space)\n",
    "n_drop = len(dropout_space)\n",
    "\n",
    "losses = np.zeros((n_drop, n_l1, n_l2), dtype=float)\n",
    "\n",
    "for di, drop in enumerate(dropout_space):\n",
    "    for i, l1 in enumerate(l1_space):\n",
    "        for j, l2 in enumerate(l2_space):\n",
    "            run_losses = []\n",
    "            # for each seed, load & eval\n",
    "            for run in range(n_runs):\n",
    "                # since depth_space & width_space each have one entry, \n",
    "                # we can just index 0 here — but this will generalize\n",
    "                tmp_losses = []\n",
    "                for d in depth_space:\n",
    "                    for w in width_space:\n",
    "                        m = load_model(w, d, run, l1, l2, drop, learning_rate, )\n",
    "                        with torch.no_grad():\n",
    "                            batch = [criterion(m(x.to(device)), y.to(device)).item()\n",
    "                                     for x,y in val_loader]\n",
    "                        tmp_losses.append(np.mean(batch))\n",
    "                run_losses.append(np.mean(tmp_losses))\n",
    "            # now average over runs\n",
    "            losses[di, i, j] = np.mean(run_losses)\n",
    "\n",
    "\n",
    "# global color‐scale\n",
    "vmin    = losses.min()\n",
    "vmax    = losses.max()\n",
    "vcenter = vmin + 0.5*(vmax - vmin)\n",
    "norm    = TwoSlopeNorm(vmin=vmin, vcenter=vcenter, vmax=vmax)\n",
    "cmap    = 'viridis'\n",
    "\n",
    "# layout: up to 2 columns\n",
    "ncols = min(2, n_drop)\n",
    "nrows = math.ceil(n_drop / ncols)\n",
    "\n",
    "fig, axes = plt.subplots(nrows, ncols,\n",
    "                         figsize=(5*ncols, 4*nrows),\n",
    "                         squeeze=False)\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for idx, drop in enumerate(dropout_space):\n",
    "    ax = axes_flat[idx]\n",
    "    im = ax.imshow(\n",
    "        losses[idx],\n",
    "        aspect='auto',\n",
    "        cmap=cmap,\n",
    "        norm=norm,\n",
    "        origin='lower',\n",
    "    )\n",
    "\n",
    "    # y = l1, x = l2\n",
    "    ax.set_yticks(np.arange(n_l1))\n",
    "    ax.set_yticklabels(l1_space)\n",
    "    ax.set_xticks(np.arange(n_l2))\n",
    "    ax.set_xticklabels(l2_space)\n",
    "\n",
    "    # only bottom row shows x‐labels\n",
    "    if idx < (nrows-1)*ncols:\n",
    "        ax.tick_params(labelbottom=False)\n",
    "    else:\n",
    "        ax.set_xlabel(r'$\\ell_2$')\n",
    "\n",
    "    # only first‐column shows y‐labels\n",
    "    if idx % ncols != 0:\n",
    "        ax.tick_params(labelleft=False)\n",
    "    else:\n",
    "        ax.set_ylabel(r'$\\ell_1$')\n",
    "\n",
    "    ax.set_title(f\"dropout = {drop:.2f}\")\n",
    "\n",
    "# clear unused\n",
    "for ax in axes_flat[n_drop:]:\n",
    "    fig.delaxes(ax)\n",
    "\n",
    "# colorbar at the right edge\n",
    "fig.subplots_adjust(right=0.88)\n",
    "cax = fig.add_axes([0.90, 0.15, 0.02, 0.7])  # [left, bottom, width, height]\n",
    "cbar = fig.colorbar(im, cax=cax)\n",
    "cbar.set_label('Average Validation Loss (MSE)')\n",
    "\n",
    "plt.savefig('figs/l1_l2_dropout_loss_amadl.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_4_'></a>[Pyramid](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model with most data on multiple parameters\n",
    "l1_space = [0.0, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "l2_space = [0.0, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "dropout_space = [0.0, 0.05, 0.1, 0.2]\n",
    "learning_rate_space = [learning_rate]\n",
    "depth_space = None\n",
    "width_space = [[32, 16, 8]]\n",
    "\n",
    "best_models_reg_pyramid = {}\n",
    "history_reg_pyramid = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # current runtime: 15h\n",
    "# for lambda_l1 in l1_space:\n",
    "#     for lambda_l2 in l2_space:\n",
    "#         for dropout in dropout_space:\n",
    "#             for learning_rate in learning_rate_space:\n",
    "#                 for hidden_width in width_space:\n",
    "#                     hidden_depth = len(hidden_width)\n",
    "#                     print(f\"\"\"Training model for year '{year}...: \n",
    "#                             lambda_l1       ={lambda_l1:.0e}\n",
    "#                             lambda_l2       ={lambda_l2:.0e}\n",
    "#                             dropout         ={dropout:.0e}\n",
    "#                             learning_rate   ={learning_rate:.0e}\n",
    "#                             hidden_depth    ={hidden_depth}\n",
    "#                             hidden_width    ={hidden_width}\"\"\")\n",
    "#                     for run in range(n_runs):\n",
    "#                         print(f\"Run {run+1} of {n_runs}\")\n",
    "#                         seed = 42+run\n",
    "#                         np.random.seed(seed)\n",
    "#                         torch.manual_seed(seed)\n",
    "#                         # Initialize the model\n",
    "#                         input_dim = X_train[year].shape[1]\n",
    "#                         name = f'l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}'\n",
    "#                         models_21[name] = MLPModel(input_dim, depth=hidden_depth, width=hidden_width, dropout=dropout, activation=activation_fun).to(device)\n",
    "#                         optimizer = torch.optim.Adam(models_21[name].parameters(), lr=learning_rate)\n",
    "#                         train = MLPdataset(X_train[year], y_train[year])\n",
    "#                         val = MLPdataset(X_val[year], y_val[year])\n",
    "#                         best_models_reg_pyramid[name], history_reg_pyramid[name] = train_mlp(train,          \n",
    "#                                                         val,\n",
    "#                                                         models_21[name],\n",
    "#                                                         criterion,\n",
    "#                                                         epochs,\n",
    "#                                                         patience,\n",
    "#                                                         print_freq,\n",
    "#                                                         device,\n",
    "#                                                         optimizer,\n",
    "#                                                         lambda_l1=lambda_l1,\n",
    "#                                                         lambda_l2=lambda_l2,\n",
    "#                                                         batch_size=batch_size,\n",
    "#                                                         shuffle_train=True,\n",
    "#                                                         shuffle_val=False,\n",
    "#                                                         save_path=f'models/amadl/hyperparam_test/mlp_y{year}_l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}.pth'\n",
    "#                                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten_history(history_reg_pyramid, \n",
    "#                 save_csv='models/amadl/hyperparam_test/history/history_reg_pyramid_critAMADL.csv')\n",
    "# gc.collect()\n",
    "# torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model with most data on multiple parameters\n",
    "l1_space = [0.0, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "l2_space = [0.0, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "dropout_space = [0.0, 0.05 , 0.1, 0.2] \n",
    "learning_rate_space = [learning_rate]\n",
    "depth_space = None\n",
    "width_space = [[32, 16, 8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "val_ds     = MLPdataset(X_val[year], y_val[year])\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# build (n_drop, n_l1, n_l2) mean-loss array\n",
    "n_l1   = len(l1_space)\n",
    "n_l2   = len(l2_space)\n",
    "n_drop = len(dropout_space)\n",
    "\n",
    "losses = np.zeros((n_drop, n_l1, n_l2), dtype=float)\n",
    "\n",
    "for di, drop in enumerate(dropout_space):\n",
    "    for i, l1 in enumerate(l1_space):\n",
    "        for j, l2 in enumerate(l2_space):\n",
    "            run_losses = []\n",
    "            # for each seed, load & eval\n",
    "            for run in range(n_runs):\n",
    "                # since depth_space & width_space each have one entry, \n",
    "                # we can just index 0 here — but this will generalize\n",
    "                tmp_losses = []\n",
    "                for w in width_space:\n",
    "                    d = len(w)  # since w is a list, we can use its length\n",
    "                    m = load_model(w, d, run, l1, l2, drop, learning_rate, )\n",
    "                    with torch.no_grad():\n",
    "                        batch = [criterion(m(x.to(device)), y.to(device)).item()\n",
    "                                    for x,y in val_loader]\n",
    "                    tmp_losses.append(np.mean(batch))\n",
    "                run_losses.append(np.mean(tmp_losses))\n",
    "            # now average over runs\n",
    "            losses[di, i, j] = np.mean(run_losses)\n",
    "\n",
    "\n",
    "# global color‐scale\n",
    "vmin    = losses.min()\n",
    "vmax    = losses.max()\n",
    "vcenter = vmin + 0.5*(vmax - vmin)\n",
    "norm    = TwoSlopeNorm(vmin=vmin, vcenter=vcenter, vmax=vmax)\n",
    "cmap    = 'viridis'\n",
    "\n",
    "# layout: up to 2 columns\n",
    "ncols = min(2, n_drop)\n",
    "nrows = math.ceil(n_drop / ncols)\n",
    "\n",
    "fig, axes = plt.subplots(nrows, ncols,\n",
    "                         figsize=(5*ncols, 4*nrows),\n",
    "                         squeeze=False)\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for idx, drop in enumerate(dropout_space):\n",
    "    ax = axes_flat[idx]\n",
    "    im = ax.imshow(\n",
    "        losses[idx],\n",
    "        aspect='auto',\n",
    "        cmap=cmap,\n",
    "        norm=norm,\n",
    "        origin='lower',\n",
    "    )\n",
    "\n",
    "    # y = l1, x = l2\n",
    "    ax.set_yticks(np.arange(n_l1))\n",
    "    ax.set_yticklabels(l1_space)\n",
    "    ax.set_xticks(np.arange(n_l2))\n",
    "    ax.set_xticklabels(l2_space)\n",
    "\n",
    "    # only bottom row shows x‐labels\n",
    "    if idx < (nrows-1)*ncols:\n",
    "        ax.tick_params(labelbottom=False)\n",
    "    else:\n",
    "        ax.set_xlabel(r'$\\ell_2$')\n",
    "\n",
    "    # only first‐column shows y‐labels\n",
    "    if idx % ncols != 0:\n",
    "        ax.tick_params(labelleft=False)\n",
    "    else:\n",
    "        ax.set_ylabel(r'$\\ell_1$')\n",
    "\n",
    "    ax.set_title(f\"dropout = {drop:.2f}\")\n",
    "\n",
    "# clear unused\n",
    "for ax in axes_flat[n_drop:]:\n",
    "    fig.delaxes(ax)\n",
    "\n",
    "# colorbar at the right edge\n",
    "fig.subplots_adjust(right=0.88)\n",
    "cax = fig.add_axes([0.90, 0.15, 0.02, 0.7])  # [left, bottom, width, height]\n",
    "cbar = fig.colorbar(im, cax=cax)\n",
    "cbar.set_label('Average Validation Loss (MSE)')\n",
    "\n",
    "plt.savefig('figs/l1_l2_dropout_loss_pyramid_amadl.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[Test of activation functions](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # general hyperparameters\n",
    "# hidden_depth = 2\n",
    "# hidden_width = 64\n",
    "# learning_rate = learning_rate\n",
    "\n",
    "# # general critereon and regularization parameters\n",
    "# criterion = AMADLTanhLoss(delta=0.5, k=20.0)\n",
    "# lambda_l1 = 1e-4 # baseline l1 regularization\n",
    "# lambda_l2 = 1e-3 # baseline l2 regularization\n",
    "# dropout = 0.0\n",
    "\n",
    "# activation_space = {'ReLU':nn.ReLU, 'LeakyReLU':nn.LeakyReLU, 'Tanh':nn.Tanh, 'Sigmoid':nn.Sigmoid}\n",
    "\n",
    "# best_models_activ = {}\n",
    "# history_activ = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for activ_name, activ_fun in activation_space.items():\n",
    "#     run_time = []\n",
    "#     start_time = time.time()\n",
    "#     print(f\"\"\"Training model for year '{year}...: \n",
    "#             activation      ={activ_name}\n",
    "#             lambda_l1       ={lambda_l1:.0e}\n",
    "#             lambda_l2       ={lambda_l2:.0e}\n",
    "#             dropout         ={dropout:.0e}\n",
    "#             learning_rate   ={learning_rate:.0e}\n",
    "#             hidden_depth    ={hidden_depth}\n",
    "#             hidden_width    ={hidden_width}\"\"\")\n",
    "#     for run in range(n_runs):\n",
    "#         print(f\"Run {run+1} of {n_runs}\")\n",
    "#         seed = 42+run\n",
    "#         np.random.seed(seed)\n",
    "#         torch.manual_seed(seed)\n",
    "#         # Initialize the model\n",
    "#         input_dim = X_train[year].shape[1]\n",
    "#         name = f'l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}_activ{activ_name}'\n",
    "#         models_21[name] = MLPModel(input_dim, depth=hidden_depth, width=hidden_width, dropout=dropout, activation=activ_fun).to(device)\n",
    "#         optimizer = torch.optim.Adam(models_21[name].parameters(), lr=learning_rate)\n",
    "#         train = MLPdataset(X_train[year], y_train[year])\n",
    "#         val = MLPdataset(X_val[year], y_val[year])\n",
    "#         best_models_activ[name], history_activ[name], time_ = train_mlp(train,          \n",
    "#                                         val,\n",
    "#                                         models_21[name],\n",
    "#                                         criterion,\n",
    "#                                         epochs,\n",
    "#                                         patience,\n",
    "#                                         print_freq,\n",
    "#                                         device,\n",
    "#                                         optimizer,\n",
    "#                                         lambda_l1=lambda_l1,\n",
    "#                                         lambda_l2=lambda_l2,\n",
    "#                                         batch_size=batch_size,\n",
    "#                                         shuffle_train=True,\n",
    "#                                         shuffle_val=False,\n",
    "#                                         save_path=f'models/amadl/hyperparam_test/mlp_y{year}_l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}_activ{activ_name}.pth',\n",
    "#                                         timing = True\n",
    "#                                         )\n",
    "#         run_time.append(time_)\n",
    "#     end_time = time.time()\n",
    "#     print(f\"Training with activation nn.{activ_name} took {end_time - start_time:.2f} seconds.\")\n",
    "#     print(f\"Average time per epoch: {np.concatenate(run_time).mean()} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten_history_activ(history_activ, \n",
    "#                 save_csv='models/amadl/hyperparam_test/history/history_activ_critAMADL.csv')\n",
    "# gc.collect()\n",
    "# torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_activ = pd.read_csv('models/amadl/hyperparam_test/history/history_activ_critAMADL.csv')\n",
    "\n",
    "# activations = sorted(df_activ['activ_name'].unique())\n",
    "# n_acts = len(activations)+1\n",
    "# cmap = plt.get_cmap('viridis', n_acts)\n",
    "# color_map = {act: cmap(i) for i, act in enumerate(activations)}\n",
    "\n",
    "# # create 2x2 grid\n",
    "# fig, axes = plt.subplots(2, 2, figsize=(12, 8), sharex=True, sharey=True)\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# # plot\n",
    "# for i, act in enumerate(activations):\n",
    "#     ax = axes[i]\n",
    "#     df_act = df_activ[df_activ['activ_name'] == act]\n",
    "#     # Plot each run for this activation\n",
    "#     for run_id, grp in df_act.groupby('run'):\n",
    "#         ax.plot(\n",
    "#             grp['epoch'],\n",
    "#             grp['val_loss'],\n",
    "#             color=color_map[act],\n",
    "#             alpha=0.5,\n",
    "#             linewidth=2,\n",
    "#         )\n",
    "#     ax.set_title(act)\n",
    "#     # show x-label on bottom row \n",
    "#     if i // 2 == 1:\n",
    "#         ax.set_xlabel('Epoch', fontsize=12)\n",
    "#     else:\n",
    "#         ax.tick_params(labelbottom=False)\n",
    "#     # show y-label on left column\n",
    "#     if i % 2 == 0:\n",
    "#         ax.set_ylabel('Validation Loss', fontsize=12)\n",
    "#     else:\n",
    "#         ax.tick_params(labelleft=False)\n",
    "#     ax.set_ylim(1.1, 1.2)\n",
    "#     ax.set_xlim(0, 250)\n",
    "\n",
    "# # remove any unused subplots\n",
    "# for j in range(len(activations), len(axes)):\n",
    "#     fig.delaxes(axes[j])\n",
    "\n",
    "# fig.tight_layout()\n",
    "# fig.savefig('figs/activation_fun_history_amadl.png', dpi=300, bbox_inches='tight')\n",
    "# # plt.show()\n",
    "# plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the best model for each activation function for each run\n",
    "# preds_activ = {}\n",
    "\n",
    "# for activ_name, activ_fun in activation_space.items():\n",
    "#     print(f\"Loading models for activation {activ_name}...\")\n",
    "#     all_preds = []\n",
    "#     activation_fun = activ_fun\n",
    "#     for run in range(n_runs):\n",
    "#         best_models_activ[f'activ{activ_name}_run{run+1}'] = load_model(\n",
    "#             hidden_width, hidden_depth, run, \n",
    "#             lambda_l1, lambda_l2, dropout, learning_rate, activ=activ_name\n",
    "#         )\n",
    "#         preds = predict_mlp(\n",
    "#             best_models_activ[f'activ{activ_name}_run{run+1}'], \n",
    "#             X_val[year], \n",
    "#             y_val[year], \n",
    "#             y_scalers[year],\n",
    "#             batch_size=batch_size,\n",
    "#             device=device,\n",
    "#         )\n",
    "#         all_preds.append(preds)\n",
    "#     np.stack(all_preds, axis=0)\n",
    "#     preds_activ[activ_name] = np.mean(all_preds, axis=0)\n",
    "\n",
    "# df_preds_activ = pd.DataFrame(preds_activ)\n",
    "# df_preds_activ['y_true'] = y_scalers[year].inverse_transform(y_val[year].reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics_full = {}\n",
    "# results_full = {}\n",
    "# for activ_name in activation_space.keys():\n",
    "#     y_true = df_preds_activ['y_true'].values\n",
    "#     y_pred = df_preds_activ[activ_name].values\n",
    "#     results_full[activ_name] = {\n",
    "#         'RMSE': rmse_fun(y_true, y_pred),\n",
    "#         'MAE': mae_fun(y_true, y_pred),\n",
    "#         'AMADL': amadl_fun(y_true, y_pred),\n",
    "#     }\n",
    "\n",
    "# for metric in ['RMSE', 'MAE', 'AMADL']:\n",
    "#         vals = [\n",
    "#         results_full['ReLU'][metric],\n",
    "#         results_full['LeakyReLU'][metric],\n",
    "#         results_full['Tanh'][metric],\n",
    "#         results_full['Sigmoid'][metric],\n",
    "#         ]\n",
    "#         metrics_full[metric] = vals\n",
    "\n",
    "# tab_activ = latex_table(list(activation_space.keys()),metrics_full)\n",
    "# with open('tabs/activ_fun_perf.tex', 'w') as f:\n",
    "#     f.write(tab_activ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define plot limits\n",
    "# x_min, x_max = -0.5, 0.5\n",
    "# bins = 100\n",
    "\n",
    "# # 2×2 grid with shared axes\n",
    "# fig, axes = plt.subplots(2, 2, figsize=(12, 6),\n",
    "#                          sharex=True, sharey=True)\n",
    "\n",
    "# for ax, act in zip(axes.flat, activation_space):\n",
    "#     res = df_preds_activ['y_true'] - df_preds_activ[act]\n",
    "\n",
    "#     # density=True makes the histogram area = 1\n",
    "#     ax.hist(res, bins=bins, density=True, alpha=0.7)\n",
    "\n",
    "#     x = np.linspace(x_min, x_max, 500)\n",
    "#     mu, sigma = res.mean(), res.std(ddof=0)\n",
    "#     mu_lap    = np.median(res)\n",
    "#     b_lap     = np.mean(np.abs(res - mu_lap))\n",
    "#     nu        = 1\n",
    "\n",
    "#     # Normal pdf\n",
    "#     pdf_gauss = (1/(sigma*np.sqrt(2*np.pi))) * np.exp(-0.5*((x - mu)/sigma)**2)\n",
    "#     ax.plot(x, pdf_gauss, \"C1--\", lw=2,\n",
    "#             label=rf\"Normal($\\mu$={mu:.3f}, $\\sigma$={sigma:.3f})\")\n",
    "\n",
    "#     # Laplace pdf\n",
    "#     pdf_lap = (1/(2*b_lap)) * np.exp(-np.abs(x - mu_lap)/b_lap)\n",
    "#     ax.plot(x, pdf_lap, \"C3:\", lw=2,\n",
    "#             label=rf\"Laplace($\\mu_L$={mu_lap:.3f}, b={b_lap:.3f})\")\n",
    "\n",
    "#     # Student-t pdf\n",
    "#     pdf_t = student_t.pdf(x, df=nu, loc=mu, scale=sigma)\n",
    "#     ax.plot(x, pdf_t, \"C6-.\", lw=2,\n",
    "#             label=rf\"Student-$t_{{{nu}}}$($\\mu$={mu:.3f}, $\\sigma$={sigma:.3f})\")\n",
    "\n",
    "#     ax.set_xlim(x_min, x_max)\n",
    "#     ax.set_title(act)\n",
    "#     ax.legend(fontsize=\"small\")\n",
    "\n",
    "# # Only label the outer axes\n",
    "# axes[0, 0].set_ylabel('Density')\n",
    "# axes[1, 0].set_ylabel('Density')\n",
    "# axes[1, 0].set_xlabel('Residual')\n",
    "# axes[1, 1].set_xlabel('Residual')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('figs/residuals_activations_density_amadl.png', dpi=300, bbox_inches='tight')\n",
    "# # plt.show()\n",
    "# plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

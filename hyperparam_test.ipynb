{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Notebook for hyperparameter tuning of the MLP model](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Notebook for hyperparameter tuning of the MLP model](#toc1_)    \n",
    "- [Import libraries](#toc2_)    \n",
    "- [Import data](#toc3_)    \n",
    "- [Prepare data for training](#toc4_)    \n",
    "- [Test hyperparameters](#toc5_)    \n",
    "  - [Constant scheme](#toc5_1_)    \n",
    "  - [Pyramid scheme](#toc5_2_)    \n",
    "    - [Depth and width comparison](#toc5_2_1_)    \n",
    "    - [Model convergence](#toc5_2_2_)    \n",
    "  - [Regularization strength](#toc5_3_)    \n",
    "  - [Pyramid](#toc5_4_)    \n",
    "- [Test of activation functions](#toc6_)    \n",
    "- [Test of criterion functions](#toc7_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Import libraries](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "import matplotlib.gridspec as gridspec\n",
    "import math\n",
    "import ast\n",
    "import gc\n",
    "import time\n",
    "from scipy.stats import t as student_t\n",
    "\n",
    "from libs.models import *\n",
    "from libs.functions import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "plt.rcParams.update({'font.size': 12, 'figure.figsize': (10, 4), 'figure.dpi': 300})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Import data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df = pd.read_csv('data/data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Prepare data for training](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare expanding window splits\n",
    "periods = {\n",
    "    '21' : '2019-12-31', # 2021 is the test set\n",
    "    # '22' : '2020-12-31', # 2022 is the test set\n",
    "    # '23' : '2021-12-31', # 2023 is the test set\n",
    "    # '24': '2022-12-31' # 2024 is the test set\n",
    "}\n",
    "\n",
    "# identify dummy vs. numeric columns\n",
    "feature_cols = [col for col in df.columns if col not in ['timestamp', 'ticker', 'target']]\n",
    "nace_cols = [c for c in feature_cols if c.startswith('NACE_')]\n",
    "dummy_cols = ['divi','divo'] # sin removed\n",
    "macro_cols = ['discount', 'tms', 'dp', 'ep', 'svar'] # 'bm_macro'\n",
    "\n",
    "# nummeric cols = cols not in cat and macro cols\n",
    "numeric_cols = [c for c in feature_cols if c not in dummy_cols and c not in nace_cols and c not in macro_cols]\n",
    "\n",
    "df_raw = df.copy(deep=True)\n",
    "df_raw['timestamp'] = pd.to_datetime(df_raw['timestamp'])\n",
    "\n",
    "# drop data from 2025\n",
    "df_raw = df_raw[df_raw['timestamp'] < '2025-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create interaction features between numeric and macro features\n",
    "C = df[numeric_cols].values         # shape = (n_rows, P_c)\n",
    "X = df[macro_cols].values           # shape = (n_rows, P_x)\n",
    "\n",
    "# compute all pairwise products with broadcasting:\n",
    "K = C[:,:,None] * X[:,None,:]\n",
    "\n",
    "# reshape to (n_rows, P_c * P_x)\n",
    "Z = K.reshape(len(df), -1)\n",
    "\n",
    "# build the column names in the same order\n",
    "xc_names = [\n",
    "    f\"{c}_x_{m}\"\n",
    "    for c in numeric_cols\n",
    "    for m in macro_cols\n",
    "]\n",
    "\n",
    "# wrap back into a DataFrame\n",
    "df_xc = pd.DataFrame(Z, columns=xc_names, index=df.index)\n",
    "\n",
    "feature_cols = numeric_cols + xc_names + dummy_cols + nace_cols\n",
    "numeric_cols = numeric_cols + xc_names\n",
    "cat_cols = dummy_cols + nace_cols\n",
    "df_z = df_raw.merge(df_xc, left_index=True, right_index=True)\n",
    "# drop macro_cols\n",
    "df_z = df_z.drop(columns=macro_cols)\n",
    "# sort columns by feature_cols\n",
    "df_norm = df_z[['timestamp', 'ticker', 'target'] + feature_cols]\n",
    "\n",
    "y_values = df_norm['target'].values.astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare containers\n",
    "X_train, X_val, X_test = {}, {}, {}\n",
    "y_train, y_val, y_test = {}, {}, {}\n",
    "preprocessors = {}\n",
    "y_scalers = {}\n",
    "\n",
    "for y, period in periods.items():\n",
    "    period = pd.to_datetime(period)\n",
    "\n",
    "    # split masks\n",
    "    tr_mask = df_norm['timestamp'] < period\n",
    "    va_mask = (df_norm['timestamp'] >= period) & \\\n",
    "              (df_norm['timestamp'] - pd.DateOffset(years=1) < period)\n",
    "    te_mask = (df_norm['timestamp'] - pd.DateOffset(years=1) >= period) & \\\n",
    "              (df_norm['timestamp'] - pd.DateOffset(years=2) < period)\n",
    "\n",
    "    # extract raw feature DataFrames\n",
    "    X_tr_df = df_norm.loc[tr_mask, feature_cols].copy()\n",
    "    X_va_df = df_norm.loc[va_mask, feature_cols].copy()\n",
    "    X_te_df = df_norm.loc[te_mask, feature_cols].copy()\n",
    "    y_tr    = y_values[tr_mask]\n",
    "    y_va    = y_values[va_mask]\n",
    "    y_te    = y_values[te_mask]\n",
    "\n",
    "    # compute winsorization bounds on train\n",
    "    lower = X_tr_df[numeric_cols].quantile(0.01)\n",
    "    upper = X_tr_df[numeric_cols].quantile(0.99)\n",
    "\n",
    "    # apply clipping to train, val, test\n",
    "    X_tr_df[numeric_cols] = X_tr_df[numeric_cols].clip(lower=lower, upper=upper, axis=1)\n",
    "    X_va_df[numeric_cols] = X_va_df[numeric_cols].clip(lower=lower, upper=upper, axis=1)\n",
    "    X_te_df[numeric_cols] = X_te_df[numeric_cols].clip(lower=lower, upper=upper, axis=1)\n",
    "\n",
    "    # now fit scaler on numeric only\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', StandardScaler(), numeric_cols),\n",
    "        ('cat', 'passthrough',  cat_cols)\n",
    "    ])\n",
    "    preprocessor.fit(X_tr_df)\n",
    "    preprocessors[y] = preprocessor\n",
    "\n",
    "    # transform all splits\n",
    "    X_train[y] = preprocessor.transform(X_tr_df).astype('float32')\n",
    "    X_val[y]   = preprocessor.transform(X_va_df).astype('float32')\n",
    "    X_test[y]  = preprocessor.transform(X_te_df).astype('float32')\n",
    "\n",
    "    # fit standard scaler on y values\n",
    "    y_scaler = StandardScaler()\n",
    "    y_scaler.fit(y_tr.reshape(-1, 1))\n",
    "    y_scalers[y] = y_scaler\n",
    "    y_tr = y_scaler.transform(y_tr.reshape(-1, 1)).flatten()\n",
    "    y_va = y_scaler.transform(y_va.reshape(-1, 1)).flatten()\n",
    "    y_te = y_scaler.transform(y_te.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "    # store targets as before\n",
    "    y_train[y] = y_tr.reshape(-1, 1)\n",
    "    y_val[y]   = y_va.reshape(-1, 1)\n",
    "    y_test[y]  = y_te.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# moving to metal or CUDA GPU if available\n",
    "device = torch.device((\"cuda\" if torch.cuda.is_available() \n",
    "                       else \"mps\" if torch.backends.mps.is_available() \n",
    "                       else \"cpu\"))\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# general hyperparameters\n",
    "hidden_depth = None \n",
    "hidden_width = None \n",
    "learning_rate = 5e-5 # 1e-4 # 1e-5 \n",
    "activation_fun = nn.ReLU # nn.ReLU, nn.Tanh, nn.Sigmoid, nn.LeakyReLU\n",
    "\n",
    "# general critereon and regularization parameters\n",
    "criterion = nn.MSELoss()\n",
    "lambda_l1 = 1e-4 # baseline l1 regularization # 1e-5\n",
    "lambda_l2 = 1e-4 # baseline l2 regularization\n",
    "drop = 0.0\n",
    "\n",
    "# general parmeters\n",
    "patience = 25\n",
    "print_freq = 100\n",
    "epochs = 250\n",
    "batch_size = 4096 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = '21'\n",
    "models_21 = {}\n",
    "n_runs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model loader fun\n",
    "def load_model(w, d, run, lambda_l1, lambda_l2, drop, lr, activ = None, crit = None):\n",
    "    m = MLPModel(\n",
    "        input_dim  = X_train[year].shape[1],\n",
    "        depth      = d,\n",
    "        width      = w,\n",
    "        dropout    = drop,\n",
    "        activation = activation_fun,\n",
    "    ).to(device)\n",
    "    path = (\n",
    "        f\"models/hyperparam_test/mlp_y{year}\"\n",
    "        f\"_l1{lambda_l1}_l2{lambda_l2}\"\n",
    "        f\"_drop{drop}_lr{lr}\"\n",
    "        f\"_w{w}_d{d}_run{run+1}.pth\"\n",
    "    )\n",
    "    if activ is not None:\n",
    "        path = path.replace('.pth', f'_activ{activ}.pth')\n",
    "    if crit is not None:\n",
    "        path = path.replace('.pth', f'_crit{crit}.pth')\n",
    "    m.load_state_dict(torch.load(path, map_location=device))\n",
    "    m.eval()\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Test hyperparameters](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_1_'></a>[Constant scheme](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training model with most data on multiple parameters\n",
    "# l1_space = [lambda_l1]\n",
    "# l2_space = [lambda_l2]\n",
    "# dropout_space = [drop]\n",
    "# learning_rate_space = [learning_rate]\n",
    "# depth_space = [1, 2, 3, 4, 5, 6, 7]\n",
    "# width_space = [8, 16, 32, 64, 128]\n",
    "# best_models_size = {}\n",
    "# history_size = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # current runtime: 2h 25m at 5 runs per model\n",
    "# for lambda_l1 in l1_space:\n",
    "#     for lambda_l2 in l2_space:\n",
    "#         for dropout in dropout_space:\n",
    "#             for learning_rate in learning_rate_space:\n",
    "#                 for hidden_depth in depth_space:\n",
    "#                     for hidden_width in width_space:\n",
    "#                         print(f\"\"\"Training model for year '{year}...: \n",
    "#                                 lambda_l1       ={lambda_l1:.0e}\n",
    "#                                 lambda_l2       ={lambda_l2:.0e}\n",
    "#                                 dropout         ={dropout:.0e}\n",
    "#                                 learning_rate   ={learning_rate:.0e}\n",
    "#                                 hidden_depth    ={hidden_depth}\n",
    "#                                 hidden_width    ={hidden_width}\"\"\")\n",
    "#                         for run in range(n_runs):\n",
    "#                             print(f\"Run {run+1} of {n_runs}\")\n",
    "#                             seed = 42+run   \n",
    "#                             np.random.seed(seed)\n",
    "#                             torch.manual_seed(seed)\n",
    "#                             # Initialize the model\n",
    "#                             input_dim = X_train[year].shape[1]\n",
    "#                             name = f'l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}'\n",
    "#                             models_21[name] = MLPModel(input_dim, depth=hidden_depth, width=hidden_width, dropout=dropout, activation=activation_fun).to(device)\n",
    "#                             optimizer = torch.optim.Adam(models_21[name].parameters(), lr=learning_rate)\n",
    "#                             train = MLPdataset(X_train[year], y_train[year])\n",
    "#                             val = MLPdataset(X_val[year], y_val[year])\n",
    "#                             best_models_size[name], history_size[name] = train_mlp(train,          \n",
    "#                                                             val,\n",
    "#                                                             models_21[name],\n",
    "#                                                             criterion,\n",
    "#                                                             epochs,\n",
    "#                                                             patience,\n",
    "#                                                             print_freq,\n",
    "#                                                             device,\n",
    "#                                                             optimizer,\n",
    "#                                                             lambda_l1=lambda_l1,\n",
    "#                                                             lambda_l2=lambda_l2,\n",
    "#                                                             batch_size=batch_size,\n",
    "#                                                             shuffle_train=True,\n",
    "#                                                             shuffle_val=False,\n",
    "#                                                             save_path=f'models/hyperparam_test/mlp_y{year}_l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}.pth'\n",
    "#                                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten_history(history_size, \n",
    "#                 save_csv='models/hyperparam_test/history/history_size.csv')\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_2_'></a>[Pyramid scheme](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training model with most data on multiple parameters\n",
    "# l1_space = [lambda_l1]\n",
    "# l2_space = [lambda_l2]\n",
    "# dropout_space = [drop]\n",
    "# learning_rate_space = [learning_rate]\n",
    "# depth_space = None\n",
    "# width_space = [[32], \n",
    "#                [32, 16], \n",
    "#                [32, 16, 8], \n",
    "#                [32, 16, 8, 4], \n",
    "#                [32, 16, 8, 4, 2]]\n",
    "# best_models_pyramid = {}\n",
    "# history_pyramid = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # current runtime: 30m\n",
    "# for lambda_l1 in l1_space:\n",
    "#     for lambda_l2 in l2_space:\n",
    "#         for dropout in dropout_space:\n",
    "#             for learning_rate in learning_rate_space:\n",
    "#                 for hidden_width in width_space:\n",
    "#                     hidden_depth = len(hidden_width)\n",
    "#                     print(f\"\"\"Training model for year '{year}...: \n",
    "#                             lambda_l1       ={lambda_l1:.0e}\n",
    "#                             lambda_l2       ={lambda_l2:.0e}\n",
    "#                             dropout         ={dropout:.0e}\n",
    "#                             learning_rate   ={learning_rate:.0e}\n",
    "#                             hidden_depth    ={hidden_depth}\n",
    "#                             hidden_width    ={hidden_width}\"\"\")\n",
    "#                     for run in range(n_runs):\n",
    "#                         print(f\"Run {run+1} of {n_runs}\")\n",
    "#                         seed = 42+run\n",
    "#                         np.random.seed(seed)\n",
    "#                         torch.manual_seed(seed)\n",
    "#                         # Initialize the model\n",
    "#                         input_dim = X_train[year].shape[1]\n",
    "#                         name = f'l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}'\n",
    "#                         models_21[name] = MLPModel(input_dim, depth=hidden_depth, width=hidden_width, dropout=dropout, activation=activation_fun).to(device)\n",
    "#                         optimizer = torch.optim.Adam(models_21[name].parameters(), lr=learning_rate)\n",
    "#                         train = MLPdataset(X_train[year], y_train[year])\n",
    "#                         val = MLPdataset(X_val[year], y_val[year])\n",
    "#                         best_models_pyramid[name], history_pyramid[name] = train_mlp(train,          \n",
    "#                                                         val,\n",
    "#                                                         models_21[name],\n",
    "#                                                         criterion,\n",
    "#                                                         epochs,\n",
    "#                                                         patience,\n",
    "#                                                         print_freq,\n",
    "#                                                         device,\n",
    "#                                                         optimizer,\n",
    "#                                                         lambda_l1=lambda_l1,\n",
    "#                                                         lambda_l2=lambda_l2,\n",
    "#                                                         batch_size=batch_size,\n",
    "#                                                         shuffle_train=True,\n",
    "#                                                         shuffle_val=False,\n",
    "#                                                         save_path=f'models/hyperparam_test/mlp_y{year}_l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}.pth'\n",
    "#                                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten_history(history_pyramid, \n",
    "#                 save_csv='models/hyperparam_test/history/history_pyramid.csv')\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_2_1_'></a>[Depth and width comparison](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant-width grid\n",
    "const_widths = [8, 16, 32, 64, 128]\n",
    "const_depths = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "# pyramid sequences (each column of the right heatmap)\n",
    "pyr_seqs = [\n",
    "    [32],\n",
    "    [32,16],\n",
    "    [32,16,8],\n",
    "    [32,16,8,4],\n",
    "    [32,16,8,4,2],\n",
    "]\n",
    "Wc = len(const_widths)\n",
    "Dc = len(const_depths)\n",
    "Wp = len(pyr_seqs)\n",
    "\n",
    "# DataLoader\n",
    "val_ds     = MLPdataset(X_val[year], y_val[year])\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# constant-width loss matrix\n",
    "loss_const = np.zeros((Dc, Wc))\n",
    "for di, d in enumerate(const_depths):\n",
    "    for wj, w in enumerate(const_widths):\n",
    "        run_losses = []\n",
    "        for run in range(n_runs):\n",
    "            m = load_model(w, d, run, lambda_l1, lambda_l2, drop, learning_rate)\n",
    "            with torch.no_grad():\n",
    "                batch = [criterion(m(x.to(device)), y.to(device)).item()\n",
    "                         for x,y in val_loader]\n",
    "            run_losses.append(np.mean(batch))\n",
    "        loss_const[di, wj] = np.mean(run_losses)\n",
    "\n",
    "# pyramid loss matrix\n",
    "loss_pyr = np.full((Wp, Wp), np.nan)\n",
    "for sj, seq in enumerate(pyr_seqs):\n",
    "    depth = len(seq)\n",
    "    run_losses = []\n",
    "    for run in range(n_runs):\n",
    "        # we “flatten” seq into width param for loader\n",
    "        m = load_model(seq, depth, run, lambda_l1, lambda_l2, drop, learning_rate)\n",
    "        with torch.no_grad():\n",
    "            batch = [criterion(m(x.to(device)), y.to(device)).item()\n",
    "                     for x,y in val_loader]\n",
    "        run_losses.append(np.mean(batch))\n",
    "    loss_pyr[depth-1, sj] = np.mean(run_losses)\n",
    "\n",
    "# plot\n",
    "fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(12,5),\n",
    "                               gridspec_kw={'width_ratios':[4,1]})\n",
    "\n",
    "# color‐scale\n",
    "all_vals = loss_const.ravel()\n",
    "vmin    = all_vals.min()\n",
    "vmax    = all_vals.max()+0.005\n",
    "vcenter = vmin + 0.5*(vmax - vmin)\n",
    "norm    = TwoSlopeNorm(vmin=vmin, vcenter=vcenter, vmax=vmax)\n",
    "cmap    = 'viridis'\n",
    "\n",
    "# left: constant‐width heatmap\n",
    "im0 = ax0.imshow(loss_const, aspect='auto', cmap=cmap, norm=norm)\n",
    "im0 = ax0.imshow(loss_const, aspect='auto', cmap=cmap, norm=norm)\n",
    "ax0.set_xticks(range(Wc))\n",
    "ax0.set_xticklabels(const_widths)\n",
    "ax0.set_yticks(range(Dc))\n",
    "ax0.set_yticklabels(const_depths)\n",
    "ax0.set_xlabel('Hidden Width')\n",
    "ax0.set_ylabel('Hidden Depth')\n",
    "ax0.set_title(f'Constant Width')\n",
    "\n",
    "# right: collapsed‐x pyramid heatmap\n",
    "diag_pyr = np.diag(loss_pyr)\n",
    "diag_mat = diag_pyr[:, np.newaxis]\n",
    "\n",
    "im1 = ax1.imshow(diag_mat, aspect='auto', cmap=cmap, norm=norm)\n",
    "ax1.set_xticks([0])\n",
    "ax1.set_xticklabels(['Geometric\\npyramid'])\n",
    "ax1.set_yticks(np.arange(Wp))\n",
    "ax1.set_yticklabels(np.arange(1, Wp+1))\n",
    "ax1.set_ylabel('Hidden Depth')\n",
    "ax1.set_title('Pyramid Scheme')\n",
    "\n",
    "# shared colorbar\n",
    "cbar = fig.colorbar(im0, ax=[ax0,ax1], shrink=0.9, pad=0.02)\n",
    "cbar.set_label('Average Validation Loss (MSE)')\n",
    "\n",
    "plt.savefig('figs/width_depth.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre‐compute global vmin/vmax across all runs so color‐scale is consistent\n",
    "all_vals = []\n",
    "for run in range(n_runs):\n",
    "    # constant‐width for this run\n",
    "    tmp = []\n",
    "    for d in const_depths:\n",
    "        for w in const_widths:\n",
    "            m = load_model(w, d, run, lambda_l1, lambda_l2, drop, learning_rate)\n",
    "            with torch.no_grad():\n",
    "                batch = [criterion(m(x.to(device)), y.to(device)).item()\n",
    "                         for x,y in val_loader]\n",
    "            tmp.append(np.mean(batch))\n",
    "    all_vals.extend(tmp)\n",
    "\n",
    "    # pyramid diag for this run\n",
    "    diag = []\n",
    "    for seq in pyr_seqs:\n",
    "        depth = len(seq)\n",
    "        m = load_model(seq, depth, run, lambda_l1, lambda_l2, drop, learning_rate)\n",
    "        with torch.no_grad():\n",
    "            batch = [criterion(m(x.to(device)), y.to(device)).item()\n",
    "                     for x,y in val_loader]\n",
    "        diag.append(np.mean(batch))\n",
    "    all_vals.extend(diag)\n",
    "\n",
    "runs = list(range(n_runs))\n",
    "# split into pages of 4 runs each\n",
    "pages = [runs[i:i+4] for i in range(0, len(runs), 4)]\n",
    "\n",
    "for p, page_runs in enumerate(pages, start=1):\n",
    "    nrows = len(page_runs)\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows, 2,\n",
    "        figsize=(10, 3*nrows),\n",
    "        gridspec_kw={'width_ratios': [4, 1]},\n",
    "        constrained_layout=False  # turn off for manual colorbar placement\n",
    "    )\n",
    "\n",
    "    # if only one row, wrap axes\n",
    "    if nrows == 1:\n",
    "        axes = np.expand_dims(axes, 0)\n",
    "\n",
    "    for i, run in enumerate(page_runs):\n",
    "        ax0, ax1 = axes[i]\n",
    "\n",
    "        # ---- left: constant-width heatmap for this run ----\n",
    "        loss_const = np.zeros((Dc, Wc))\n",
    "        for di, d in enumerate(const_depths):\n",
    "            for wj, w in enumerate(const_widths):\n",
    "                m = load_model(w, d, run, lambda_l1, lambda_l2, drop, learning_rate)\n",
    "                with torch.no_grad():\n",
    "                    batch = [criterion(m(x.to(device)), y.to(device)).item()\n",
    "                             for x,y in val_loader]\n",
    "                loss_const[di, wj] = np.mean(batch)\n",
    "\n",
    "        im0 = ax0.imshow(loss_const, aspect='auto', cmap=cmap, norm=norm)\n",
    "        if i == 0:\n",
    "            ax0.set_title('Constant Width')\n",
    "        ax0.set_ylabel(f'Run {run+1}\\nHidden Depth')\n",
    "        ax0.set_xticks(range(Wc))\n",
    "        ax0.set_xticklabels(const_widths)\n",
    "        ax0.set_yticks(range(Dc))\n",
    "        ax0.set_yticklabels(const_depths)\n",
    "        if i == nrows - 1:\n",
    "            ax0.set_xlabel('Hidden Width')\n",
    "        else:\n",
    "            ax0.set_xlabel('')\n",
    "            ax0.tick_params(labelbottom=False)\n",
    "\n",
    "        # collapsed pyramid for this run\n",
    "        diag = []\n",
    "        for seq in pyr_seqs:\n",
    "            depth = len(seq)\n",
    "            m = load_model(seq, depth, run, lambda_l1, lambda_l2, drop, learning_rate)\n",
    "            with torch.no_grad():\n",
    "                batch = [criterion(m(x.to(device)), y.to(device)).item()\n",
    "                         for x,y in val_loader]\n",
    "            diag.append(np.mean(batch))\n",
    "        diag_mat = np.array(diag)[:, None]\n",
    "\n",
    "        im1 = ax1.imshow(diag_mat, aspect='auto', cmap=cmap, norm=norm)\n",
    "        if i == 0:\n",
    "            ax1.set_title('Pyramid Scheme')\n",
    "        ax1.set_xticks([0])\n",
    "        # only label the bottom subplot\n",
    "        if i == nrows - 1:\n",
    "            ax1.set_xticklabels(['Geometric\\npyramid'])\n",
    "        else:\n",
    "            ax1.set_xticklabels([])\n",
    "        ax1.set_yticks(range(Wp))\n",
    "        ax1.set_yticklabels(range(1, Wp+1))\n",
    "        ax1.set_ylabel('Hidden Depth')\n",
    "\n",
    "    # shared colorbar on the right of this page\n",
    "    cax = fig.add_axes([0.92,  # 92% from left\n",
    "                        0.11,  # 10% from bottom\n",
    "                        0.02,  # 2% figure‐width\n",
    "                        0.77   # 80% figure‐height\n",
    "                       ])\n",
    "    cbar = fig.colorbar(im0, cax=cax)\n",
    "    cbar.set_label('Validation Loss (MSE)')\n",
    "\n",
    "    fig.savefig(f'figs/width_depth_page{p}.png', dpi=300, bbox_inches='tight')\n",
    "    # plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_2_2_'></a>[Model convergence](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSVs\n",
    "df_size = pd.read_csv('models/hyperparam_test/history/history_size.csv')\n",
    "df_pyr  = pd.read_csv('models/hyperparam_test/history/history_pyramid.csv')\n",
    "\n",
    "# extract name\n",
    "df_size['widths'] = df_size['widths'].apply(ast.literal_eval)\n",
    "df_pyr ['widths'] = df_pyr ['widths'].apply(ast.literal_eval)\n",
    "\n",
    "df_size['width'] = df_size['widths'].apply(lambda w: w[0])\n",
    "df_pyr ['width'] = df_pyr ['widths'].apply(lambda w: w[0])\n",
    "\n",
    "df_size['name'] = df_size['width'].apply(\n",
    "    lambda w: f\"Constant width = {w}\"\n",
    ")\n",
    "\n",
    "width_to_str = (\n",
    "    df_pyr\n",
    "    .groupby('width')['widths_str']\n",
    "    .agg(lambda ss: max(ss, key=len))\n",
    "    .to_dict()\n",
    ")\n",
    "df_pyr['name'] = df_pyr['width'].map(\n",
    "    lambda w: f\"Pyramid width = {width_to_str[w]}\"\n",
    ")\n",
    "\n",
    "# combine\n",
    "df = pd.concat([df_size, df_pyr], ignore_index=True)\n",
    "\n",
    "# sort by width\n",
    "panel_df = (\n",
    "    df[['name','width']]            # pick only the two columns we care about\n",
    "      .drop_duplicates()            # one row per unique panel\n",
    "      .assign(is_pyr=lambda x: x['name'].str.startswith('Pyramid'))\n",
    "      .sort_values(['is_pyr','width'])  # constants (is_pyr=False) first, then pyramids; each by ascending width\n",
    ")\n",
    "\n",
    "plot_names = panel_df['name'].tolist()\n",
    "n = len(plot_names)\n",
    "n_cols = min(3, n)\n",
    "n_rows = math.ceil(n / n_cols)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols,\n",
    "                         figsize=(4*n_cols, 4*n_rows),\n",
    "                         sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "# build a unified color map over depths\n",
    "depths = sorted(df['depth'].unique())\n",
    "cmap   = plt.get_cmap('viridis')\n",
    "colors = {d: cmap(i/(len(depths)-1)) for i, d in enumerate(depths)}\n",
    "\n",
    "for idx, name in enumerate(plot_names):\n",
    "    ax = axes[idx]\n",
    "    sub_df = df[df['name'] == name]\n",
    "    for d in depths:\n",
    "        sd = sub_df[sub_df['depth'] == d]\n",
    "        for run in sd['run'].unique():\n",
    "            run_df = sd[sd['run'] == run].sort_values('epoch')\n",
    "            ax.plot(\n",
    "                run_df['epoch'],\n",
    "                run_df['val_loss'],\n",
    "                color=colors[d],\n",
    "                alpha=0.8,\n",
    "                linewidth=2,\n",
    "                label=(f\"d={d}\" if run == sd['run'].min() else None)\n",
    "            )\n",
    "\n",
    "    ax.set_title(name)\n",
    "    # ax.set_ylim(1.75e-2, 3e-2)\n",
    "    ax.set_ylim(1.1, 1.3)\n",
    "    ax.set_xlim(0, 250)\n",
    "    ax.set_xticks(np.linspace(0, 250, 6, dtype=int))\n",
    "\n",
    "    # only bottom row: x-label\n",
    "    if idx // n_cols == n_rows - 1:\n",
    "        ax.set_xlabel(\"Epoch\", fontsize=12)\n",
    "    else:\n",
    "        ax.set_xticklabels([])\n",
    "\n",
    "    # only leftmost column: y-label\n",
    "    if idx % n_cols == 0:\n",
    "        ax.set_ylabel(\"Validation Loss\", fontsize=12)\n",
    "\n",
    "# remove any unused axes\n",
    "for j in range(len(plot_names), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# global legend on right\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels,\n",
    "           title=\"Hidden Depth\",\n",
    "           loc='center left',\n",
    "           bbox_to_anchor=(1, 0.5),\n",
    "           fontsize=12)\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.savefig('figs/val_loss_history.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_3_'></a>[Regularization strength](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model with most data on multiple parameters\n",
    "l1_space = [0.0, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "l2_space = [0.0, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "dropout_space = [0.05, 0.1, 0.2] # [0.0, 0.05, 0.1, 0.2]\n",
    "learning_rate_space = [learning_rate] \n",
    "depth_space = [2] \n",
    "width_space = [64]\n",
    "best_models_reg = {}\n",
    "history_reg = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for year '21...: \n",
      "                                lambda_l1       =0e+00\n",
      "                                lambda_l2       =0e+00\n",
      "                                dropout         =5e-02\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 72\n",
      "Best val loss: 1.13853E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.05_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13277E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.05_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Early stopping at epoch 90\n",
      "Best val loss: 1.13487E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.05_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13585E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.05_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 66\n",
      "Best val loss: 1.13207E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.05_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =0e+00\n",
      "                                lambda_l2       =0e+00\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 72\n",
      "Best val loss: 1.13842E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.1_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13275E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.1_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.43220E-01  - Val Loss: 1.13494E+00\n",
      "Early stopping at epoch 128\n",
      "Best val loss: 1.13403E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.1_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13588E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.1_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 86\n",
      "Best val loss: 1.13325E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.1_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =0e+00\n",
      "                                lambda_l2       =0e+00\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.54433E-01  - Val Loss: 1.14004E+00\n",
      "Early stopping at epoch 100\n",
      "Best val loss: 1.13923E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.2_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13276E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.2_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.56344E-01  - Val Loss: 1.13663E+00\n",
      "Early stopping at epoch 145\n",
      "Best val loss: 1.13582E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.2_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13593E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.2_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 86\n",
      "Best val loss: 1.13394E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.2_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =0e+00\n",
      "                                lambda_l2       =1e-05\n",
      "                                dropout         =5e-02\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 72\n",
      "Best val loss: 1.13854E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.05_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13277E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.05_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Early stopping at epoch 90\n",
      "Best val loss: 1.13489E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.05_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13585E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.05_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 66\n",
      "Best val loss: 1.13205E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.05_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =0e+00\n",
      "                                lambda_l2       =1e-05\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 72\n",
      "Best val loss: 1.13845E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.1_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13275E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.1_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.43633E-01  - Val Loss: 1.13492E+00\n",
      "Early stopping at epoch 128\n",
      "Best val loss: 1.13398E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.1_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13588E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.1_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 86\n",
      "Best val loss: 1.13325E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.1_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =0e+00\n",
      "                                lambda_l2       =1e-05\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.54884E-01  - Val Loss: 1.14000E+00\n",
      "Early stopping at epoch 100\n",
      "Best val loss: 1.13917E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.2_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13276E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.2_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.56797E-01  - Val Loss: 1.13668E+00\n",
      "Early stopping at epoch 145\n",
      "Best val loss: 1.13587E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.2_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13593E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.2_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 86\n",
      "Best val loss: 1.13401E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.2_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =0e+00\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =5e-02\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 72\n",
      "Best val loss: 1.13855E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.05_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13277E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.05_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Early stopping at epoch 90\n",
      "Best val loss: 1.13483E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.05_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13585E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.05_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 66\n",
      "Best val loss: 1.13206E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.05_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =0e+00\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 72\n",
      "Best val loss: 1.13843E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.1_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13275E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.1_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.47602E-01  - Val Loss: 1.13468E+00\n",
      "Early stopping at epoch 128\n",
      "Best val loss: 1.13377E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.1_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13588E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.1_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 86\n",
      "Best val loss: 1.13317E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.1_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =0e+00\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.58696E-01  - Val Loss: 1.14002E+00\n",
      "Early stopping at epoch 100\n",
      "Best val loss: 1.13919E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.2_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13276E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.2_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.60579E-01  - Val Loss: 1.13661E+00\n",
      "Early stopping at epoch 145\n",
      "Best val loss: 1.13571E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.2_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13593E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.2_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 86\n",
      "Best val loss: 1.13394E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.2_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =0e+00\n",
      "                                lambda_l2       =1e-03\n",
      "                                dropout         =5e-02\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 72\n",
      "Best val loss: 1.13855E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.05_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13276E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.05_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Early stopping at epoch 90\n",
      "Best val loss: 1.13504E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.05_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13585E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.05_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 66\n",
      "Best val loss: 1.13253E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.05_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =0e+00\n",
      "                                lambda_l2       =1e-03\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 73\n",
      "Best val loss: 1.13838E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.1_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13274E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.1_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.77248E-01  - Val Loss: 1.13456E+00\n",
      "Epoch 200/250  - Train Loss: 9.52046E-01  - Val Loss: 1.12722E+00\n",
      "Best val loss: 1.12305E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.1_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13588E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.1_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 86\n",
      "Best val loss: 1.13352E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.1_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =0e+00\n",
      "                                lambda_l2       =1e-03\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.87541E-01  - Val Loss: 1.13969E+00\n",
      "Early stopping at epoch 100\n",
      "Best val loss: 1.13910E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.2_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13275E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.2_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.89254E-01  - Val Loss: 1.13654E+00\n",
      "Epoch 200/250  - Train Loss: 9.70820E-01  - Val Loss: 1.13295E+00\n",
      "Best val loss: 1.12968E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.2_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13592E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.2_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 86\n",
      "Best val loss: 1.13419E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.2_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =0e+00\n",
      "                                lambda_l2       =1e-02\n",
      "                                dropout         =5e-02\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 99\n",
      "Best val loss: 1.13940E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.05_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13258E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.05_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07317E+00  - Val Loss: 1.13615E+00\n",
      "Early stopping at epoch 128\n",
      "Best val loss: 1.13570E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.05_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13560E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.05_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 68\n",
      "Best val loss: 1.13540E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.05_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =0e+00\n",
      "                                lambda_l2       =1e-02\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 99\n",
      "Best val loss: 1.13914E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.1_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13256E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.1_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07741E+00  - Val Loss: 1.13667E+00\n",
      "Early stopping at epoch 128\n",
      "Best val loss: 1.13617E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.1_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13566E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.1_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 68\n",
      "Best val loss: 1.13604E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.1_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =0e+00\n",
      "                                lambda_l2       =1e-02\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.08478E+00  - Val Loss: 1.13951E+00\n",
      "Early stopping at epoch 121\n",
      "Best val loss: 1.13929E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.2_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13260E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.2_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.08548E+00  - Val Loss: 1.13826E+00\n",
      "Early stopping at epoch 145\n",
      "Best val loss: 1.13755E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.2_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13572E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.2_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 86\n",
      "Best val loss: 1.13688E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.2_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-05\n",
      "                                lambda_l2       =0e+00\n",
      "                                dropout         =5e-02\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 72\n",
      "Best val loss: 1.13849E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.05_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13277E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.05_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Early stopping at epoch 90\n",
      "Best val loss: 1.13495E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.05_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13585E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.05_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 66\n",
      "Best val loss: 1.13208E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.05_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-05\n",
      "                                lambda_l2       =0e+00\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 72\n",
      "Best val loss: 1.13846E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.1_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13275E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.1_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Early stopping at epoch 90\n",
      "Best val loss: 1.13535E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.1_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13588E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.1_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 86\n",
      "Best val loss: 1.13322E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.1_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-05\n",
      "                                lambda_l2       =0e+00\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.62900E-01  - Val Loss: 1.13995E+00\n",
      "Early stopping at epoch 100\n",
      "Best val loss: 1.13913E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.2_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13276E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.2_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.64803E-01  - Val Loss: 1.13660E+00\n",
      "Early stopping at epoch 145\n",
      "Best val loss: 1.13580E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.2_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13593E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.2_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 86\n",
      "Best val loss: 1.13393E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.2_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-05\n",
      "                                lambda_l2       =1e-05\n",
      "                                dropout         =5e-02\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 72\n",
      "Best val loss: 1.13848E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.05_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13277E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.05_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Early stopping at epoch 90\n",
      "Best val loss: 1.13491E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.05_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13585E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.05_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 66\n",
      "Best val loss: 1.13204E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.05_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-05\n",
      "                                lambda_l2       =1e-05\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 72\n",
      "Best val loss: 1.13844E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.1_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13275E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.1_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.52221E-01  - Val Loss: 1.13483E+00\n",
      "Early stopping at epoch 128\n",
      "Best val loss: 1.13383E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.1_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13588E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.1_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 86\n",
      "Best val loss: 1.13320E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.1_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-05\n",
      "                                lambda_l2       =1e-05\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.63312E-01  - Val Loss: 1.13993E+00\n",
      "Early stopping at epoch 100\n",
      "Best val loss: 1.13909E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.2_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13276E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.2_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.65202E-01  - Val Loss: 1.13660E+00\n",
      "Epoch 200/250  - Train Loss: 9.51732E-01  - Val Loss: 1.13313E+00\n",
      "Best val loss: 1.13026E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.2_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13593E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.2_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 86\n",
      "Best val loss: 1.13393E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.2_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-05\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =5e-02\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 72\n",
      "Best val loss: 1.13850E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.05_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13277E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.05_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Early stopping at epoch 90\n",
      "Best val loss: 1.13489E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.05_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13585E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.05_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 66\n",
      "Best val loss: 1.13209E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.05_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-05\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 73\n",
      "Best val loss: 1.13839E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.1_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13275E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.1_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.55804E-01  - Val Loss: 1.13466E+00\n",
      "Epoch 200/250  - Train Loss: 9.33769E-01  - Val Loss: 1.12723E+00\n",
      "Best val loss: 1.12330E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.1_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13588E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.1_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 86\n",
      "Best val loss: 1.13321E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.1_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-05\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.66764E-01  - Val Loss: 1.13980E+00\n",
      "Early stopping at epoch 100\n",
      "Best val loss: 1.13903E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.2_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13276E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.2_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.68691E-01  - Val Loss: 1.13650E+00\n",
      "Epoch 200/250  - Train Loss: 9.55046E-01  - Val Loss: 1.13309E+00\n",
      "Best val loss: 1.13010E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.2_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13593E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.2_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 86\n",
      "Best val loss: 1.13395E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.2_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-05\n",
      "                                lambda_l2       =1e-03\n",
      "                                dropout         =5e-02\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 72\n",
      "Best val loss: 1.13851E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.05_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13275E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.05_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Early stopping at epoch 90\n",
      "Best val loss: 1.13505E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.05_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13585E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.05_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 66\n",
      "Best val loss: 1.13258E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.05_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-05\n",
      "                                lambda_l2       =1e-03\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 73\n",
      "Best val loss: 1.13834E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.1_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13274E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.1_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.83757E-01  - Val Loss: 1.13451E+00\n",
      "Epoch 200/250  - Train Loss: 9.57562E-01  - Val Loss: 1.12740E+00\n",
      "Best val loss: 1.12333E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.1_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13588E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.1_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 86\n",
      "Best val loss: 1.13370E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.1_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-05\n",
      "                                lambda_l2       =1e-03\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 9.94007E-01  - Val Loss: 1.13957E+00\n",
      "Early stopping at epoch 100\n",
      "Best val loss: 1.13899E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.2_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13275E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.2_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.95704E-01  - Val Loss: 1.13647E+00\n",
      "Epoch 200/250  - Train Loss: 9.76118E-01  - Val Loss: 1.13277E+00\n",
      "Best val loss: 1.12954E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.2_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13593E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.2_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 86\n",
      "Best val loss: 1.13425E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.2_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-05\n",
      "                                lambda_l2       =1e-02\n",
      "                                dropout         =5e-02\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 99\n",
      "Best val loss: 1.13935E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.05_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13258E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.05_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07589E+00  - Val Loss: 1.13615E+00\n",
      "Early stopping at epoch 128\n",
      "Best val loss: 1.13567E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.05_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13560E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.05_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 68\n",
      "Best val loss: 1.13544E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.05_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-05\n",
      "                                lambda_l2       =1e-02\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 99\n",
      "Best val loss: 1.13909E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.1_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13256E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.1_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.08010E+00  - Val Loss: 1.13661E+00\n",
      "Early stopping at epoch 128\n",
      "Best val loss: 1.13609E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.1_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13565E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.1_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 68\n",
      "Best val loss: 1.13607E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.1_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-05\n",
      "                                lambda_l2       =1e-02\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.08745E+00  - Val Loss: 1.13952E+00\n",
      "Early stopping at epoch 121\n",
      "Best val loss: 1.13929E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.2_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13260E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.2_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.08823E+00  - Val Loss: 1.13828E+00\n",
      "Early stopping at epoch 145\n",
      "Best val loss: 1.13758E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.2_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13572E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.2_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 86\n",
      "Best val loss: 1.13693E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.2_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =0e+00\n",
      "                                dropout         =5e-02\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 72\n",
      "Best val loss: 1.13810E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.05_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13276E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.05_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Early stopping at epoch 90\n",
      "Best val loss: 1.13510E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.05_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13585E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.05_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 66\n",
      "Best val loss: 1.13272E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.05_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =0e+00\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 73\n",
      "Best val loss: 1.13772E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.1_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13274E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.1_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.01243E+00  - Val Loss: 1.13437E+00\n",
      "Early stopping at epoch 128\n",
      "Best val loss: 1.13341E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.1_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13588E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.1_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 86\n",
      "Best val loss: 1.13383E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.1_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =0e+00\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.02248E+00  - Val Loss: 1.13868E+00\n",
      "Early stopping at epoch 100\n",
      "Best val loss: 1.13840E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.2_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13276E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.2_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.02468E+00  - Val Loss: 1.13635E+00\n",
      "Early stopping at epoch 145\n",
      "Best val loss: 1.13557E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.2_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13592E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.2_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 86\n",
      "Best val loss: 1.13427E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.2_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-05\n",
      "                                dropout         =5e-02\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 72\n",
      "Best val loss: 1.13807E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.05_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13276E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.05_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Early stopping at epoch 90\n",
      "Best val loss: 1.13516E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.05_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13585E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.05_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 66\n",
      "Best val loss: 1.13274E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.05_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-05\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 73\n",
      "Best val loss: 1.13772E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.1_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13274E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.1_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.01268E+00  - Val Loss: 1.13434E+00\n",
      "Early stopping at epoch 128\n",
      "Best val loss: 1.13342E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.1_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13588E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.1_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 86\n",
      "Best val loss: 1.13382E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.1_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-05\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.02272E+00  - Val Loss: 1.13864E+00\n",
      "Early stopping at epoch 100\n",
      "Best val loss: 1.13830E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.2_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13276E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.2_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.02491E+00  - Val Loss: 1.13636E+00\n",
      "Early stopping at epoch 145\n",
      "Best val loss: 1.13560E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.2_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13592E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.2_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 86\n",
      "Best val loss: 1.13429E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.2_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =5e-02\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 72\n",
      "Best val loss: 1.13808E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.05_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13276E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.05_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Early stopping at epoch 90\n",
      "Best val loss: 1.13510E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.05_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13585E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.05_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 66\n",
      "Best val loss: 1.13274E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.05_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 73\n",
      "Best val loss: 1.13771E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.1_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13274E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.1_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.01487E+00  - Val Loss: 1.13431E+00\n",
      "Early stopping at epoch 128\n",
      "Best val loss: 1.13340E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.1_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13588E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.1_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 86\n",
      "Best val loss: 1.13387E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.1_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.02483E+00  - Val Loss: 1.13861E+00\n",
      "Early stopping at epoch 100\n",
      "Best val loss: 1.13827E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.2_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13276E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.2_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.02701E+00  - Val Loss: 1.13637E+00\n",
      "Early stopping at epoch 145\n",
      "Best val loss: 1.13556E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.2_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13592E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.2_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 86\n",
      "Best val loss: 1.13433E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.2_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-03\n",
      "                                dropout         =5e-02\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 72\n",
      "Best val loss: 1.13803E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.05_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13275E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.05_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Early stopping at epoch 90\n",
      "Best val loss: 1.13511E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.05_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13583E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.05_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 66\n",
      "Best val loss: 1.13310E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.05_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-03\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 73\n",
      "Best val loss: 1.13768E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.1_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13273E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.1_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.03265E+00  - Val Loss: 1.13451E+00\n",
      "Early stopping at epoch 128\n",
      "Best val loss: 1.13368E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.1_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13587E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.1_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 68\n",
      "Best val loss: 1.13425E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.1_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-03\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.04229E+00  - Val Loss: 1.13853E+00\n",
      "Early stopping at epoch 100\n",
      "Best val loss: 1.13820E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.2_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13275E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.2_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.04425E+00  - Val Loss: 1.13643E+00\n",
      "Early stopping at epoch 145\n",
      "Best val loss: 1.13569E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.2_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13591E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.2_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 86\n",
      "Best val loss: 1.13472E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.2_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-02\n",
      "                                dropout         =5e-02\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 99\n",
      "Best val loss: 1.13939E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.05_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13257E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.05_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.09765E+00  - Val Loss: 1.13653E+00\n",
      "Early stopping at epoch 128\n",
      "Best val loss: 1.13610E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.05_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13557E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.05_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 68\n",
      "Best val loss: 1.13591E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.05_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-02\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 99\n",
      "Best val loss: 1.13923E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.1_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13255E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.1_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.10152E+00  - Val Loss: 1.13695E+00\n",
      "Early stopping at epoch 145\n",
      "Best val loss: 1.13650E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.1_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13563E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.1_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 68\n",
      "Best val loss: 1.13651E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.1_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-04\n",
      "                                lambda_l2       =1e-02\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.10851E+00  - Val Loss: 1.13968E+00\n",
      "Early stopping at epoch 132\n",
      "Best val loss: 1.13949E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.2_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13259E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.2_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.10962E+00  - Val Loss: 1.13864E+00\n",
      "Early stopping at epoch 159\n",
      "Best val loss: 1.13788E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.2_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13570E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.2_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 86\n",
      "Best val loss: 1.13732E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.2_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-03\n",
      "                                lambda_l2       =0e+00\n",
      "                                dropout         =5e-02\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.16210E+00  - Val Loss: 1.13837E+00\n",
      "Early stopping at epoch 141\n",
      "Best val loss: 1.13814E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.05_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13270E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.05_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.16474E+00  - Val Loss: 1.13748E+00\n",
      "Early stopping at epoch 144\n",
      "Best val loss: 1.13698E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.05_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13565E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.05_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 68\n",
      "Best val loss: 1.13741E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.05_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-03\n",
      "                                lambda_l2       =0e+00\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.16667E+00  - Val Loss: 1.13894E+00\n",
      "Early stopping at epoch 199\n",
      "Best val loss: 1.13807E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.1_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13270E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.1_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.16979E+00  - Val Loss: 1.13777E+00\n",
      "Early stopping at epoch 145\n",
      "Best val loss: 1.13730E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.1_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13569E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.1_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 86\n",
      "Best val loss: 1.13794E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.1_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-03\n",
      "                                lambda_l2       =0e+00\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.17945E+00  - Val Loss: 1.13930E+00\n",
      "Early stopping at epoch 199\n",
      "Best val loss: 1.13807E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.2_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13272E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.2_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.18197E+00  - Val Loss: 1.13901E+00\n",
      "Early stopping at epoch 158\n",
      "Best val loss: 1.13850E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.2_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13575E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.2_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 86\n",
      "Best val loss: 1.13829E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.2_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-03\n",
      "                                lambda_l2       =1e-05\n",
      "                                dropout         =5e-02\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.16213E+00  - Val Loss: 1.13832E+00\n",
      "Early stopping at epoch 141\n",
      "Best val loss: 1.13811E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.05_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13270E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.05_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.16475E+00  - Val Loss: 1.13749E+00\n",
      "Early stopping at epoch 144\n",
      "Best val loss: 1.13698E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.05_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13565E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.05_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 68\n",
      "Best val loss: 1.13742E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.05_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-03\n",
      "                                lambda_l2       =1e-05\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.16667E+00  - Val Loss: 1.13894E+00\n",
      "Early stopping at epoch 199\n",
      "Best val loss: 1.13806E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.1_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13270E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.1_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.16981E+00  - Val Loss: 1.13777E+00\n",
      "Early stopping at epoch 145\n",
      "Best val loss: 1.13730E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.1_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13569E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.1_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 86\n",
      "Best val loss: 1.13795E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.1_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-03\n",
      "                                lambda_l2       =1e-05\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.17947E+00  - Val Loss: 1.13930E+00\n",
      "Early stopping at epoch 199\n",
      "Best val loss: 1.13805E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.2_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13272E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.2_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.18197E+00  - Val Loss: 1.13904E+00\n",
      "Early stopping at epoch 158\n",
      "Best val loss: 1.13852E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.2_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13575E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.2_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 86\n",
      "Best val loss: 1.13830E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.2_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-03\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =5e-02\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.16232E+00  - Val Loss: 1.13836E+00\n",
      "Early stopping at epoch 141\n",
      "Best val loss: 1.13813E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.05_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13270E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.05_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.16490E+00  - Val Loss: 1.13755E+00\n",
      "Early stopping at epoch 144\n",
      "Best val loss: 1.13705E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.05_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13565E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.05_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 68\n",
      "Best val loss: 1.13745E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.05_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-03\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.16686E+00  - Val Loss: 1.13899E+00\n",
      "Early stopping at epoch 199\n",
      "Best val loss: 1.13808E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.1_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13269E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.1_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.16996E+00  - Val Loss: 1.13783E+00\n",
      "Early stopping at epoch 145\n",
      "Best val loss: 1.13736E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.1_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13569E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.1_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 86\n",
      "Best val loss: 1.13797E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.1_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-03\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.17955E+00  - Val Loss: 1.13933E+00\n",
      "Early stopping at epoch 199\n",
      "Best val loss: 1.13809E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.2_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13272E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.2_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.18207E+00  - Val Loss: 1.13908E+00\n",
      "Early stopping at epoch 158\n",
      "Best val loss: 1.13857E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.2_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13574E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.2_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 86\n",
      "Best val loss: 1.13832E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.2_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-03\n",
      "                                lambda_l2       =1e-03\n",
      "                                dropout         =5e-02\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.16460E+00  - Val Loss: 1.13862E+00\n",
      "Early stopping at epoch 199\n",
      "Best val loss: 1.13802E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.05_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13267E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.05_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.16692E+00  - Val Loss: 1.13814E+00\n",
      "Early stopping at epoch 144\n",
      "Best val loss: 1.13760E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.05_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13562E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.05_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 68\n",
      "Best val loss: 1.13776E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.05_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-03\n",
      "                                lambda_l2       =1e-03\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.16897E+00  - Val Loss: 1.13924E+00\n",
      "Early stopping at epoch 176\n",
      "Best val loss: 1.13801E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.1_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13266E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.1_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.17174E+00  - Val Loss: 1.13842E+00\n",
      "Early stopping at epoch 144\n",
      "Best val loss: 1.13791E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.1_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13566E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.1_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 86\n",
      "Best val loss: 1.13821E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.1_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-03\n",
      "                                lambda_l2       =1e-03\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.18112E+00  - Val Loss: 1.13972E+00\n",
      "Early stopping at epoch 199\n",
      "Best val loss: 1.13843E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.2_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13270E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.2_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.18328E+00  - Val Loss: 1.13962E+00\n",
      "Early stopping at epoch 153\n",
      "Best val loss: 1.13910E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.2_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13572E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.2_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 86\n",
      "Best val loss: 1.13862E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.2_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-03\n",
      "                                lambda_l2       =1e-02\n",
      "                                dropout         =5e-02\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.20712E+00  - Val Loss: 1.14207E+00\n",
      "Epoch 200/250  - Train Loss: 1.03227E+00  - Val Loss: 1.14028E+00\n",
      "Best val loss: 1.13978E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.05_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13243E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.05_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.20946E+00  - Val Loss: 1.14272E+00\n",
      "Epoch 200/250  - Train Loss: 1.03132E+00  - Val Loss: 1.14135E+00\n",
      "Early stopping at epoch 208\n",
      "Best val loss: 1.14124E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.05_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13536E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.05_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.14068E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.05_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-03\n",
      "                                lambda_l2       =1e-02\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.21013E+00  - Val Loss: 1.14236E+00\n",
      "Epoch 200/250  - Train Loss: 1.03298E+00  - Val Loss: 1.14063E+00\n",
      "Best val loss: 1.14012E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.1_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13242E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.1_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.21279E+00  - Val Loss: 1.14296E+00\n",
      "Epoch 200/250  - Train Loss: 1.03278E+00  - Val Loss: 1.14155E+00\n",
      "Best val loss: 1.14108E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.1_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13540E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.1_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.14070E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.1_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-03\n",
      "                                lambda_l2       =1e-02\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.21837E+00  - Val Loss: 1.14271E+00\n",
      "Epoch 200/250  - Train Loss: 1.03545E+00  - Val Loss: 1.14139E+00\n",
      "Best val loss: 1.14082E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.2_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13246E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.2_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.22029E+00  - Val Loss: 1.14356E+00\n",
      "Epoch 200/250  - Train Loss: 1.03603E+00  - Val Loss: 1.14203E+00\n",
      "Early stopping at epoch 209\n",
      "Best val loss: 1.14197E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.2_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13548E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.2_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.14083E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.2_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-02\n",
      "                                lambda_l2       =0e+00\n",
      "                                dropout         =5e-02\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.97447E+00  - Val Loss: 1.15664E+00\n",
      "Epoch 200/250  - Train Loss: 1.11642E+00  - Val Loss: 1.15302E+00\n",
      "Best val loss: 1.15292E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.05_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13175E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.05_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.98544E+00  - Val Loss: 1.15937E+00\n",
      "Epoch 200/250  - Train Loss: 1.11541E+00  - Val Loss: 1.15364E+00\n",
      "Best val loss: 1.15293E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.05_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13453E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.05_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.14044E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.05_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-02\n",
      "                                lambda_l2       =0e+00\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.97542E+00  - Val Loss: 1.15662E+00\n",
      "Epoch 200/250  - Train Loss: 1.11685E+00  - Val Loss: 1.15303E+00\n",
      "Best val loss: 1.15292E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.1_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13176E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.1_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.98613E+00  - Val Loss: 1.15936E+00\n",
      "Epoch 200/250  - Train Loss: 1.11568E+00  - Val Loss: 1.15365E+00\n",
      "Best val loss: 1.15293E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.1_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13455E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.1_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.14048E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.1_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-02\n",
      "                                lambda_l2       =0e+00\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.97781E+00  - Val Loss: 1.15660E+00\n",
      "Epoch 200/250  - Train Loss: 1.11803E+00  - Val Loss: 1.15304E+00\n",
      "Best val loss: 1.15292E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.2_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13179E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.2_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.98783E+00  - Val Loss: 1.15934E+00\n",
      "Epoch 200/250  - Train Loss: 1.11639E+00  - Val Loss: 1.15367E+00\n",
      "Best val loss: 1.15293E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.2_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13459E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.2_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.14052E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.2_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-02\n",
      "                                lambda_l2       =1e-05\n",
      "                                dropout         =5e-02\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.97456E+00  - Val Loss: 1.15664E+00\n",
      "Epoch 200/250  - Train Loss: 1.11644E+00  - Val Loss: 1.15302E+00\n",
      "Best val loss: 1.15292E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.05_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13175E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.05_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.98553E+00  - Val Loss: 1.15937E+00\n",
      "Epoch 200/250  - Train Loss: 1.11545E+00  - Val Loss: 1.15364E+00\n",
      "Best val loss: 1.15293E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.05_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13453E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.05_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.14044E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.05_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-02\n",
      "                                lambda_l2       =1e-05\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.97550E+00  - Val Loss: 1.15662E+00\n",
      "Epoch 200/250  - Train Loss: 1.11688E+00  - Val Loss: 1.15303E+00\n",
      "Best val loss: 1.15292E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.1_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13176E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.1_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.98622E+00  - Val Loss: 1.15936E+00\n",
      "Epoch 200/250  - Train Loss: 1.11572E+00  - Val Loss: 1.15365E+00\n",
      "Best val loss: 1.15293E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.1_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13455E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.1_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.14048E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.1_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-02\n",
      "                                lambda_l2       =1e-05\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.97790E+00  - Val Loss: 1.15660E+00\n",
      "Epoch 200/250  - Train Loss: 1.11806E+00  - Val Loss: 1.15304E+00\n",
      "Best val loss: 1.15292E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.2_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13179E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.2_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.98792E+00  - Val Loss: 1.15934E+00\n",
      "Epoch 200/250  - Train Loss: 1.11641E+00  - Val Loss: 1.15367E+00\n",
      "Best val loss: 1.15293E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.2_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13459E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.2_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.14052E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.2_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-02\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =5e-02\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.97532E+00  - Val Loss: 1.15664E+00\n",
      "Epoch 200/250  - Train Loss: 1.11676E+00  - Val Loss: 1.15302E+00\n",
      "Best val loss: 1.15292E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.05_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13175E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.05_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.98629E+00  - Val Loss: 1.15937E+00\n",
      "Epoch 200/250  - Train Loss: 1.11576E+00  - Val Loss: 1.15364E+00\n",
      "Best val loss: 1.15293E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.05_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13453E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.05_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.14044E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.05_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-02\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.97626E+00  - Val Loss: 1.15662E+00\n",
      "Epoch 200/250  - Train Loss: 1.11718E+00  - Val Loss: 1.15303E+00\n",
      "Best val loss: 1.15292E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.1_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13176E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.1_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.98698E+00  - Val Loss: 1.15937E+00\n",
      "Epoch 200/250  - Train Loss: 1.11602E+00  - Val Loss: 1.15365E+00\n",
      "Best val loss: 1.15293E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.1_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13455E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.1_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.14048E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.1_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-02\n",
      "                                lambda_l2       =1e-04\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.97864E+00  - Val Loss: 1.15661E+00\n",
      "Epoch 200/250  - Train Loss: 1.11835E+00  - Val Loss: 1.15304E+00\n",
      "Best val loss: 1.15293E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.2_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13179E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.2_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.98868E+00  - Val Loss: 1.15934E+00\n",
      "Epoch 200/250  - Train Loss: 1.11672E+00  - Val Loss: 1.15367E+00\n",
      "Best val loss: 1.15293E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.2_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13459E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.2_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.14052E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.2_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-02\n",
      "                                lambda_l2       =1e-03\n",
      "                                dropout         =5e-02\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.98289E+00  - Val Loss: 1.15666E+00\n",
      "Epoch 200/250  - Train Loss: 1.11984E+00  - Val Loss: 1.15302E+00\n",
      "Best val loss: 1.15292E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.05_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13175E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.05_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.99392E+00  - Val Loss: 1.15939E+00\n",
      "Epoch 200/250  - Train Loss: 1.11884E+00  - Val Loss: 1.15363E+00\n",
      "Best val loss: 1.15293E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.05_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13453E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.05_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.14043E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.05_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-02\n",
      "                                lambda_l2       =1e-03\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.98384E+00  - Val Loss: 1.15663E+00\n",
      "Epoch 200/250  - Train Loss: 1.12025E+00  - Val Loss: 1.15303E+00\n",
      "Best val loss: 1.15292E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.1_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13176E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.1_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.99461E+00  - Val Loss: 1.15939E+00\n",
      "Epoch 200/250  - Train Loss: 1.11910E+00  - Val Loss: 1.15364E+00\n",
      "Best val loss: 1.15293E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.1_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13454E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.1_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.14047E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.1_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-02\n",
      "                                lambda_l2       =1e-03\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.98620E+00  - Val Loss: 1.15662E+00\n",
      "Epoch 200/250  - Train Loss: 1.12140E+00  - Val Loss: 1.15305E+00\n",
      "Best val loss: 1.15292E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.2_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13179E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.2_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.99631E+00  - Val Loss: 1.15936E+00\n",
      "Epoch 200/250  - Train Loss: 1.11978E+00  - Val Loss: 1.15366E+00\n",
      "Best val loss: 1.15293E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.2_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13458E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.2_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.14052E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.2_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-02\n",
      "                                lambda_l2       =1e-02\n",
      "                                dropout         =5e-02\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 2.05861E+00  - Val Loss: 1.15676E+00\n",
      "Epoch 200/250  - Train Loss: 1.14868E+00  - Val Loss: 1.15303E+00\n",
      "Best val loss: 1.15292E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.05_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13173E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.05_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 2.07017E+00  - Val Loss: 1.15951E+00\n",
      "Epoch 200/250  - Train Loss: 1.14789E+00  - Val Loss: 1.15359E+00\n",
      "Best val loss: 1.15293E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.05_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13451E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.05_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.14042E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.05_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-02\n",
      "                                lambda_l2       =1e-02\n",
      "                                dropout         =1e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 2.05951E+00  - Val Loss: 1.15675E+00\n",
      "Epoch 200/250  - Train Loss: 1.14902E+00  - Val Loss: 1.15303E+00\n",
      "Best val loss: 1.15292E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.1_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13174E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.1_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 2.07086E+00  - Val Loss: 1.15950E+00\n",
      "Epoch 200/250  - Train Loss: 1.14812E+00  - Val Loss: 1.15361E+00\n",
      "Best val loss: 1.15293E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.1_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13452E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.1_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.14045E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.1_lr5e-05_w64_d2_run5.pth\n",
      "Training model for year '21...: \n",
      "                                lambda_l1       =1e-02\n",
      "                                lambda_l2       =1e-02\n",
      "                                dropout         =2e-01\n",
      "                                learning_rate   =5e-05\n",
      "                                hidden_depth    =2\n",
      "                                hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 2.06178E+00  - Val Loss: 1.15673E+00\n",
      "Epoch 200/250  - Train Loss: 1.14998E+00  - Val Loss: 1.15305E+00\n",
      "Best val loss: 1.15292E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.2_lr5e-05_w64_d2_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13177E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.2_lr5e-05_w64_d2_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 2.07251E+00  - Val Loss: 1.15950E+00\n",
      "Epoch 200/250  - Train Loss: 1.14869E+00  - Val Loss: 1.15363E+00\n",
      "Best val loss: 1.15293E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.2_lr5e-05_w64_d2_run3.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13455E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.2_lr5e-05_w64_d2_run4.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.14050E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.2_lr5e-05_w64_d2_run5.pth\n"
     ]
    }
   ],
   "source": [
    "# current runtime: 9h 50\n",
    "for lambda_l1 in l1_space:\n",
    "    for lambda_l2 in l2_space:\n",
    "        for dropout in dropout_space:\n",
    "            for learning_rate in learning_rate_space:\n",
    "                for hidden_depth in depth_space:\n",
    "                    for hidden_width in width_space:\n",
    "                        print(f\"\"\"Training model for year '{year}...: \n",
    "                                lambda_l1       ={lambda_l1:.0e}\n",
    "                                lambda_l2       ={lambda_l2:.0e}\n",
    "                                dropout         ={dropout:.0e}\n",
    "                                learning_rate   ={learning_rate:.0e}\n",
    "                                hidden_depth    ={hidden_depth}\n",
    "                                hidden_width    ={hidden_width}\"\"\")\n",
    "                        for run in range(n_runs):\n",
    "                            print(f\"Run {run+1} of {n_runs}\")\n",
    "                            seed = 42+run\n",
    "                            np.random.seed(seed)\n",
    "                            torch.manual_seed(seed)\n",
    "                            # Initialize the model\n",
    "                            input_dim = X_train[year].shape[1]\n",
    "                            name = f'l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}'\n",
    "                            models_21[name] = MLPModel(input_dim, depth=hidden_depth, width=hidden_width, dropout=dropout, activation=activation_fun).to(device)\n",
    "                            optimizer = torch.optim.Adam(models_21[name].parameters(), lr=learning_rate)\n",
    "                            train = MLPdataset(X_train[year], y_train[year])\n",
    "                            val = MLPdataset(X_val[year], y_val[year])\n",
    "                            best_models_reg[name], history_reg[name] = train_mlp(train,          \n",
    "                                                            val,\n",
    "                                                            models_21[name],\n",
    "                                                            criterion,\n",
    "                                                            epochs,\n",
    "                                                            patience,\n",
    "                                                            print_freq,\n",
    "                                                            device,\n",
    "                                                            optimizer,\n",
    "                                                            lambda_l1=lambda_l1,\n",
    "                                                            lambda_l2=lambda_l2,\n",
    "                                                            batch_size=batch_size,\n",
    "                                                            shuffle_train=True,\n",
    "                                                            shuffle_val=False,\n",
    "                                                            save_path=f'models/hyperparam_test/mlp_y{year}_l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}.pth'\n",
    "                                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_history(history_reg, \n",
    "                save_csv='models/hyperparam_test/history/history_reg.csv')\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model with most data on multiple parameters\n",
    "l1_space = [0.0, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "l2_space = [0.0, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "dropout_space = [0.0, 0.05 , 0.1, 0.2] \n",
    "learning_rate_space = [learning_rate] \n",
    "depth_space = [2] \n",
    "width_space = [64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "val_ds     = MLPdataset(X_val[year], y_val[year])\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# build (n_drop, n_l1, n_l2) mean-loss array\n",
    "n_l1   = len(l1_space)\n",
    "n_l2   = len(l2_space)\n",
    "n_drop = len(dropout_space)\n",
    "\n",
    "losses = np.zeros((n_drop, n_l1, n_l2), dtype=float)\n",
    "\n",
    "for di, drop in enumerate(dropout_space):\n",
    "    for i, l1 in enumerate(l1_space):\n",
    "        for j, l2 in enumerate(l2_space):\n",
    "            run_losses = []\n",
    "            # for each seed, load & eval\n",
    "            for run in range(n_runs):\n",
    "                # since depth_space & width_space each have one entry, \n",
    "                # we can just index 0 here — but this will generalize\n",
    "                tmp_losses = []\n",
    "                for d in depth_space:\n",
    "                    for w in width_space:\n",
    "                        m = load_model(w, d, run, l1, l2, drop, learning_rate, )\n",
    "                        with torch.no_grad():\n",
    "                            batch = [criterion(m(x.to(device)), y.to(device)).item()\n",
    "                                     for x,y in val_loader]\n",
    "                        tmp_losses.append(np.mean(batch))\n",
    "                run_losses.append(np.mean(tmp_losses))\n",
    "            # now average over runs\n",
    "            losses[di, i, j] = np.mean(run_losses)\n",
    "\n",
    "\n",
    "# global color‐scale\n",
    "vmin    = losses.min()\n",
    "vmax    = losses.max()\n",
    "vcenter = vmin + 0.5*(vmax - vmin)\n",
    "norm    = TwoSlopeNorm(vmin=vmin, vcenter=vcenter, vmax=vmax)\n",
    "cmap    = 'viridis'\n",
    "\n",
    "# layout: up to 2 columns\n",
    "ncols = min(2, n_drop)\n",
    "nrows = math.ceil(n_drop / ncols)\n",
    "\n",
    "fig, axes = plt.subplots(nrows, ncols,\n",
    "                         figsize=(5*ncols, 4*nrows),\n",
    "                         squeeze=False)\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for idx, drop in enumerate(dropout_space):\n",
    "    ax = axes_flat[idx]\n",
    "    im = ax.imshow(\n",
    "        losses[idx],\n",
    "        aspect='auto',\n",
    "        cmap=cmap,\n",
    "        norm=norm,\n",
    "        origin='lower',\n",
    "    )\n",
    "\n",
    "    # y = l1, x = l2\n",
    "    ax.set_yticks(np.arange(n_l1))\n",
    "    ax.set_yticklabels(l1_space)\n",
    "    ax.set_xticks(np.arange(n_l2))\n",
    "    ax.set_xticklabels(l2_space)\n",
    "\n",
    "    # only bottom row shows x‐labels\n",
    "    if idx < (nrows-1)*ncols:\n",
    "        ax.tick_params(labelbottom=False)\n",
    "    else:\n",
    "        ax.set_xlabel(r'$\\ell_2$')\n",
    "\n",
    "    # only first‐column shows y‐labels\n",
    "    if idx % ncols != 0:\n",
    "        ax.tick_params(labelleft=False)\n",
    "    else:\n",
    "        ax.set_ylabel(r'$\\ell_1$')\n",
    "\n",
    "    ax.set_title(f\"dropout = {drop:.2f}\")\n",
    "\n",
    "# clear unused\n",
    "for ax in axes_flat[n_drop:]:\n",
    "    fig.delaxes(ax)\n",
    "\n",
    "# colorbar at the right edge\n",
    "fig.subplots_adjust(right=0.88)\n",
    "cax = fig.add_axes([0.90, 0.15, 0.02, 0.7])  # [left, bottom, width, height]\n",
    "cbar = fig.colorbar(im, cax=cax)\n",
    "cbar.set_label('Average Validation Loss (MSE)')\n",
    "\n",
    "plt.savefig('figs/l1_l2_dropout_loss.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_4_'></a>[Pyramid](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model with most data on multiple parameters\n",
    "l1_space = [0.0, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "l2_space = [0.0, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "dropout_space = [0.05 , 0.1, 0.2] # [0.0, 0.05, 0.1, 0.2]\n",
    "learning_rate_space = [learning_rate]\n",
    "depth_space = None\n",
    "width_space = [[32, 16, 8]]\n",
    "\n",
    "best_models_reg_pyramid = {}\n",
    "history_reg_pyramid = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for year '21...: \n",
      "                            lambda_l1       =0e+00\n",
      "                            lambda_l2       =0e+00\n",
      "                            dropout         =5e-02\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 76\n",
      "Best val loss: 1.12832E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.05_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 85\n",
      "Best val loss: 1.14028E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.05_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.56799E-01  - Val Loss: 1.14071E+00\n",
      "Early stopping at epoch 182\n",
      "Best val loss: 1.13786E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.05_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.62097E-01  - Val Loss: 1.14844E+00\n",
      "Epoch 200/250  - Train Loss: 9.41359E-01  - Val Loss: 1.14265E+00\n",
      "Best val loss: 1.14050E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.05_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.63062E-01  - Val Loss: 1.13023E+00\n",
      "Early stopping at epoch 101\n",
      "Best val loss: 1.12909E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.05_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =0e+00\n",
      "                            lambda_l2       =0e+00\n",
      "                            dropout         =1e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 74\n",
      "Best val loss: 1.12836E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.1_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 85\n",
      "Best val loss: 1.13986E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.1_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.62513E-01  - Val Loss: 1.14055E+00\n",
      "Early stopping at epoch 182\n",
      "Best val loss: 1.13729E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.1_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.67141E-01  - Val Loss: 1.14883E+00\n",
      "Epoch 200/250  - Train Loss: 9.50984E-01  - Val Loss: 1.14172E+00\n",
      "Best val loss: 1.13953E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.1_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.67442E-01  - Val Loss: 1.13078E+00\n",
      "Early stopping at epoch 105\n",
      "Best val loss: 1.13003E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.1_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =0e+00\n",
      "                            lambda_l2       =0e+00\n",
      "                            dropout         =2e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 77\n",
      "Best val loss: 1.12869E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.2_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.72682E-01  - Val Loss: 1.13930E+00\n",
      "Early stopping at epoch 153\n",
      "Best val loss: 1.13865E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.2_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.70998E-01  - Val Loss: 1.14149E+00\n",
      "Epoch 200/250  - Train Loss: 9.62199E-01  - Val Loss: 1.13737E+00\n",
      "Early stopping at epoch 242\n",
      "Best val loss: 1.13677E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.2_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.77370E-01  - Val Loss: 1.15194E+00\n",
      "Epoch 200/250  - Train Loss: 9.64002E-01  - Val Loss: 1.14157E+00\n",
      "Best val loss: 1.13848E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.2_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.74133E-01  - Val Loss: 1.13145E+00\n",
      "Early stopping at epoch 112\n",
      "Best val loss: 1.13095E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0_drop0.2_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =0e+00\n",
      "                            lambda_l2       =1e-05\n",
      "                            dropout         =5e-02\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 76\n",
      "Best val loss: 1.12829E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.05_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 85\n",
      "Best val loss: 1.14026E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.05_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.57034E-01  - Val Loss: 1.14084E+00\n",
      "Early stopping at epoch 159\n",
      "Best val loss: 1.13832E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.05_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.62327E-01  - Val Loss: 1.14843E+00\n",
      "Epoch 200/250  - Train Loss: 9.41577E-01  - Val Loss: 1.14262E+00\n",
      "Early stopping at epoch 236\n",
      "Best val loss: 1.14145E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.05_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.63305E-01  - Val Loss: 1.13024E+00\n",
      "Early stopping at epoch 101\n",
      "Best val loss: 1.12908E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.05_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =0e+00\n",
      "                            lambda_l2       =1e-05\n",
      "                            dropout         =1e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 74\n",
      "Best val loss: 1.12836E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.1_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 85\n",
      "Best val loss: 1.13981E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.1_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.62724E-01  - Val Loss: 1.14059E+00\n",
      "Early stopping at epoch 181\n",
      "Best val loss: 1.13748E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.1_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.67386E-01  - Val Loss: 1.14884E+00\n",
      "Epoch 200/250  - Train Loss: 9.51218E-01  - Val Loss: 1.14164E+00\n",
      "Best val loss: 1.13960E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.1_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.67666E-01  - Val Loss: 1.13078E+00\n",
      "Early stopping at epoch 105\n",
      "Best val loss: 1.13006E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.1_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =0e+00\n",
      "                            lambda_l2       =1e-05\n",
      "                            dropout         =2e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 77\n",
      "Best val loss: 1.12873E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.2_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.72877E-01  - Val Loss: 1.13933E+00\n",
      "Early stopping at epoch 153\n",
      "Best val loss: 1.13865E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.2_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.71177E-01  - Val Loss: 1.14156E+00\n",
      "Epoch 200/250  - Train Loss: 9.62386E-01  - Val Loss: 1.13731E+00\n",
      "Early stopping at epoch 242\n",
      "Best val loss: 1.13674E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.2_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.77567E-01  - Val Loss: 1.15194E+00\n",
      "Epoch 200/250  - Train Loss: 9.64200E-01  - Val Loss: 1.14163E+00\n",
      "Best val loss: 1.13857E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.2_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.74324E-01  - Val Loss: 1.13142E+00\n",
      "Early stopping at epoch 112\n",
      "Best val loss: 1.13094E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l21e-05_drop0.2_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =0e+00\n",
      "                            lambda_l2       =1e-04\n",
      "                            dropout         =5e-02\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 76\n",
      "Best val loss: 1.12835E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 85\n",
      "Best val loss: 1.14020E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.58905E-01  - Val Loss: 1.14072E+00\n",
      "Early stopping at epoch 159\n",
      "Best val loss: 1.13842E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.64257E-01  - Val Loss: 1.14849E+00\n",
      "Epoch 200/250  - Train Loss: 9.43644E-01  - Val Loss: 1.14275E+00\n",
      "Best val loss: 1.14058E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.65077E-01  - Val Loss: 1.12996E+00\n",
      "Early stopping at epoch 101\n",
      "Best val loss: 1.12903E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =0e+00\n",
      "                            lambda_l2       =1e-04\n",
      "                            dropout         =1e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 74\n",
      "Best val loss: 1.12838E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 85\n",
      "Best val loss: 1.13980E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.64521E-01  - Val Loss: 1.14079E+00\n",
      "Early stopping at epoch 181\n",
      "Best val loss: 1.13757E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.69267E-01  - Val Loss: 1.14886E+00\n",
      "Epoch 200/250  - Train Loss: 9.53145E-01  - Val Loss: 1.14169E+00\n",
      "Best val loss: 1.13968E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.69454E-01  - Val Loss: 1.13068E+00\n",
      "Early stopping at epoch 111\n",
      "Best val loss: 1.12994E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =0e+00\n",
      "                            lambda_l2       =1e-04\n",
      "                            dropout         =2e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 77\n",
      "Best val loss: 1.12877E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.74592E-01  - Val Loss: 1.13932E+00\n",
      "Early stopping at epoch 153\n",
      "Best val loss: 1.13866E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.72873E-01  - Val Loss: 1.14166E+00\n",
      "Epoch 200/250  - Train Loss: 9.64071E-01  - Val Loss: 1.13769E+00\n",
      "Early stopping at epoch 242\n",
      "Best val loss: 1.13712E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.79383E-01  - Val Loss: 1.15188E+00\n",
      "Epoch 200/250  - Train Loss: 9.66018E-01  - Val Loss: 1.14170E+00\n",
      "Best val loss: 1.13854E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.76069E-01  - Val Loss: 1.13145E+00\n",
      "Early stopping at epoch 112\n",
      "Best val loss: 1.13093E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.0001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =0e+00\n",
      "                            lambda_l2       =1e-03\n",
      "                            dropout         =5e-02\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 77\n",
      "Best val loss: 1.12824E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 85\n",
      "Best val loss: 1.13980E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.73427E-01  - Val Loss: 1.14186E+00\n",
      "Early stopping at epoch 160\n",
      "Best val loss: 1.13974E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.80165E-01  - Val Loss: 1.14869E+00\n",
      "Epoch 200/250  - Train Loss: 9.58597E-01  - Val Loss: 1.14283E+00\n",
      "Best val loss: 1.14066E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.78917E-01  - Val Loss: 1.13004E+00\n",
      "Early stopping at epoch 110\n",
      "Best val loss: 1.12897E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =0e+00\n",
      "                            lambda_l2       =1e-03\n",
      "                            dropout         =1e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 77\n",
      "Best val loss: 1.12842E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 99\n",
      "Best val loss: 1.13943E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.78661E-01  - Val Loss: 1.14173E+00\n",
      "Early stopping at epoch 181\n",
      "Best val loss: 1.13898E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.84819E-01  - Val Loss: 1.14925E+00\n",
      "Epoch 200/250  - Train Loss: 9.66786E-01  - Val Loss: 1.14196E+00\n",
      "Best val loss: 1.14061E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.83178E-01  - Val Loss: 1.13075E+00\n",
      "Early stopping at epoch 111\n",
      "Best val loss: 1.12988E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =0e+00\n",
      "                            lambda_l2       =1e-03\n",
      "                            dropout         =2e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 77\n",
      "Best val loss: 1.12892E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.88544E-01  - Val Loss: 1.13930E+00\n",
      "Early stopping at epoch 157\n",
      "Best val loss: 1.13848E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.86534E-01  - Val Loss: 1.14231E+00\n",
      "Early stopping at epoch 181\n",
      "Best val loss: 1.13914E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.94172E-01  - Val Loss: 1.15201E+00\n",
      "Epoch 200/250  - Train Loss: 9.78577E-01  - Val Loss: 1.14196E+00\n",
      "Best val loss: 1.13848E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.89643E-01  - Val Loss: 1.13146E+00\n",
      "Early stopping at epoch 111\n",
      "Best val loss: 1.13074E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =0e+00\n",
      "                            lambda_l2       =1e-02\n",
      "                            dropout         =5e-02\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.04179E+00  - Val Loss: 1.13114E+00\n",
      "Early stopping at epoch 110\n",
      "Best val loss: 1.13055E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.05_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.03944E+00  - Val Loss: 1.13933E+00\n",
      "Early stopping at epoch 106\n",
      "Best val loss: 1.13896E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.05_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.03505E+00  - Val Loss: 1.15048E+00\n",
      "Epoch 200/250  - Train Loss: 9.99317E-01  - Val Loss: 1.14669E+00\n",
      "Best val loss: 1.14539E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.05_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05509E+00  - Val Loss: 1.15226E+00\n",
      "Epoch 200/250  - Train Loss: 1.01064E+00  - Val Loss: 1.14502E+00\n",
      "Best val loss: 1.14124E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.05_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.04008E+00  - Val Loss: 1.12943E+00\n",
      "Early stopping at epoch 112\n",
      "Best val loss: 1.12859E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.05_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =0e+00\n",
      "                            lambda_l2       =1e-02\n",
      "                            dropout         =1e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.04470E+00  - Val Loss: 1.13135E+00\n",
      "Early stopping at epoch 110\n",
      "Best val loss: 1.13065E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.1_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.04435E+00  - Val Loss: 1.13915E+00\n",
      "Early stopping at epoch 138\n",
      "Best val loss: 1.13894E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.1_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.03790E+00  - Val Loss: 1.15064E+00\n",
      "Epoch 200/250  - Train Loss: 1.00047E+00  - Val Loss: 1.14607E+00\n",
      "Best val loss: 1.14457E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.1_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05836E+00  - Val Loss: 1.15306E+00\n",
      "Epoch 200/250  - Train Loss: 1.01344E+00  - Val Loss: 1.14517E+00\n",
      "Best val loss: 1.14200E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.1_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.04313E+00  - Val Loss: 1.12977E+00\n",
      "Early stopping at epoch 112\n",
      "Best val loss: 1.12928E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.1_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =0e+00\n",
      "                            lambda_l2       =1e-02\n",
      "                            dropout         =2e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.04978E+00  - Val Loss: 1.13243E+00\n",
      "Early stopping at epoch 105\n",
      "Best val loss: 1.13128E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.2_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05313E+00  - Val Loss: 1.13952E+00\n",
      "Early stopping at epoch 157\n",
      "Best val loss: 1.13921E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.2_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.04321E+00  - Val Loss: 1.15076E+00\n",
      "Epoch 200/250  - Train Loss: 1.00456E+00  - Val Loss: 1.14560E+00\n",
      "Best val loss: 1.14391E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.2_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06525E+00  - Val Loss: 1.15573E+00\n",
      "Epoch 200/250  - Train Loss: 1.01925E+00  - Val Loss: 1.14632E+00\n",
      "Best val loss: 1.14368E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.2_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.04932E+00  - Val Loss: 1.13130E+00\n",
      "Early stopping at epoch 123\n",
      "Best val loss: 1.13130E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0_l20.01_drop0.2_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-05\n",
      "                            lambda_l2       =0e+00\n",
      "                            dropout         =5e-02\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 76\n",
      "Best val loss: 1.12836E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.05_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 85\n",
      "Best val loss: 1.14018E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.05_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.60447E-01  - Val Loss: 1.14086E+00\n",
      "Early stopping at epoch 159\n",
      "Best val loss: 1.13850E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.05_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.65857E-01  - Val Loss: 1.14833E+00\n",
      "Epoch 200/250  - Train Loss: 9.45286E-01  - Val Loss: 1.14285E+00\n",
      "Early stopping at epoch 236\n",
      "Best val loss: 1.14148E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.05_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.66726E-01  - Val Loss: 1.13013E+00\n",
      "Early stopping at epoch 101\n",
      "Best val loss: 1.12907E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.05_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-05\n",
      "                            lambda_l2       =0e+00\n",
      "                            dropout         =1e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 74\n",
      "Best val loss: 1.12840E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.1_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 85\n",
      "Best val loss: 1.13978E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.1_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.66110E-01  - Val Loss: 1.14079E+00\n",
      "Epoch 200/250  - Train Loss: 9.52014E-01  - Val Loss: 1.13814E+00\n",
      "Early stopping at epoch 209\n",
      "Best val loss: 1.13723E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.1_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.70889E-01  - Val Loss: 1.14872E+00\n",
      "Epoch 200/250  - Train Loss: 9.54707E-01  - Val Loss: 1.14161E+00\n",
      "Best val loss: 1.13952E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.1_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.71066E-01  - Val Loss: 1.13074E+00\n",
      "Early stopping at epoch 111\n",
      "Best val loss: 1.12997E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.1_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-05\n",
      "                            lambda_l2       =0e+00\n",
      "                            dropout         =2e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 77\n",
      "Best val loss: 1.12874E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.2_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.76244E-01  - Val Loss: 1.13938E+00\n",
      "Early stopping at epoch 153\n",
      "Best val loss: 1.13871E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.2_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.74545E-01  - Val Loss: 1.14172E+00\n",
      "Epoch 200/250  - Train Loss: 9.65601E-01  - Val Loss: 1.13765E+00\n",
      "Early stopping at epoch 242\n",
      "Best val loss: 1.13706E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.2_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.81016E-01  - Val Loss: 1.15184E+00\n",
      "Epoch 200/250  - Train Loss: 9.67523E-01  - Val Loss: 1.14156E+00\n",
      "Best val loss: 1.13848E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.2_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.77723E-01  - Val Loss: 1.13144E+00\n",
      "Early stopping at epoch 112\n",
      "Best val loss: 1.13091E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0_drop0.2_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-05\n",
      "                            lambda_l2       =1e-05\n",
      "                            dropout         =5e-02\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 76\n",
      "Best val loss: 1.12838E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.05_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 85\n",
      "Best val loss: 1.14018E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.05_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.60659E-01  - Val Loss: 1.14082E+00\n",
      "Early stopping at epoch 182\n",
      "Best val loss: 1.13835E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.05_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.66066E-01  - Val Loss: 1.14841E+00\n",
      "Epoch 200/250  - Train Loss: 9.45476E-01  - Val Loss: 1.14277E+00\n",
      "Best val loss: 1.14053E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.05_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.66933E-01  - Val Loss: 1.13012E+00\n",
      "Early stopping at epoch 101\n",
      "Best val loss: 1.12907E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.05_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-05\n",
      "                            lambda_l2       =1e-05\n",
      "                            dropout         =1e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 74\n",
      "Best val loss: 1.12837E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.1_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 85\n",
      "Best val loss: 1.13980E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.1_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.66309E-01  - Val Loss: 1.14075E+00\n",
      "Early stopping at epoch 182\n",
      "Best val loss: 1.13755E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.1_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.71136E-01  - Val Loss: 1.14874E+00\n",
      "Epoch 200/250  - Train Loss: 9.54839E-01  - Val Loss: 1.14169E+00\n",
      "Best val loss: 1.13967E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.1_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.71244E-01  - Val Loss: 1.13076E+00\n",
      "Early stopping at epoch 111\n",
      "Best val loss: 1.13001E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.1_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-05\n",
      "                            lambda_l2       =1e-05\n",
      "                            dropout         =2e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 77\n",
      "Best val loss: 1.12874E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.2_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.76417E-01  - Val Loss: 1.13932E+00\n",
      "Early stopping at epoch 153\n",
      "Best val loss: 1.13866E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.2_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.74711E-01  - Val Loss: 1.14167E+00\n",
      "Epoch 200/250  - Train Loss: 9.65826E-01  - Val Loss: 1.13759E+00\n",
      "Best val loss: 1.13660E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.2_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.81215E-01  - Val Loss: 1.15186E+00\n",
      "Epoch 200/250  - Train Loss: 9.67689E-01  - Val Loss: 1.14161E+00\n",
      "Best val loss: 1.13853E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.2_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.77873E-01  - Val Loss: 1.13144E+00\n",
      "Early stopping at epoch 112\n",
      "Best val loss: 1.13091E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l21e-05_drop0.2_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-05\n",
      "                            lambda_l2       =1e-04\n",
      "                            dropout         =5e-02\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 77\n",
      "Best val loss: 1.12838E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 85\n",
      "Best val loss: 1.14016E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.62339E-01  - Val Loss: 1.14099E+00\n",
      "Early stopping at epoch 159\n",
      "Best val loss: 1.13865E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.67869E-01  - Val Loss: 1.14837E+00\n",
      "Epoch 200/250  - Train Loss: 9.47354E-01  - Val Loss: 1.14296E+00\n",
      "Best val loss: 1.14076E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.68571E-01  - Val Loss: 1.13002E+00\n",
      "Early stopping at epoch 101\n",
      "Best val loss: 1.12905E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-05\n",
      "                            lambda_l2       =1e-04\n",
      "                            dropout         =1e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 75\n",
      "Best val loss: 1.12840E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 85\n",
      "Best val loss: 1.13978E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.67938E-01  - Val Loss: 1.14090E+00\n",
      "Early stopping at epoch 181\n",
      "Best val loss: 1.13773E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.72885E-01  - Val Loss: 1.14878E+00\n",
      "Epoch 200/250  - Train Loss: 9.56666E-01  - Val Loss: 1.14156E+00\n",
      "Best val loss: 1.13961E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.72891E-01  - Val Loss: 1.13069E+00\n",
      "Early stopping at epoch 111\n",
      "Best val loss: 1.12993E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-05\n",
      "                            lambda_l2       =1e-04\n",
      "                            dropout         =2e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 77\n",
      "Best val loss: 1.12878E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.78026E-01  - Val Loss: 1.13933E+00\n",
      "Early stopping at epoch 153\n",
      "Best val loss: 1.13870E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.76300E-01  - Val Loss: 1.14176E+00\n",
      "Epoch 200/250  - Train Loss: 9.67213E-01  - Val Loss: 1.13771E+00\n",
      "Early stopping at epoch 242\n",
      "Best val loss: 1.13718E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.82882E-01  - Val Loss: 1.15184E+00\n",
      "Epoch 200/250  - Train Loss: 9.69314E-01  - Val Loss: 1.14168E+00\n",
      "Best val loss: 1.13858E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.79505E-01  - Val Loss: 1.13141E+00\n",
      "Early stopping at epoch 112\n",
      "Best val loss: 1.13087E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.0001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-05\n",
      "                            lambda_l2       =1e-03\n",
      "                            dropout         =5e-02\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 77\n",
      "Best val loss: 1.12826E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 86\n",
      "Best val loss: 1.13972E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.76156E-01  - Val Loss: 1.14210E+00\n",
      "Early stopping at epoch 160\n",
      "Best val loss: 1.14016E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.83141E-01  - Val Loss: 1.14862E+00\n",
      "Epoch 200/250  - Train Loss: 9.61199E-01  - Val Loss: 1.14258E+00\n",
      "Best val loss: 1.14053E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.81478E-01  - Val Loss: 1.13011E+00\n",
      "Early stopping at epoch 110\n",
      "Best val loss: 1.12904E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-05\n",
      "                            lambda_l2       =1e-03\n",
      "                            dropout         =1e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 77\n",
      "Best val loss: 1.12839E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.81489E-01  - Val Loss: 1.13961E+00\n",
      "Early stopping at epoch 157\n",
      "Best val loss: 1.13919E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.81352E-01  - Val Loss: 1.14202E+00\n",
      "Early stopping at epoch 181\n",
      "Best val loss: 1.13929E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.87760E-01  - Val Loss: 1.14926E+00\n",
      "Epoch 200/250  - Train Loss: 9.69319E-01  - Val Loss: 1.14210E+00\n",
      "Best val loss: 1.14084E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.85789E-01  - Val Loss: 1.13082E+00\n",
      "Early stopping at epoch 111\n",
      "Best val loss: 1.12993E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-05\n",
      "                            lambda_l2       =1e-03\n",
      "                            dropout         =2e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 77\n",
      "Best val loss: 1.12893E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.91353E-01  - Val Loss: 1.13927E+00\n",
      "Early stopping at epoch 157\n",
      "Best val loss: 1.13851E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.89207E-01  - Val Loss: 1.14250E+00\n",
      "Early stopping at epoch 181\n",
      "Best val loss: 1.13930E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.97083E-01  - Val Loss: 1.15200E+00\n",
      "Epoch 200/250  - Train Loss: 9.80955E-01  - Val Loss: 1.14194E+00\n",
      "Best val loss: 1.13849E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.92254E-01  - Val Loss: 1.13152E+00\n",
      "Early stopping at epoch 111\n",
      "Best val loss: 1.13076E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-05\n",
      "                            lambda_l2       =1e-02\n",
      "                            dropout         =5e-02\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.04291E+00  - Val Loss: 1.13127E+00\n",
      "Early stopping at epoch 110\n",
      "Best val loss: 1.13069E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.05_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.04066E+00  - Val Loss: 1.13937E+00\n",
      "Early stopping at epoch 106\n",
      "Best val loss: 1.13903E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.05_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.03614E+00  - Val Loss: 1.15059E+00\n",
      "Epoch 200/250  - Train Loss: 1.00001E+00  - Val Loss: 1.14661E+00\n",
      "Best val loss: 1.14528E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.05_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05643E+00  - Val Loss: 1.15229E+00\n",
      "Epoch 200/250  - Train Loss: 1.01162E+00  - Val Loss: 1.14507E+00\n",
      "Best val loss: 1.14114E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.05_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.04118E+00  - Val Loss: 1.12944E+00\n",
      "Early stopping at epoch 112\n",
      "Best val loss: 1.12866E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.05_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-05\n",
      "                            lambda_l2       =1e-02\n",
      "                            dropout         =1e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.04581E+00  - Val Loss: 1.13143E+00\n",
      "Early stopping at epoch 110\n",
      "Best val loss: 1.13077E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.1_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.04558E+00  - Val Loss: 1.13917E+00\n",
      "Early stopping at epoch 138\n",
      "Best val loss: 1.13895E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.1_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.03899E+00  - Val Loss: 1.15074E+00\n",
      "Epoch 200/250  - Train Loss: 1.00115E+00  - Val Loss: 1.14603E+00\n",
      "Best val loss: 1.14448E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.1_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05968E+00  - Val Loss: 1.15316E+00\n",
      "Epoch 200/250  - Train Loss: 1.01431E+00  - Val Loss: 1.14527E+00\n",
      "Best val loss: 1.14197E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.1_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.04423E+00  - Val Loss: 1.12976E+00\n",
      "Early stopping at epoch 112\n",
      "Best val loss: 1.12926E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.1_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-05\n",
      "                            lambda_l2       =1e-02\n",
      "                            dropout         =2e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05087E+00  - Val Loss: 1.13242E+00\n",
      "Early stopping at epoch 105\n",
      "Best val loss: 1.13135E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.2_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05433E+00  - Val Loss: 1.13954E+00\n",
      "Early stopping at epoch 157\n",
      "Best val loss: 1.13924E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.2_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.04427E+00  - Val Loss: 1.15077E+00\n",
      "Epoch 200/250  - Train Loss: 1.00517E+00  - Val Loss: 1.14565E+00\n",
      "Best val loss: 1.14386E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.2_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06657E+00  - Val Loss: 1.15570E+00\n",
      "Epoch 200/250  - Train Loss: 1.02003E+00  - Val Loss: 1.14638E+00\n",
      "Best val loss: 1.14375E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.2_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05043E+00  - Val Loss: 1.13134E+00\n",
      "Early stopping at epoch 123\n",
      "Best val loss: 1.13134E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l11e-05_l20.01_drop0.2_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-04\n",
      "                            lambda_l2       =0e+00\n",
      "                            dropout         =5e-02\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 77\n",
      "Best val loss: 1.12860E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.05_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 85\n",
      "Best val loss: 1.13994E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.05_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.85234E-01  - Val Loss: 1.14244E+00\n",
      "Early stopping at epoch 160\n",
      "Best val loss: 1.14048E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.05_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.93083E-01  - Val Loss: 1.14822E+00\n",
      "Epoch 200/250  - Train Loss: 9.69691E-01  - Val Loss: 1.14162E+00\n",
      "Best val loss: 1.13893E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.05_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.90374E-01  - Val Loss: 1.13021E+00\n",
      "Early stopping at epoch 111\n",
      "Best val loss: 1.12908E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.05_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-04\n",
      "                            lambda_l2       =0e+00\n",
      "                            dropout         =1e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 77\n",
      "Best val loss: 1.12863E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.1_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.91350E-01  - Val Loss: 1.13950E+00\n",
      "Early stopping at epoch 157\n",
      "Best val loss: 1.13886E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.1_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.90641E-01  - Val Loss: 1.14215E+00\n",
      "Early stopping at epoch 159\n",
      "Best val loss: 1.13936E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.1_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.97925E-01  - Val Loss: 1.14879E+00\n",
      "Epoch 200/250  - Train Loss: 9.77993E-01  - Val Loss: 1.14195E+00\n",
      "Best val loss: 1.14046E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.1_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.94752E-01  - Val Loss: 1.13082E+00\n",
      "Early stopping at epoch 110\n",
      "Best val loss: 1.12983E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.1_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-04\n",
      "                            lambda_l2       =0e+00\n",
      "                            dropout         =2e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 77\n",
      "Best val loss: 1.12906E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.2_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.00159E+00  - Val Loss: 1.13910E+00\n",
      "Early stopping at epoch 175\n",
      "Best val loss: 1.13832E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.2_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.98980E-01  - Val Loss: 1.14268E+00\n",
      "Epoch 200/250  - Train Loss: 9.85519E-01  - Val Loss: 1.13872E+00\n",
      "Best val loss: 1.13799E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.2_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.00762E+00  - Val Loss: 1.15151E+00\n",
      "Epoch 200/250  - Train Loss: 9.89874E-01  - Val Loss: 1.14161E+00\n",
      "Best val loss: 1.13875E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.2_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.00150E+00  - Val Loss: 1.13144E+00\n",
      "Early stopping at epoch 111\n",
      "Best val loss: 1.13079E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0_drop0.2_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-04\n",
      "                            lambda_l2       =1e-05\n",
      "                            dropout         =5e-02\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 77\n",
      "Best val loss: 1.12863E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.05_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.83506E-01  - Val Loss: 1.14125E+00\n",
      "Early stopping at epoch 106\n",
      "Best val loss: 1.13989E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.05_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.85348E-01  - Val Loss: 1.14253E+00\n",
      "Early stopping at epoch 160\n",
      "Best val loss: 1.14058E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.05_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.93189E-01  - Val Loss: 1.14823E+00\n",
      "Epoch 200/250  - Train Loss: 9.69870E-01  - Val Loss: 1.14161E+00\n",
      "Best val loss: 1.13892E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.05_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.90494E-01  - Val Loss: 1.13020E+00\n",
      "Early stopping at epoch 111\n",
      "Best val loss: 1.12911E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.05_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-04\n",
      "                            lambda_l2       =1e-05\n",
      "                            dropout         =1e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 77\n",
      "Best val loss: 1.12862E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.1_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.91491E-01  - Val Loss: 1.13950E+00\n",
      "Early stopping at epoch 157\n",
      "Best val loss: 1.13889E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.1_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.90779E-01  - Val Loss: 1.14224E+00\n",
      "Early stopping at epoch 159\n",
      "Best val loss: 1.13941E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.1_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.98035E-01  - Val Loss: 1.14881E+00\n",
      "Epoch 200/250  - Train Loss: 9.78070E-01  - Val Loss: 1.14197E+00\n",
      "Best val loss: 1.14052E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.1_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.94882E-01  - Val Loss: 1.13081E+00\n",
      "Early stopping at epoch 111\n",
      "Best val loss: 1.12982E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.1_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-04\n",
      "                            lambda_l2       =1e-05\n",
      "                            dropout         =2e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 77\n",
      "Best val loss: 1.12906E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.2_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.00172E+00  - Val Loss: 1.13905E+00\n",
      "Early stopping at epoch 175\n",
      "Best val loss: 1.13824E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.2_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.99124E-01  - Val Loss: 1.14267E+00\n",
      "Epoch 200/250  - Train Loss: 9.85580E-01  - Val Loss: 1.13866E+00\n",
      "Best val loss: 1.13786E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.2_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.00775E+00  - Val Loss: 1.15152E+00\n",
      "Epoch 200/250  - Train Loss: 9.89983E-01  - Val Loss: 1.14160E+00\n",
      "Best val loss: 1.13870E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.2_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.00162E+00  - Val Loss: 1.13145E+00\n",
      "Early stopping at epoch 111\n",
      "Best val loss: 1.13079E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l21e-05_drop0.2_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-04\n",
      "                            lambda_l2       =1e-04\n",
      "                            dropout         =5e-02\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 77\n",
      "Best val loss: 1.12860E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.84711E-01  - Val Loss: 1.14116E+00\n",
      "Early stopping at epoch 106\n",
      "Best val loss: 1.13990E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.86442E-01  - Val Loss: 1.14264E+00\n",
      "Early stopping at epoch 160\n",
      "Best val loss: 1.14060E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.94494E-01  - Val Loss: 1.14828E+00\n",
      "Epoch 200/250  - Train Loss: 9.70827E-01  - Val Loss: 1.14180E+00\n",
      "Best val loss: 1.13916E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.91442E-01  - Val Loss: 1.13021E+00\n",
      "Early stopping at epoch 111\n",
      "Best val loss: 1.12906E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-04\n",
      "                            lambda_l2       =1e-04\n",
      "                            dropout         =1e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 77\n",
      "Best val loss: 1.12863E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.92657E-01  - Val Loss: 1.13942E+00\n",
      "Early stopping at epoch 157\n",
      "Best val loss: 1.13886E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.91830E-01  - Val Loss: 1.14222E+00\n",
      "Early stopping at epoch 181\n",
      "Best val loss: 1.13950E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 9.99299E-01  - Val Loss: 1.14880E+00\n",
      "Epoch 200/250  - Train Loss: 9.78973E-01  - Val Loss: 1.14195E+00\n",
      "Best val loss: 1.14043E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.95850E-01  - Val Loss: 1.13081E+00\n",
      "Early stopping at epoch 111\n",
      "Best val loss: 1.12986E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-04\n",
      "                            lambda_l2       =1e-04\n",
      "                            dropout         =2e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 77\n",
      "Best val loss: 1.12907E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.00282E+00  - Val Loss: 1.13909E+00\n",
      "Early stopping at epoch 175\n",
      "Best val loss: 1.13828E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.00014E+00  - Val Loss: 1.14275E+00\n",
      "Epoch 200/250  - Train Loss: 9.86178E-01  - Val Loss: 1.13875E+00\n",
      "Best val loss: 1.13798E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.00896E+00  - Val Loss: 1.15155E+00\n",
      "Epoch 200/250  - Train Loss: 9.90766E-01  - Val Loss: 1.14169E+00\n",
      "Best val loss: 1.13886E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.00262E+00  - Val Loss: 1.13144E+00\n",
      "Early stopping at epoch 111\n",
      "Best val loss: 1.13074E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.0001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-04\n",
      "                            lambda_l2       =1e-03\n",
      "                            dropout         =5e-02\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 80\n",
      "Best val loss: 1.12856E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 9.95069E-01  - Val Loss: 1.14047E+00\n",
      "Early stopping at epoch 106\n",
      "Best val loss: 1.13940E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 9.95679E-01  - Val Loss: 1.14402E+00\n",
      "Early stopping at epoch 157\n",
      "Best val loss: 1.14231E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.00569E+00  - Val Loss: 1.14862E+00\n",
      "Epoch 200/250  - Train Loss: 9.78676E-01  - Val Loss: 1.14259E+00\n",
      "Best val loss: 1.13992E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 9.99426E-01  - Val Loss: 1.13044E+00\n",
      "Early stopping at epoch 110\n",
      "Best val loss: 1.12918E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-04\n",
      "                            lambda_l2       =1e-03\n",
      "                            dropout         =1e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 79\n",
      "Best val loss: 1.12861E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.00258E+00  - Val Loss: 1.13930E+00\n",
      "Early stopping at epoch 157\n",
      "Best val loss: 1.13863E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.00088E+00  - Val Loss: 1.14332E+00\n",
      "Early stopping at epoch 159\n",
      "Best val loss: 1.14104E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.01017E+00  - Val Loss: 1.14908E+00\n",
      "Epoch 200/250  - Train Loss: 9.85859E-01  - Val Loss: 1.14231E+00\n",
      "Best val loss: 1.14057E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.00395E+00  - Val Loss: 1.13103E+00\n",
      "Early stopping at epoch 111\n",
      "Best val loss: 1.13008E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-04\n",
      "                            lambda_l2       =1e-03\n",
      "                            dropout         =2e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 79\n",
      "Best val loss: 1.12919E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.01263E+00  - Val Loss: 1.13920E+00\n",
      "Early stopping at epoch 175\n",
      "Best val loss: 1.13823E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.00893E+00  - Val Loss: 1.14359E+00\n",
      "Early stopping at epoch 180\n",
      "Best val loss: 1.14036E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.01941E+00  - Val Loss: 1.15187E+00\n",
      "Epoch 200/250  - Train Loss: 9.96897E-01  - Val Loss: 1.14206E+00\n",
      "Best val loss: 1.13937E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.01096E+00  - Val Loss: 1.13155E+00\n",
      "Early stopping at epoch 111\n",
      "Best val loss: 1.13079E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-04\n",
      "                            lambda_l2       =1e-02\n",
      "                            dropout         =5e-02\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05198E+00  - Val Loss: 1.13223E+00\n",
      "Early stopping at epoch 118\n",
      "Best val loss: 1.13218E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.05_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05014E+00  - Val Loss: 1.13926E+00\n",
      "Early stopping at epoch 138\n",
      "Best val loss: 1.13907E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.05_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.04467E+00  - Val Loss: 1.15088E+00\n",
      "Epoch 200/250  - Train Loss: 1.00515E+00  - Val Loss: 1.14565E+00\n",
      "Best val loss: 1.14400E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.05_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06708E+00  - Val Loss: 1.15260E+00\n",
      "Epoch 200/250  - Train Loss: 1.01857E+00  - Val Loss: 1.14500E+00\n",
      "Best val loss: 1.14146E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.05_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.04990E+00  - Val Loss: 1.12971E+00\n",
      "Early stopping at epoch 112\n",
      "Best val loss: 1.12888E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.05_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-04\n",
      "                            lambda_l2       =1e-02\n",
      "                            dropout         =1e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05473E+00  - Val Loss: 1.13228E+00\n",
      "Early stopping at epoch 111\n",
      "Best val loss: 1.13209E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.1_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05524E+00  - Val Loss: 1.13927E+00\n",
      "Early stopping at epoch 152\n",
      "Best val loss: 1.13901E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.1_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.04748E+00  - Val Loss: 1.15116E+00\n",
      "Epoch 200/250  - Train Loss: 1.00609E+00  - Val Loss: 1.14519E+00\n",
      "Best val loss: 1.14351E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.1_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07032E+00  - Val Loss: 1.15333E+00\n",
      "Epoch 200/250  - Train Loss: 1.02104E+00  - Val Loss: 1.14531E+00\n",
      "Best val loss: 1.14206E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.1_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05287E+00  - Val Loss: 1.12976E+00\n",
      "Early stopping at epoch 112\n",
      "Best val loss: 1.12923E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.1_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-04\n",
      "                            lambda_l2       =1e-02\n",
      "                            dropout         =2e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05952E+00  - Val Loss: 1.13284E+00\n",
      "Early stopping at epoch 109\n",
      "Best val loss: 1.13207E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.2_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06421E+00  - Val Loss: 1.13969E+00\n",
      "Early stopping at epoch 157\n",
      "Best val loss: 1.13934E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.2_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05278E+00  - Val Loss: 1.15108E+00\n",
      "Epoch 200/250  - Train Loss: 1.00972E+00  - Val Loss: 1.14504E+00\n",
      "Best val loss: 1.14305E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.2_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07723E+00  - Val Loss: 1.15589E+00\n",
      "Epoch 200/250  - Train Loss: 1.02610E+00  - Val Loss: 1.14674E+00\n",
      "Best val loss: 1.14449E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.2_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05906E+00  - Val Loss: 1.13113E+00\n",
      "Early stopping at epoch 125\n",
      "Best val loss: 1.13113E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.01_drop0.2_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-03\n",
      "                            lambda_l2       =0e+00\n",
      "                            dropout         =5e-02\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05044E+00  - Val Loss: 1.13415E+00\n",
      "Early stopping at epoch 110\n",
      "Best val loss: 1.13388E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.05_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05670E+00  - Val Loss: 1.13753E+00\n",
      "Early stopping at epoch 152\n",
      "Best val loss: 1.13719E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.05_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.04579E+00  - Val Loss: 1.14917E+00\n",
      "Epoch 200/250  - Train Loss: 1.00610E+00  - Val Loss: 1.14255E+00\n",
      "Early stopping at epoch 240\n",
      "Best val loss: 1.14224E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.05_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07464E+00  - Val Loss: 1.15097E+00\n",
      "Epoch 200/250  - Train Loss: 1.02122E+00  - Val Loss: 1.13972E+00\n",
      "Best val loss: 1.13610E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.05_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.04972E+00  - Val Loss: 1.13169E+00\n",
      "Early stopping at epoch 103\n",
      "Best val loss: 1.13019E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.05_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-03\n",
      "                            lambda_l2       =0e+00\n",
      "                            dropout         =1e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05228E+00  - Val Loss: 1.13423E+00\n",
      "Early stopping at epoch 110\n",
      "Best val loss: 1.13385E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.1_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06305E+00  - Val Loss: 1.13781E+00\n",
      "Early stopping at epoch 157\n",
      "Best val loss: 1.13738E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.1_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.04961E+00  - Val Loss: 1.14932E+00\n",
      "Epoch 200/250  - Train Loss: 1.00813E+00  - Val Loss: 1.14209E+00\n",
      "Best val loss: 1.14078E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.1_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07875E+00  - Val Loss: 1.15202E+00\n",
      "Epoch 200/250  - Train Loss: 1.02378E+00  - Val Loss: 1.14143E+00\n",
      "Best val loss: 1.13762E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.1_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05281E+00  - Val Loss: 1.13163E+00\n",
      "Early stopping at epoch 107\n",
      "Best val loss: 1.13053E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.1_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-03\n",
      "                            lambda_l2       =0e+00\n",
      "                            dropout         =2e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05876E+00  - Val Loss: 1.13416E+00\n",
      "Early stopping at epoch 109\n",
      "Best val loss: 1.13384E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.2_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07612E+00  - Val Loss: 1.13874E+00\n",
      "Early stopping at epoch 174\n",
      "Best val loss: 1.13782E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.2_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05760E+00  - Val Loss: 1.14982E+00\n",
      "Epoch 200/250  - Train Loss: 1.01265E+00  - Val Loss: 1.14198E+00\n",
      "Best val loss: 1.14049E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.2_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.08825E+00  - Val Loss: 1.15398E+00\n",
      "Epoch 200/250  - Train Loss: 1.02888E+00  - Val Loss: 1.14407E+00\n",
      "Best val loss: 1.14075E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.2_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06027E+00  - Val Loss: 1.13173E+00\n",
      "Early stopping at epoch 112\n",
      "Best val loss: 1.13147E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0_drop0.2_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-03\n",
      "                            lambda_l2       =1e-05\n",
      "                            dropout         =5e-02\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05050E+00  - Val Loss: 1.13415E+00\n",
      "Early stopping at epoch 110\n",
      "Best val loss: 1.13389E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.05_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05673E+00  - Val Loss: 1.13752E+00\n",
      "Early stopping at epoch 152\n",
      "Best val loss: 1.13719E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.05_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.04582E+00  - Val Loss: 1.14919E+00\n",
      "Epoch 200/250  - Train Loss: 1.00612E+00  - Val Loss: 1.14257E+00\n",
      "Early stopping at epoch 240\n",
      "Best val loss: 1.14224E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.05_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07470E+00  - Val Loss: 1.15099E+00\n",
      "Epoch 200/250  - Train Loss: 1.02126E+00  - Val Loss: 1.13968E+00\n",
      "Best val loss: 1.13612E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.05_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.04977E+00  - Val Loss: 1.13169E+00\n",
      "Early stopping at epoch 103\n",
      "Best val loss: 1.13019E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.05_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-03\n",
      "                            lambda_l2       =1e-05\n",
      "                            dropout         =1e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05232E+00  - Val Loss: 1.13423E+00\n",
      "Early stopping at epoch 110\n",
      "Best val loss: 1.13386E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.1_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06309E+00  - Val Loss: 1.13780E+00\n",
      "Early stopping at epoch 156\n",
      "Best val loss: 1.13738E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.1_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.04962E+00  - Val Loss: 1.14933E+00\n",
      "Epoch 200/250  - Train Loss: 1.00814E+00  - Val Loss: 1.14208E+00\n",
      "Best val loss: 1.14077E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.1_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07880E+00  - Val Loss: 1.15202E+00\n",
      "Epoch 200/250  - Train Loss: 1.02381E+00  - Val Loss: 1.14143E+00\n",
      "Best val loss: 1.13762E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.1_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05286E+00  - Val Loss: 1.13163E+00\n",
      "Early stopping at epoch 107\n",
      "Best val loss: 1.13054E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.1_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-03\n",
      "                            lambda_l2       =1e-05\n",
      "                            dropout         =2e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05880E+00  - Val Loss: 1.13415E+00\n",
      "Early stopping at epoch 109\n",
      "Best val loss: 1.13384E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.2_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07616E+00  - Val Loss: 1.13874E+00\n",
      "Early stopping at epoch 174\n",
      "Best val loss: 1.13784E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.2_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05764E+00  - Val Loss: 1.14985E+00\n",
      "Epoch 200/250  - Train Loss: 1.01267E+00  - Val Loss: 1.14199E+00\n",
      "Best val loss: 1.14050E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.2_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.08829E+00  - Val Loss: 1.15395E+00\n",
      "Epoch 200/250  - Train Loss: 1.02892E+00  - Val Loss: 1.14407E+00\n",
      "Best val loss: 1.14074E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.2_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06032E+00  - Val Loss: 1.13174E+00\n",
      "Early stopping at epoch 112\n",
      "Best val loss: 1.13147E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l21e-05_drop0.2_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-03\n",
      "                            lambda_l2       =1e-04\n",
      "                            dropout         =5e-02\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05109E+00  - Val Loss: 1.13415E+00\n",
      "Early stopping at epoch 110\n",
      "Best val loss: 1.13394E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05706E+00  - Val Loss: 1.13755E+00\n",
      "Early stopping at epoch 152\n",
      "Best val loss: 1.13724E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.04613E+00  - Val Loss: 1.14927E+00\n",
      "Epoch 200/250  - Train Loss: 1.00631E+00  - Val Loss: 1.14251E+00\n",
      "Early stopping at epoch 240\n",
      "Best val loss: 1.14215E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07513E+00  - Val Loss: 1.15103E+00\n",
      "Epoch 200/250  - Train Loss: 1.02162E+00  - Val Loss: 1.13972E+00\n",
      "Best val loss: 1.13603E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05023E+00  - Val Loss: 1.13171E+00\n",
      "Early stopping at epoch 103\n",
      "Best val loss: 1.13021E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-03\n",
      "                            lambda_l2       =1e-04\n",
      "                            dropout         =1e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05287E+00  - Val Loss: 1.13422E+00\n",
      "Early stopping at epoch 110\n",
      "Best val loss: 1.13389E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06331E+00  - Val Loss: 1.13786E+00\n",
      "Early stopping at epoch 157\n",
      "Best val loss: 1.13742E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.04994E+00  - Val Loss: 1.14940E+00\n",
      "Epoch 200/250  - Train Loss: 1.00831E+00  - Val Loss: 1.14201E+00\n",
      "Best val loss: 1.14068E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07920E+00  - Val Loss: 1.15205E+00\n",
      "Epoch 200/250  - Train Loss: 1.02415E+00  - Val Loss: 1.14146E+00\n",
      "Best val loss: 1.13764E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05328E+00  - Val Loss: 1.13164E+00\n",
      "Early stopping at epoch 107\n",
      "Best val loss: 1.13056E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-03\n",
      "                            lambda_l2       =1e-04\n",
      "                            dropout         =2e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05918E+00  - Val Loss: 1.13413E+00\n",
      "Early stopping at epoch 110\n",
      "Best val loss: 1.13385E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07638E+00  - Val Loss: 1.13877E+00\n",
      "Early stopping at epoch 174\n",
      "Best val loss: 1.13789E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05791E+00  - Val Loss: 1.14989E+00\n",
      "Epoch 200/250  - Train Loss: 1.01279E+00  - Val Loss: 1.14195E+00\n",
      "Best val loss: 1.14048E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.08869E+00  - Val Loss: 1.15404E+00\n",
      "Epoch 200/250  - Train Loss: 1.02923E+00  - Val Loss: 1.14409E+00\n",
      "Best val loss: 1.14081E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06069E+00  - Val Loss: 1.13174E+00\n",
      "Early stopping at epoch 112\n",
      "Best val loss: 1.13146E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.0001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-03\n",
      "                            lambda_l2       =1e-03\n",
      "                            dropout         =5e-02\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05620E+00  - Val Loss: 1.13432E+00\n",
      "Early stopping at epoch 113\n",
      "Best val loss: 1.13425E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06024E+00  - Val Loss: 1.13786E+00\n",
      "Early stopping at epoch 157\n",
      "Best val loss: 1.13747E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.04930E+00  - Val Loss: 1.14976E+00\n",
      "Epoch 200/250  - Train Loss: 1.00834E+00  - Val Loss: 1.14201E+00\n",
      "Best val loss: 1.14101E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07938E+00  - Val Loss: 1.15155E+00\n",
      "Epoch 200/250  - Train Loss: 1.02506E+00  - Val Loss: 1.14000E+00\n",
      "Best val loss: 1.13648E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05475E+00  - Val Loss: 1.13188E+00\n",
      "Early stopping at epoch 103\n",
      "Best val loss: 1.13045E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-03\n",
      "                            lambda_l2       =1e-03\n",
      "                            dropout         =1e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05839E+00  - Val Loss: 1.13432E+00\n",
      "Early stopping at epoch 113\n",
      "Best val loss: 1.13421E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06625E+00  - Val Loss: 1.13816E+00\n",
      "Early stopping at epoch 157\n",
      "Best val loss: 1.13777E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05284E+00  - Val Loss: 1.15008E+00\n",
      "Epoch 200/250  - Train Loss: 1.01009E+00  - Val Loss: 1.14174E+00\n",
      "Best val loss: 1.14030E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.08331E+00  - Val Loss: 1.15255E+00\n",
      "Epoch 200/250  - Train Loss: 1.02745E+00  - Val Loss: 1.14175E+00\n",
      "Best val loss: 1.13797E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.05766E+00  - Val Loss: 1.13179E+00\n",
      "Early stopping at epoch 107\n",
      "Best val loss: 1.13075E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-03\n",
      "                            lambda_l2       =1e-03\n",
      "                            dropout         =2e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06348E+00  - Val Loss: 1.13408E+00\n",
      "Early stopping at epoch 115\n",
      "Best val loss: 1.13394E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.07907E+00  - Val Loss: 1.13904E+00\n",
      "Early stopping at epoch 174\n",
      "Best val loss: 1.13836E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06040E+00  - Val Loss: 1.15046E+00\n",
      "Epoch 200/250  - Train Loss: 1.01409E+00  - Val Loss: 1.14190E+00\n",
      "Best val loss: 1.14046E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.09232E+00  - Val Loss: 1.15453E+00\n",
      "Epoch 200/250  - Train Loss: 1.03219E+00  - Val Loss: 1.14450E+00\n",
      "Best val loss: 1.14134E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.06455E+00  - Val Loss: 1.13182E+00\n",
      "Early stopping at epoch 112\n",
      "Best val loss: 1.13150E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-03\n",
      "                            lambda_l2       =1e-02\n",
      "                            dropout         =5e-02\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.10512E+00  - Val Loss: 1.13475E+00\n",
      "Early stopping at epoch 135\n",
      "Best val loss: 1.13468E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.05_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.09970E+00  - Val Loss: 1.14009E+00\n",
      "Early stopping at epoch 177\n",
      "Best val loss: 1.13974E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.05_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.08910E+00  - Val Loss: 1.15217E+00\n",
      "Epoch 200/250  - Train Loss: 1.03150E+00  - Val Loss: 1.14301E+00\n",
      "Best val loss: 1.14141E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.05_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.12464E+00  - Val Loss: 1.15650E+00\n",
      "Epoch 200/250  - Train Loss: 1.05430E+00  - Val Loss: 1.14663E+00\n",
      "Best val loss: 1.14339E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.05_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.10063E+00  - Val Loss: 1.13261E+00\n",
      "Early stopping at epoch 122\n",
      "Best val loss: 1.13256E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.05_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-03\n",
      "                            lambda_l2       =1e-02\n",
      "                            dropout         =1e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.10693E+00  - Val Loss: 1.13481E+00\n",
      "Early stopping at epoch 132\n",
      "Best val loss: 1.13478E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.1_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.10388E+00  - Val Loss: 1.14061E+00\n",
      "Early stopping at epoch 177\n",
      "Best val loss: 1.14029E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.1_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.09115E+00  - Val Loss: 1.15251E+00\n",
      "Epoch 200/250  - Train Loss: 1.03256E+00  - Val Loss: 1.14354E+00\n",
      "Best val loss: 1.14199E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.1_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.12707E+00  - Val Loss: 1.15760E+00\n",
      "Epoch 200/250  - Train Loss: 1.05526E+00  - Val Loss: 1.14773E+00\n",
      "Best val loss: 1.14525E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.1_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.10240E+00  - Val Loss: 1.13279E+00\n",
      "Early stopping at epoch 119\n",
      "Best val loss: 1.13274E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.1_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-03\n",
      "                            lambda_l2       =1e-02\n",
      "                            dropout         =2e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.10935E+00  - Val Loss: 1.13491E+00\n",
      "Early stopping at epoch 129\n",
      "Best val loss: 1.13487E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.2_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Epoch 100/250  - Train Loss: 1.11277E+00  - Val Loss: 1.14164E+00\n",
      "Early stopping at epoch 177\n",
      "Best val loss: 1.14138E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.2_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.09555E+00  - Val Loss: 1.15313E+00\n",
      "Epoch 200/250  - Train Loss: 1.03451E+00  - Val Loss: 1.14449E+00\n",
      "Best val loss: 1.14307E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.2_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.13294E+00  - Val Loss: 1.15985E+00\n",
      "Epoch 200/250  - Train Loss: 1.05750E+00  - Val Loss: 1.14985E+00\n",
      "Best val loss: 1.14806E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.2_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.10687E+00  - Val Loss: 1.13326E+00\n",
      "Early stopping at epoch 119\n",
      "Best val loss: 1.13321E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.001_l20.01_drop0.2_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-02\n",
      "                            lambda_l2       =0e+00\n",
      "                            dropout         =5e-02\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.39662E+00  - Val Loss: 1.13546E+00\n",
      "Early stopping at epoch 150\n",
      "Best val loss: 1.13501E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.05_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 58\n",
      "Best val loss: 1.14710E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.05_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.36853E+00  - Val Loss: 1.15974E+00\n",
      "Epoch 200/250  - Train Loss: 1.16288E+00  - Val Loss: 1.15267E+00\n",
      "Best val loss: 1.15174E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.05_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.42120E+00  - Val Loss: 1.19028E+00\n",
      "Epoch 200/250  - Train Loss: 1.19981E+00  - Val Loss: 1.16426E+00\n",
      "Best val loss: 1.16042E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.05_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.40621E+00  - Val Loss: 1.13559E+00\n",
      "Early stopping at epoch 148\n",
      "Best val loss: 1.13501E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.05_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-02\n",
      "                            lambda_l2       =0e+00\n",
      "                            dropout         =1e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.39675E+00  - Val Loss: 1.13546E+00\n",
      "Early stopping at epoch 150\n",
      "Best val loss: 1.13501E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.1_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 58\n",
      "Best val loss: 1.14686E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.1_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.36877E+00  - Val Loss: 1.15972E+00\n",
      "Epoch 200/250  - Train Loss: 1.16300E+00  - Val Loss: 1.15268E+00\n",
      "Best val loss: 1.15174E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.1_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.42142E+00  - Val Loss: 1.19024E+00\n",
      "Epoch 200/250  - Train Loss: 1.19991E+00  - Val Loss: 1.16428E+00\n",
      "Best val loss: 1.16042E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.1_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.40629E+00  - Val Loss: 1.13562E+00\n",
      "Early stopping at epoch 149\n",
      "Best val loss: 1.13501E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.1_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-02\n",
      "                            lambda_l2       =0e+00\n",
      "                            dropout         =2e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.39686E+00  - Val Loss: 1.13545E+00\n",
      "Early stopping at epoch 149\n",
      "Best val loss: 1.13501E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.2_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 58\n",
      "Best val loss: 1.14633E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.2_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.36953E+00  - Val Loss: 1.15972E+00\n",
      "Epoch 200/250  - Train Loss: 1.16344E+00  - Val Loss: 1.15271E+00\n",
      "Best val loss: 1.15175E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.2_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.42209E+00  - Val Loss: 1.19028E+00\n",
      "Epoch 200/250  - Train Loss: 1.20035E+00  - Val Loss: 1.16446E+00\n",
      "Best val loss: 1.16045E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.2_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.40652E+00  - Val Loss: 1.13569E+00\n",
      "Early stopping at epoch 151\n",
      "Best val loss: 1.13501E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0_drop0.2_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-02\n",
      "                            lambda_l2       =1e-05\n",
      "                            dropout         =5e-02\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.39666E+00  - Val Loss: 1.13546E+00\n",
      "Early stopping at epoch 150\n",
      "Best val loss: 1.13501E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.05_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 58\n",
      "Best val loss: 1.14710E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.05_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.36858E+00  - Val Loss: 1.15974E+00\n",
      "Epoch 200/250  - Train Loss: 1.16290E+00  - Val Loss: 1.15267E+00\n",
      "Best val loss: 1.15174E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.05_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.42125E+00  - Val Loss: 1.19028E+00\n",
      "Epoch 200/250  - Train Loss: 1.19983E+00  - Val Loss: 1.16427E+00\n",
      "Best val loss: 1.16042E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.05_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.40625E+00  - Val Loss: 1.13559E+00\n",
      "Early stopping at epoch 148\n",
      "Best val loss: 1.13501E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.05_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-02\n",
      "                            lambda_l2       =1e-05\n",
      "                            dropout         =1e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.39680E+00  - Val Loss: 1.13546E+00\n",
      "Early stopping at epoch 150\n",
      "Best val loss: 1.13501E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.1_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 58\n",
      "Best val loss: 1.14686E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.1_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.36882E+00  - Val Loss: 1.15972E+00\n",
      "Epoch 200/250  - Train Loss: 1.16302E+00  - Val Loss: 1.15268E+00\n",
      "Best val loss: 1.15174E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.1_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.42147E+00  - Val Loss: 1.19024E+00\n",
      "Epoch 200/250  - Train Loss: 1.19993E+00  - Val Loss: 1.16429E+00\n",
      "Best val loss: 1.16043E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.1_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.40634E+00  - Val Loss: 1.13562E+00\n",
      "Early stopping at epoch 149\n",
      "Best val loss: 1.13501E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.1_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-02\n",
      "                            lambda_l2       =1e-05\n",
      "                            dropout         =2e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.39691E+00  - Val Loss: 1.13545E+00\n",
      "Early stopping at epoch 149\n",
      "Best val loss: 1.13501E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.2_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 58\n",
      "Best val loss: 1.14633E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.2_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.36959E+00  - Val Loss: 1.15972E+00\n",
      "Epoch 200/250  - Train Loss: 1.16346E+00  - Val Loss: 1.15272E+00\n",
      "Best val loss: 1.15175E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.2_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.42214E+00  - Val Loss: 1.19029E+00\n",
      "Epoch 200/250  - Train Loss: 1.20038E+00  - Val Loss: 1.16447E+00\n",
      "Best val loss: 1.16045E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.2_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.40656E+00  - Val Loss: 1.13569E+00\n",
      "Early stopping at epoch 151\n",
      "Best val loss: 1.13501E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l21e-05_drop0.2_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-02\n",
      "                            lambda_l2       =1e-04\n",
      "                            dropout         =5e-02\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.39709E+00  - Val Loss: 1.13546E+00\n",
      "Early stopping at epoch 150\n",
      "Best val loss: 1.13501E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 58\n",
      "Best val loss: 1.14710E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.36901E+00  - Val Loss: 1.15974E+00\n",
      "Epoch 200/250  - Train Loss: 1.16312E+00  - Val Loss: 1.15267E+00\n",
      "Best val loss: 1.15174E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.42171E+00  - Val Loss: 1.19034E+00\n",
      "Epoch 200/250  - Train Loss: 1.20005E+00  - Val Loss: 1.16430E+00\n",
      "Best val loss: 1.16045E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.40669E+00  - Val Loss: 1.13559E+00\n",
      "Early stopping at epoch 149\n",
      "Best val loss: 1.13501E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-02\n",
      "                            lambda_l2       =1e-04\n",
      "                            dropout         =1e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.39723E+00  - Val Loss: 1.13546E+00\n",
      "Early stopping at epoch 150\n",
      "Best val loss: 1.13501E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 58\n",
      "Best val loss: 1.14686E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.36924E+00  - Val Loss: 1.15972E+00\n",
      "Epoch 200/250  - Train Loss: 1.16325E+00  - Val Loss: 1.15268E+00\n",
      "Best val loss: 1.15174E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.42193E+00  - Val Loss: 1.19029E+00\n",
      "Epoch 200/250  - Train Loss: 1.20015E+00  - Val Loss: 1.16432E+00\n",
      "Best val loss: 1.16045E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.40678E+00  - Val Loss: 1.13562E+00\n",
      "Early stopping at epoch 149\n",
      "Best val loss: 1.13501E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-02\n",
      "                            lambda_l2       =1e-04\n",
      "                            dropout         =2e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.39734E+00  - Val Loss: 1.13545E+00\n",
      "Early stopping at epoch 149\n",
      "Best val loss: 1.13501E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 58\n",
      "Best val loss: 1.14633E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.37001E+00  - Val Loss: 1.15972E+00\n",
      "Epoch 200/250  - Train Loss: 1.16368E+00  - Val Loss: 1.15271E+00\n",
      "Best val loss: 1.15175E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.42259E+00  - Val Loss: 1.19034E+00\n",
      "Epoch 200/250  - Train Loss: 1.20059E+00  - Val Loss: 1.16450E+00\n",
      "Best val loss: 1.16048E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.40701E+00  - Val Loss: 1.13570E+00\n",
      "Early stopping at epoch 152\n",
      "Best val loss: 1.13501E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.0001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-02\n",
      "                            lambda_l2       =1e-03\n",
      "                            dropout         =5e-02\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.40142E+00  - Val Loss: 1.13546E+00\n",
      "Early stopping at epoch 150\n",
      "Best val loss: 1.13501E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 58\n",
      "Best val loss: 1.14706E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.37334E+00  - Val Loss: 1.15973E+00\n",
      "Epoch 200/250  - Train Loss: 1.16530E+00  - Val Loss: 1.15266E+00\n",
      "Best val loss: 1.15172E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.42625E+00  - Val Loss: 1.19086E+00\n",
      "Epoch 200/250  - Train Loss: 1.20220E+00  - Val Loss: 1.16466E+00\n",
      "Best val loss: 1.16069E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.41113E+00  - Val Loss: 1.13560E+00\n",
      "Early stopping at epoch 149\n",
      "Best val loss: 1.13501E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.05_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-02\n",
      "                            lambda_l2       =1e-03\n",
      "                            dropout         =1e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.40155E+00  - Val Loss: 1.13546E+00\n",
      "Early stopping at epoch 150\n",
      "Best val loss: 1.13501E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 58\n",
      "Best val loss: 1.14683E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.37358E+00  - Val Loss: 1.15971E+00\n",
      "Epoch 200/250  - Train Loss: 1.16543E+00  - Val Loss: 1.15266E+00\n",
      "Best val loss: 1.15172E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.42646E+00  - Val Loss: 1.19081E+00\n",
      "Epoch 200/250  - Train Loss: 1.20231E+00  - Val Loss: 1.16467E+00\n",
      "Best val loss: 1.16069E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.41122E+00  - Val Loss: 1.13564E+00\n",
      "Early stopping at epoch 150\n",
      "Best val loss: 1.13501E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.1_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-02\n",
      "                            lambda_l2       =1e-03\n",
      "                            dropout         =2e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.40165E+00  - Val Loss: 1.13545E+00\n",
      "Early stopping at epoch 150\n",
      "Best val loss: 1.13501E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 58\n",
      "Best val loss: 1.14631E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.37435E+00  - Val Loss: 1.15971E+00\n",
      "Epoch 200/250  - Train Loss: 1.16585E+00  - Val Loss: 1.15270E+00\n",
      "Best val loss: 1.15174E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.42711E+00  - Val Loss: 1.19083E+00\n",
      "Epoch 200/250  - Train Loss: 1.20274E+00  - Val Loss: 1.16483E+00\n",
      "Best val loss: 1.16070E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.41144E+00  - Val Loss: 1.13571E+00\n",
      "Early stopping at epoch 151\n",
      "Best val loss: 1.13501E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.001_drop0.2_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-02\n",
      "                            lambda_l2       =1e-02\n",
      "                            dropout         =5e-02\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.44518E+00  - Val Loss: 1.13552E+00\n",
      "Early stopping at epoch 154\n",
      "Best val loss: 1.13501E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.05_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 62\n",
      "Best val loss: 1.14675E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.05_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.41732E+00  - Val Loss: 1.15954E+00\n",
      "Epoch 200/250  - Train Loss: 1.18647E+00  - Val Loss: 1.15253E+00\n",
      "Best val loss: 1.15164E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.05_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.47197E+00  - Val Loss: 1.19547E+00\n",
      "Epoch 200/250  - Train Loss: 1.22367E+00  - Val Loss: 1.16753E+00\n",
      "Best val loss: 1.16292E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.05_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.45575E+00  - Val Loss: 1.13573E+00\n",
      "Early stopping at epoch 153\n",
      "Best val loss: 1.13501E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.05_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-02\n",
      "                            lambda_l2       =1e-02\n",
      "                            dropout         =1e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.44528E+00  - Val Loss: 1.13552E+00\n",
      "Early stopping at epoch 153\n",
      "Best val loss: 1.13501E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.1_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 62\n",
      "Best val loss: 1.14659E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.1_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.41756E+00  - Val Loss: 1.15951E+00\n",
      "Epoch 200/250  - Train Loss: 1.18659E+00  - Val Loss: 1.15253E+00\n",
      "Best val loss: 1.15164E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.1_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.47223E+00  - Val Loss: 1.19531E+00\n",
      "Epoch 200/250  - Train Loss: 1.22381E+00  - Val Loss: 1.16749E+00\n",
      "Best val loss: 1.16291E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.1_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.45580E+00  - Val Loss: 1.13574E+00\n",
      "Early stopping at epoch 153\n",
      "Best val loss: 1.13501E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.1_lr5e-05_w[32, 16, 8]_d3_run5.pth\n",
      "Training model for year '21...: \n",
      "                            lambda_l1       =1e-02\n",
      "                            lambda_l2       =1e-02\n",
      "                            dropout         =2e-01\n",
      "                            learning_rate   =5e-05\n",
      "                            hidden_depth    =3\n",
      "                            hidden_width    =[32, 16, 8]\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.44529E+00  - Val Loss: 1.13551E+00\n",
      "Early stopping at epoch 153\n",
      "Best val loss: 1.13501E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.2_lr5e-05_w[32, 16, 8]_d3_run1.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 62\n",
      "Best val loss: 1.14617E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.2_lr5e-05_w[32, 16, 8]_d3_run2.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.41829E+00  - Val Loss: 1.15950E+00\n",
      "Epoch 200/250  - Train Loss: 1.18697E+00  - Val Loss: 1.15254E+00\n",
      "Best val loss: 1.15165E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.2_lr5e-05_w[32, 16, 8]_d3_run3.pth\n",
      "Run 4 of 5\n",
      "Epoch 100/250  - Train Loss: 1.47300E+00  - Val Loss: 1.19504E+00\n",
      "Epoch 200/250  - Train Loss: 1.22430E+00  - Val Loss: 1.16740E+00\n",
      "Best val loss: 1.16285E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.2_lr5e-05_w[32, 16, 8]_d3_run4.pth\n",
      "Run 5 of 5\n",
      "Epoch 100/250  - Train Loss: 1.45592E+00  - Val Loss: 1.13580E+00\n",
      "Early stopping at epoch 155\n",
      "Best val loss: 1.13501E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.01_l20.01_drop0.2_lr5e-05_w[32, 16, 8]_d3_run5.pth\n"
     ]
    }
   ],
   "source": [
    "# current runtime: 15h\n",
    "for lambda_l1 in l1_space:\n",
    "    for lambda_l2 in l2_space:\n",
    "        for dropout in dropout_space:\n",
    "            for learning_rate in learning_rate_space:\n",
    "                for hidden_width in width_space:\n",
    "                    hidden_depth = len(hidden_width)\n",
    "                    print(f\"\"\"Training model for year '{year}...: \n",
    "                            lambda_l1       ={lambda_l1:.0e}\n",
    "                            lambda_l2       ={lambda_l2:.0e}\n",
    "                            dropout         ={dropout:.0e}\n",
    "                            learning_rate   ={learning_rate:.0e}\n",
    "                            hidden_depth    ={hidden_depth}\n",
    "                            hidden_width    ={hidden_width}\"\"\")\n",
    "                    for run in range(n_runs):\n",
    "                        print(f\"Run {run+1} of {n_runs}\")\n",
    "                        seed = 42+run\n",
    "                        np.random.seed(seed)\n",
    "                        torch.manual_seed(seed)\n",
    "                        # Initialize the model\n",
    "                        input_dim = X_train[year].shape[1]\n",
    "                        name = f'l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}'\n",
    "                        models_21[name] = MLPModel(input_dim, depth=hidden_depth, width=hidden_width, dropout=dropout, activation=activation_fun).to(device)\n",
    "                        optimizer = torch.optim.Adam(models_21[name].parameters(), lr=learning_rate)\n",
    "                        train = MLPdataset(X_train[year], y_train[year])\n",
    "                        val = MLPdataset(X_val[year], y_val[year])\n",
    "                        best_models_reg_pyramid[name], history_reg_pyramid[name] = train_mlp(train,          \n",
    "                                                        val,\n",
    "                                                        models_21[name],\n",
    "                                                        criterion,\n",
    "                                                        epochs,\n",
    "                                                        patience,\n",
    "                                                        print_freq,\n",
    "                                                        device,\n",
    "                                                        optimizer,\n",
    "                                                        lambda_l1=lambda_l1,\n",
    "                                                        lambda_l2=lambda_l2,\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        shuffle_train=True,\n",
    "                                                        shuffle_val=False,\n",
    "                                                        save_path=f'models/hyperparam_test/mlp_y{year}_l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}.pth'\n",
    "                                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_history(history_reg_pyramid, \n",
    "                save_csv='models/hyperparam_test/history/history_reg_pyramid.csv')\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model with most data on multiple parameters\n",
    "l1_space = [0.0, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "l2_space = [0.0, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "dropout_space = [0.0, 0.05 , 0.1, 0.2] \n",
    "learning_rate_space = [learning_rate]\n",
    "depth_space = None\n",
    "width_space = [[32, 16, 8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "val_ds     = MLPdataset(X_val[year], y_val[year])\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# build (n_drop, n_l1, n_l2) mean-loss array\n",
    "n_l1   = len(l1_space)\n",
    "n_l2   = len(l2_space)\n",
    "n_drop = len(dropout_space)\n",
    "\n",
    "losses = np.zeros((n_drop, n_l1, n_l2), dtype=float)\n",
    "\n",
    "for di, drop in enumerate(dropout_space):\n",
    "    for i, l1 in enumerate(l1_space):\n",
    "        for j, l2 in enumerate(l2_space):\n",
    "            run_losses = []\n",
    "            # for each seed, load & eval\n",
    "            for run in range(n_runs):\n",
    "                # since depth_space & width_space each have one entry, \n",
    "                # we can just index 0 here — but this will generalize\n",
    "                tmp_losses = []\n",
    "                for w in width_space:\n",
    "                    d = len(w)  # since w is a list, we can use its length\n",
    "                    m = load_model(w, d, run, l1, l2, drop, learning_rate, )\n",
    "                    with torch.no_grad():\n",
    "                        batch = [criterion(m(x.to(device)), y.to(device)).item()\n",
    "                                    for x,y in val_loader]\n",
    "                    tmp_losses.append(np.mean(batch))\n",
    "                run_losses.append(np.mean(tmp_losses))\n",
    "            # now average over runs\n",
    "            losses[di, i, j] = np.mean(run_losses)\n",
    "\n",
    "\n",
    "# global color‐scale\n",
    "vmin    = losses.min()\n",
    "vmax    = losses.max()\n",
    "vcenter = vmin + 0.5*(vmax - vmin)\n",
    "norm    = TwoSlopeNorm(vmin=vmin, vcenter=vcenter, vmax=vmax)\n",
    "cmap    = 'viridis'\n",
    "\n",
    "# layout: up to 2 columns\n",
    "ncols = min(2, n_drop)\n",
    "nrows = math.ceil(n_drop / ncols)\n",
    "\n",
    "fig, axes = plt.subplots(nrows, ncols,\n",
    "                         figsize=(5*ncols, 4*nrows),\n",
    "                         squeeze=False)\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for idx, drop in enumerate(dropout_space):\n",
    "    ax = axes_flat[idx]\n",
    "    im = ax.imshow(\n",
    "        losses[idx],\n",
    "        aspect='auto',\n",
    "        cmap=cmap,\n",
    "        norm=norm,\n",
    "        origin='lower',\n",
    "    )\n",
    "\n",
    "    # y = l1, x = l2\n",
    "    ax.set_yticks(np.arange(n_l1))\n",
    "    ax.set_yticklabels(l1_space)\n",
    "    ax.set_xticks(np.arange(n_l2))\n",
    "    ax.set_xticklabels(l2_space)\n",
    "\n",
    "    # only bottom row shows x‐labels\n",
    "    if idx < (nrows-1)*ncols:\n",
    "        ax.tick_params(labelbottom=False)\n",
    "    else:\n",
    "        ax.set_xlabel(r'$\\ell_2$')\n",
    "\n",
    "    # only first‐column shows y‐labels\n",
    "    if idx % ncols != 0:\n",
    "        ax.tick_params(labelleft=False)\n",
    "    else:\n",
    "        ax.set_ylabel(r'$\\ell_1$')\n",
    "\n",
    "    ax.set_title(f\"dropout = {drop:.2f}\")\n",
    "\n",
    "# clear unused\n",
    "for ax in axes_flat[n_drop:]:\n",
    "    fig.delaxes(ax)\n",
    "\n",
    "# colorbar at the right edge\n",
    "fig.subplots_adjust(right=0.88)\n",
    "cax = fig.add_axes([0.90, 0.15, 0.02, 0.7])  # [left, bottom, width, height]\n",
    "cbar = fig.colorbar(im, cax=cax)\n",
    "cbar.set_label('Average Validation Loss (MSE)')\n",
    "\n",
    "plt.savefig('figs/l1_l2_dropout_loss_pyramid.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[Test of activation functions](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general hyperparameters\n",
    "hidden_depth = 2\n",
    "hidden_width = 64\n",
    "learning_rate = learning_rate\n",
    "\n",
    "# general critereon and regularization parameters\n",
    "criterion = nn.MSELoss()\n",
    "lambda_l1 = 1e-4 # baseline l1 regularization\n",
    "lambda_l2 = 1e-3 # baseline l2 regularization\n",
    "dropout = 0.0\n",
    "\n",
    "activation_space = {'ReLU':nn.ReLU, 'LeakyReLU':nn.LeakyReLU, 'Tanh':nn.Tanh, 'Sigmoid':nn.Sigmoid}\n",
    "\n",
    "best_models_activ = {}\n",
    "history_activ = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for activ_name, activ_fun in activation_space.items():\n",
    "#     run_time = []\n",
    "#     start_time = time.time()\n",
    "#     print(f\"\"\"Training model for year '{year}...: \n",
    "#             activation      ={activ_name}\n",
    "#             lambda_l1       ={lambda_l1:.0e}\n",
    "#             lambda_l2       ={lambda_l2:.0e}\n",
    "#             dropout         ={dropout:.0e}\n",
    "#             learning_rate   ={learning_rate:.0e}\n",
    "#             hidden_depth    ={hidden_depth}\n",
    "#             hidden_width    ={hidden_width}\"\"\")\n",
    "#     for run in range(n_runs):\n",
    "#         print(f\"Run {run+1} of {n_runs}\")\n",
    "#         seed = 42+run\n",
    "#         np.random.seed(seed)\n",
    "#         torch.manual_seed(seed)\n",
    "#         # Initialize the model\n",
    "#         input_dim = X_train[year].shape[1]\n",
    "#         name = f'l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}_activ{activ_name}'\n",
    "#         models_21[name] = MLPModel(input_dim, depth=hidden_depth, width=hidden_width, dropout=dropout, activation=activ_fun).to(device)\n",
    "#         optimizer = torch.optim.Adam(models_21[name].parameters(), lr=learning_rate)\n",
    "#         train = MLPdataset(X_train[year], y_train[year])\n",
    "#         val = MLPdataset(X_val[year], y_val[year])\n",
    "#         best_models_activ[name], history_activ[name], time_ = train_mlp(train,          \n",
    "#                                         val,\n",
    "#                                         models_21[name],\n",
    "#                                         criterion,\n",
    "#                                         epochs,\n",
    "#                                         patience,\n",
    "#                                         print_freq,\n",
    "#                                         device,\n",
    "#                                         optimizer,\n",
    "#                                         lambda_l1=lambda_l1,\n",
    "#                                         lambda_l2=lambda_l2,\n",
    "#                                         batch_size=batch_size,\n",
    "#                                         shuffle_train=True,\n",
    "#                                         shuffle_val=False,\n",
    "#                                         save_path=f'models/hyperparam_test/mlp_y{year}_l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}_activ{activ_name}.pth',\n",
    "#                                         timing = True\n",
    "#                                         )\n",
    "#         run_time.append(time_)\n",
    "#     end_time = time.time()\n",
    "#     print(f\"Training with activation nn.{activ_name} took {end_time - start_time:.2f} seconds.\")\n",
    "#     print(f\"Average time per epoch: {np.concatenate(run_time).mean()} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten_history_activ(history_activ, \n",
    "#                 save_csv='models/hyperparam_test/history/history_activ.csv')\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_activ = pd.read_csv('models/hyperparam_test/history/history_activ.csv')\n",
    "\n",
    "activations = sorted(df_activ['activ_name'].unique())\n",
    "n_acts = len(activations)+1\n",
    "cmap = plt.get_cmap('viridis', n_acts)\n",
    "color_map = {act: cmap(i) for i, act in enumerate(activations)}\n",
    "\n",
    "# create 2x2 grid\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "# plot\n",
    "for i, act in enumerate(activations):\n",
    "    ax = axes[i]\n",
    "    df_act = df_activ[df_activ['activ_name'] == act]\n",
    "    # Plot each run for this activation\n",
    "    for run_id, grp in df_act.groupby('run'):\n",
    "        ax.plot(\n",
    "            grp['epoch'],\n",
    "            grp['val_loss'],\n",
    "            color=color_map[act],\n",
    "            alpha=0.5,\n",
    "            linewidth=2,\n",
    "        )\n",
    "    ax.set_title(act)\n",
    "    # show x-label on bottom row \n",
    "    if i // 2 == 1:\n",
    "        ax.set_xlabel('Epoch', fontsize=12)\n",
    "    else:\n",
    "        ax.tick_params(labelbottom=False)\n",
    "    # show y-label on left column\n",
    "    if i % 2 == 0:\n",
    "        ax.set_ylabel('Validation Loss', fontsize=12)\n",
    "    else:\n",
    "        ax.tick_params(labelleft=False)\n",
    "    ax.set_ylim(1.1, 1.2)\n",
    "\n",
    "# remove any unused subplots\n",
    "for j in range(len(activations), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('figs/activation_fun_history.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models for activation ReLU...\n",
      "Loading models for activation LeakyReLU...\n",
      "Loading models for activation Tanh...\n",
      "Loading models for activation Sigmoid...\n"
     ]
    }
   ],
   "source": [
    "# load the best model for each activation function for each run\n",
    "preds_activ = {}\n",
    "\n",
    "for activ_name, activ_fun in activation_space.items():\n",
    "    print(f\"Loading models for activation {activ_name}...\")\n",
    "    all_preds = []\n",
    "    activation_fun = activ_fun\n",
    "    for run in range(n_runs):\n",
    "        best_models_activ[f'activ{activ_name}_run{run+1}'] = load_model(\n",
    "            hidden_width, hidden_depth, run, \n",
    "            lambda_l1, lambda_l2, dropout, learning_rate, activ=activ_name\n",
    "        )\n",
    "        preds = predict_mlp(\n",
    "            best_models_activ[f'activ{activ_name}_run{run+1}'], \n",
    "            X_val[year], \n",
    "            y_val[year], \n",
    "            y_scalers[year],\n",
    "            batch_size=batch_size,\n",
    "            device=device,\n",
    "        )\n",
    "        all_preds.append(preds)\n",
    "    np.stack(all_preds, axis=0)\n",
    "    preds_activ[activ_name] = np.mean(all_preds, axis=0)\n",
    "\n",
    "df_preds_activ = pd.DataFrame(preds_activ)\n",
    "df_preds_activ['y_true'] = y_scalers[year].inverse_transform(y_val[year].reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_full = {}\n",
    "results_full = {}\n",
    "for activ_name in activation_space.keys():\n",
    "    y_true = df_preds_activ['y_true'].values\n",
    "    y_pred = df_preds_activ[activ_name].values\n",
    "    results_full[activ_name] = {\n",
    "        'RMSE': rmse_fun(y_true, y_pred),\n",
    "        'MAE': mae_fun(y_true, y_pred),\n",
    "        'AMADL': amadl_fun(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "for metric in ['RMSE', 'MAE', 'AMADL']:\n",
    "        vals = [\n",
    "        results_full['ReLU'][metric],\n",
    "        results_full['LeakyReLU'][metric],\n",
    "        results_full['Tanh'][metric],\n",
    "        results_full['Sigmoid'][metric],\n",
    "        ]\n",
    "        metrics_full[metric] = vals\n",
    "\n",
    "tab_activ = latex_table(list(activation_space.keys()),metrics_full)\n",
    "with open('tabs/activ_fun_perf.tex', 'w') as f:\n",
    "    f.write(tab_activ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define plot limits\n",
    "x_min, x_max = -0.5, 0.5\n",
    "bins = 100\n",
    "\n",
    "# 2×2 grid with shared axes\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 6),\n",
    "                         sharex=True, sharey=True)\n",
    "\n",
    "for ax, act in zip(axes.flat, activation_space):\n",
    "    res = df_preds_activ['y_true'] - df_preds_activ[act]\n",
    "\n",
    "    # density=True makes the histogram area = 1\n",
    "    ax.hist(res, bins=bins, density=True, alpha=0.7)\n",
    "\n",
    "    x = np.linspace(x_min, x_max, 500)\n",
    "    mu, sigma = res.mean(), res.std(ddof=0)\n",
    "    mu_lap    = np.median(res)\n",
    "    b_lap     = np.mean(np.abs(res - mu_lap))\n",
    "    nu        = 1\n",
    "\n",
    "    # Normal pdf\n",
    "    pdf_gauss = (1/(sigma*np.sqrt(2*np.pi))) * np.exp(-0.5*((x - mu)/sigma)**2)\n",
    "    ax.plot(x, pdf_gauss, \"C1--\", lw=2,\n",
    "            label=rf\"Normal($\\mu$={mu:.3f}, $\\sigma$={sigma:.3f})\")\n",
    "\n",
    "    # Laplace pdf\n",
    "    pdf_lap = (1/(2*b_lap)) * np.exp(-np.abs(x - mu_lap)/b_lap)\n",
    "    ax.plot(x, pdf_lap, \"C3:\", lw=2,\n",
    "            label=rf\"Laplace($\\mu_L$={mu_lap:.3f}, b={b_lap:.3f})\")\n",
    "\n",
    "    # Student-t pdf\n",
    "    pdf_t = student_t.pdf(x, df=nu, loc=mu, scale=sigma)\n",
    "    ax.plot(x, pdf_t, \"C6-.\", lw=2,\n",
    "            label=rf\"Student-$t_{{{nu}}}$($\\mu$={mu:.3f}, $\\sigma$={sigma:.3f})\")\n",
    "\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_title(act)\n",
    "    ax.legend(fontsize=\"small\")\n",
    "\n",
    "# Only label the outer axes\n",
    "axes[0, 0].set_ylabel('Density')\n",
    "axes[1, 0].set_ylabel('Density')\n",
    "axes[1, 0].set_xlabel('Residual')\n",
    "axes[1, 1].set_xlabel('Residual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figs/residuals_activations_density.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc7_'></a>[Test of criterion functions](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general hyperparameters\n",
    "hidden_depth = 2 \n",
    "hidden_width = 64 \n",
    "learning_rate = learning_rate\n",
    "\n",
    "# general critereon and regularization parameters\n",
    "lambda_l1 = 1e-4 # baseline l1 regularization\n",
    "lambda_l2 = 1e-3 # baseline l2 regularization\n",
    "dropout = 0.0\n",
    "activation_fun = nn.ReLU\n",
    "\n",
    "criterion_space = {'MSE':nn.MSELoss(),'MAE':nn.L1Loss(),'Huber':nn.HuberLoss(delta=0.5)}\n",
    "\n",
    "best_models_crit = {}\n",
    "history_crit = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for crit, crit_fun in criterion_space.items():\n",
    "#     run_time = []\n",
    "#     start_time = time.time()\n",
    "#     print(f\"\"\"Training model for year '{year}...: \n",
    "#             criterion       ={crit}\n",
    "#             lambda_l1       ={lambda_l1:.0e}\n",
    "#             lambda_l2       ={lambda_l2:.0e}\n",
    "#             dropout         ={dropout:.0e}\n",
    "#             learning_rate   ={learning_rate:.0e}\n",
    "#             hidden_depth    ={hidden_depth}\n",
    "#             hidden_width    ={hidden_width}\"\"\")\n",
    "#     for run in range(n_runs):\n",
    "#         print(f\"Run {run+1} of {n_runs}\")\n",
    "#         seed = 42+run\n",
    "#         np.random.seed(seed)\n",
    "#         torch.manual_seed(seed)\n",
    "#         # Initialize the model\n",
    "#         input_dim = X_train[year].shape[1]\n",
    "#         name = f'l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}_crit{crit}'\n",
    "#         models_21[name] = MLPModel(input_dim, depth=hidden_depth, width=hidden_width, dropout=dropout, activation=activation_fun).to(device)\n",
    "#         optimizer = torch.optim.Adam(models_21[name].parameters(), lr=learning_rate)\n",
    "#         train = MLPdataset(X_train[year], y_train[year])\n",
    "#         val = MLPdataset(X_val[year], y_val[year])\n",
    "#         best_models_crit[name], history_crit[name], time_ = train_mlp(train,          \n",
    "#                                         val,\n",
    "#                                         models_21[name],\n",
    "#                                         crit_fun,\n",
    "#                                         epochs,\n",
    "#                                         patience,\n",
    "#                                         print_freq,\n",
    "#                                         device,\n",
    "#                                         optimizer,\n",
    "#                                         lambda_l1=lambda_l1,\n",
    "#                                         lambda_l2=lambda_l2,\n",
    "#                                         batch_size=batch_size,\n",
    "#                                         shuffle_train=True,\n",
    "#                                         shuffle_val=False,\n",
    "#                                         save_path=f'models/hyperparam_test/mlp_y{year}_l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}_crit{crit}.pth',\n",
    "#                                         timing = True\n",
    "#                                         )\n",
    "#         run_time.append(time_)\n",
    "#     end_time = time.time()\n",
    "#     print(f\"Training with criterion {crit_fun} took {end_time - start_time:.2f} seconds.\")\n",
    "#     print(f\"Average time per epoch: {np.concatenate(run_time).mean()} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten_history_crit(history_crit, \n",
    "#                 save_csv='models/hyperparam_test/history/history_crit.csv')\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crit = pd.read_csv('models/hyperparam_test/history/history_crit.csv')\n",
    "\n",
    "criterions = sorted(df_crit['crit_name'].unique(), reverse=True)\n",
    "n_crits = len(criterions)+1\n",
    "cmap = plt.get_cmap('viridis', n_crits)\n",
    "color_map = {crit: cmap(i) for i, crit in enumerate(criterions)}\n",
    "\n",
    "\n",
    "# 2×2 grid with shared axes\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "gs  = gridspec.GridSpec(2, 4, height_ratios=[1, 1])\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0,0:2])\n",
    "ax2 = fig.add_subplot(gs[0,2:4])\n",
    "ax3 = fig.add_subplot(gs[1,1:3])\n",
    "axes = [ax1, ax2, ax3]\n",
    "\n",
    "# plot\n",
    "for i, crit in enumerate(criterions):\n",
    "    ax = axes[i]\n",
    "    df_cr = df_crit[df_crit['crit_name'] == crit]\n",
    "    # Plot each run for this criterion\n",
    "    for run_id, grp in df_cr.groupby('run'):\n",
    "        ax.plot(\n",
    "            grp['epoch'],\n",
    "            grp['val_loss'],\n",
    "            color=color_map[crit],\n",
    "            alpha=0.5,\n",
    "            linewidth=2,\n",
    "        )\n",
    "    ax.set_title(crit)\n",
    "    ax.set_ylabel(f'Validation Loss ({crit})', fontsize=12)\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "\n",
    "ax1.set_ylim(1.1, 1.2)\n",
    "ax2.set_ylim(0.65, 0.7)\n",
    "ax3.set_ylim(0.24, 0.25)\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('figs/criterion_fun_history.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models for criterion MSE...\n",
      "Loading models for criterion MAE...\n",
      "Loading models for criterion Huber...\n"
     ]
    }
   ],
   "source": [
    "# load the best model for each criterion function for each run\n",
    "preds_crit = {}\n",
    "\n",
    "for crit_name, crit_fun in criterion_space.items():\n",
    "    print(f\"Loading models for criterion {crit_name}...\")\n",
    "    all_preds = []\n",
    "    for run in range(n_runs):\n",
    "        best_models_crit[f'crit{crit_name}_run{run+1}'] = load_model(\n",
    "            hidden_width, hidden_depth, run, \n",
    "            lambda_l1, lambda_l2, dropout, learning_rate, crit=crit_name\n",
    "        )\n",
    "        preds = predict_mlp(\n",
    "            best_models_crit[f'crit{crit_name}_run{run+1}'], \n",
    "            X_val[year], \n",
    "            y_val[year], \n",
    "            y_scalers[year],\n",
    "            batch_size=batch_size,\n",
    "            device=device,\n",
    "        )\n",
    "        all_preds.append(preds)\n",
    "    np.stack(all_preds, axis=0)\n",
    "    preds_crit[crit_name] = np.mean(all_preds, axis=0)\n",
    "\n",
    "df_preds_crit = pd.DataFrame(preds_crit)\n",
    "df_preds_crit['y_true'] = y_scalers[year].inverse_transform(y_val[year].reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_full = {}\n",
    "results_full = {}\n",
    "for crit_name in criterion_space.keys():\n",
    "    y_true = df_preds_crit['y_true'].values\n",
    "    y_pred = df_preds_crit[crit_name].values\n",
    "    results_full[crit_name] = {\n",
    "        'RMSE': rmse_fun(y_true, y_pred),\n",
    "        'MAE': mae_fun(y_true, y_pred),\n",
    "        'AMADL': amadl_fun(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "for metric in ['RMSE', 'MAE', 'AMADL']:\n",
    "        vals = [\n",
    "        results_full['MSE'][metric],\n",
    "        results_full['MAE'][metric],\n",
    "        results_full['Huber'][metric],\n",
    "        ]\n",
    "        metrics_full[metric] = vals\n",
    "\n",
    "tab_crit = latex_table(list(criterion_space.keys()),metrics_full)\n",
    "with open('tabs/crit_fun_perf.tex', 'w') as f:\n",
    "    f.write(tab_crit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define plot limits\n",
    "x_min, x_max = -0.5, 0.5\n",
    "bins = 100\n",
    "\n",
    "# 2×2 grid with shared axes\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "gs  = gridspec.GridSpec(2, 4, height_ratios=[1, 1])\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0,0:2])\n",
    "ax2 = fig.add_subplot(gs[0,2:4],  sharex=ax1, sharey=ax1)\n",
    "ax2.tick_params(axis='y', labelleft=False)\n",
    "ax3 = fig.add_subplot(gs[1,1:3],  sharex=ax1, sharey=ax1)\n",
    "axes = [ax1, ax2, ax3]\n",
    "\n",
    "for ax, act in zip(axes, criterion_space):\n",
    "    res = df_preds_crit['y_true'] - df_preds_crit[act]\n",
    "\n",
    "    # density=True makes the histogram area = 1\n",
    "    ax.hist(res, bins=bins, density=True, alpha=0.7)\n",
    "\n",
    "    x = np.linspace(x_min, x_max, 500)\n",
    "    mu, sigma = res.mean(), res.std(ddof=0)\n",
    "    mu_lap    = np.median(res)\n",
    "    b_lap     = np.mean(np.abs(res - mu_lap))\n",
    "    nu        = 1\n",
    "\n",
    "    # Normal pdf\n",
    "    pdf_gauss = (1/(sigma*np.sqrt(2*np.pi))) * np.exp(-0.5*((x - mu)/sigma)**2)\n",
    "    ax.plot(x, pdf_gauss, \"C1--\", lw=2,\n",
    "            label=rf\"Normal($\\mu$={mu:.3f}, $\\sigma$={sigma:.3f})\")\n",
    "\n",
    "    # Laplace pdf\n",
    "    pdf_lap = (1/(2*b_lap)) * np.exp(-np.abs(x - mu_lap)/b_lap)\n",
    "    ax.plot(x, pdf_lap, \"C3:\", lw=2,\n",
    "            label=rf\"Laplace($\\mu_L$={mu_lap:.3f}, b={b_lap:.3f})\")\n",
    "\n",
    "    # Student-t pdf\n",
    "    pdf_t = student_t.pdf(x, df=nu, loc=mu, scale=sigma)\n",
    "    ax.plot(x, pdf_t, \"C6-.\", lw=2,\n",
    "            label=rf\"Student-$t_{{{nu}}}$($\\mu$={mu:.3f}, $\\sigma$={sigma:.3f})\")\n",
    "\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_title(act)\n",
    "    ax.legend(fontsize=\"small\")\n",
    "    ax.set_xlabel('Residual')\n",
    "\n",
    "# Only label the outer axes\n",
    "ax1.set_ylabel('Density')\n",
    "ax3.set_ylabel('Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figs/residuals_criterion_density.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

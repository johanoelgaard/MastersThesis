{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Notebook for hyperparameter tuning of the MLP model](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Notebook for hyperparameter tuning of the MLP model](#toc1_)    \n",
    "- [Import libraries](#toc2_)    \n",
    "- [Import data](#toc3_)    \n",
    "- [Prepare data for training](#toc4_)    \n",
    "- [Test hyperparameters](#toc5_)    \n",
    "  - [Constant scheme](#toc5_1_)    \n",
    "  - [Pyramid scheme](#toc5_2_)    \n",
    "    - [Depth and width comparison](#toc5_2_1_)    \n",
    "    - [Model convergence](#toc5_2_2_)    \n",
    "  - [Regularization strength](#toc5_3_)    \n",
    "  - [Pyramid](#toc5_4_)    \n",
    "- [Test of activation functions](#toc6_)    \n",
    "- [Test of criterion functions](#toc7_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Import libraries](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "import matplotlib.gridspec as gridspec\n",
    "import math\n",
    "import ast\n",
    "import gc\n",
    "import time\n",
    "from scipy.stats import t as student_t\n",
    "\n",
    "from libs.models import *\n",
    "from libs.functions import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "plt.rcParams.update({'font.size': 12, 'figure.figsize': (10, 4), 'figure.dpi': 300})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Import data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df = pd.read_csv('data/data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Prepare data for training](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare expanding window splits\n",
    "periods = {\n",
    "    '21' : '2019-12-31', # 2021 is the test set\n",
    "    # '22' : '2020-12-31', # 2022 is the test set\n",
    "    # '23' : '2021-12-31', # 2023 is the test set\n",
    "    # '24': '2022-12-31' # 2024 is the test set\n",
    "}\n",
    "\n",
    "# identify dummy vs. numeric columns\n",
    "feature_cols = [col for col in df.columns if col not in ['timestamp', 'ticker', 'target']]\n",
    "nace_cols = [c for c in feature_cols if c.startswith('NACE_')]\n",
    "dummy_cols = ['divi','divo'] # sin removed\n",
    "macro_cols = ['discount', 'tms', 'dp', 'ep', 'svar'] # 'bm_macro'\n",
    "\n",
    "# nummeric cols = cols not in cat and macro cols\n",
    "numeric_cols = [c for c in feature_cols if c not in dummy_cols and c not in nace_cols and c not in macro_cols]\n",
    "\n",
    "df_raw = df.copy(deep=True)\n",
    "df_raw['timestamp'] = pd.to_datetime(df_raw['timestamp'])\n",
    "\n",
    "# drop data from 2025\n",
    "df_raw = df_raw[df_raw['timestamp'] < '2025-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create interaction features between numeric and macro features\n",
    "C = df[numeric_cols].values         # shape = (n_rows, P_c)\n",
    "X = df[macro_cols].values           # shape = (n_rows, P_x)\n",
    "\n",
    "# compute all pairwise products with broadcasting:\n",
    "K = C[:,:,None] * X[:,None,:]\n",
    "\n",
    "# reshape to (n_rows, P_c * P_x)\n",
    "Z = K.reshape(len(df), -1)\n",
    "\n",
    "# build the column names in the same order\n",
    "xc_names = [\n",
    "    f\"{c}_x_{m}\"\n",
    "    for c in numeric_cols\n",
    "    for m in macro_cols\n",
    "]\n",
    "\n",
    "# wrap back into a DataFrame\n",
    "df_xc = pd.DataFrame(Z, columns=xc_names, index=df.index)\n",
    "\n",
    "feature_cols = numeric_cols + xc_names + dummy_cols + nace_cols\n",
    "numeric_cols = numeric_cols + xc_names\n",
    "cat_cols = dummy_cols + nace_cols\n",
    "df_z = df_raw.merge(df_xc, left_index=True, right_index=True)\n",
    "# drop macro_cols\n",
    "df_z = df_z.drop(columns=macro_cols)\n",
    "# sort columns by feature_cols\n",
    "df_norm = df_z[['timestamp', 'ticker', 'target'] + feature_cols]\n",
    "\n",
    "y_values = df_norm['target'].values.astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare containers\n",
    "X_train, X_val, X_test = {}, {}, {}\n",
    "y_train, y_val, y_test = {}, {}, {}\n",
    "preprocessors = {}\n",
    "y_scalers = {}\n",
    "\n",
    "for y, period in periods.items():\n",
    "    period = pd.to_datetime(period)\n",
    "\n",
    "    # split masks\n",
    "    tr_mask = df_norm['timestamp'] < period\n",
    "    va_mask = (df_norm['timestamp'] >= period) & \\\n",
    "              (df_norm['timestamp'] - pd.DateOffset(years=1) < period)\n",
    "    te_mask = (df_norm['timestamp'] - pd.DateOffset(years=1) >= period) & \\\n",
    "              (df_norm['timestamp'] - pd.DateOffset(years=2) < period)\n",
    "\n",
    "    # extract raw feature DataFrames\n",
    "    X_tr_df = df_norm.loc[tr_mask, feature_cols].copy()\n",
    "    X_va_df = df_norm.loc[va_mask, feature_cols].copy()\n",
    "    X_te_df = df_norm.loc[te_mask, feature_cols].copy()\n",
    "    y_tr    = y_values[tr_mask]\n",
    "    y_va    = y_values[va_mask]\n",
    "    y_te    = y_values[te_mask]\n",
    "\n",
    "    # compute winsorization bounds on train\n",
    "    lower = X_tr_df[numeric_cols].quantile(0.01)\n",
    "    upper = X_tr_df[numeric_cols].quantile(0.99)\n",
    "\n",
    "    # apply clipping to train, val, test\n",
    "    X_tr_df[numeric_cols] = X_tr_df[numeric_cols].clip(lower=lower, upper=upper, axis=1)\n",
    "    X_va_df[numeric_cols] = X_va_df[numeric_cols].clip(lower=lower, upper=upper, axis=1)\n",
    "    X_te_df[numeric_cols] = X_te_df[numeric_cols].clip(lower=lower, upper=upper, axis=1)\n",
    "\n",
    "    # now fit scaler on numeric only\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', StandardScaler(), numeric_cols),\n",
    "        ('cat', 'passthrough',  cat_cols)\n",
    "    ])\n",
    "    preprocessor.fit(X_tr_df)\n",
    "    preprocessors[y] = preprocessor\n",
    "\n",
    "    # transform all splits\n",
    "    X_train[y] = preprocessor.transform(X_tr_df).astype('float32')\n",
    "    X_val[y]   = preprocessor.transform(X_va_df).astype('float32')\n",
    "    X_test[y]  = preprocessor.transform(X_te_df).astype('float32')\n",
    "\n",
    "    # fit standard scaler on y values\n",
    "    y_scaler = StandardScaler()\n",
    "    y_scaler.fit(y_tr.reshape(-1, 1))\n",
    "    y_scalers[y] = y_scaler\n",
    "    y_tr = y_scaler.transform(y_tr.reshape(-1, 1)).flatten()\n",
    "    y_va = y_scaler.transform(y_va.reshape(-1, 1)).flatten()\n",
    "    y_te = y_scaler.transform(y_te.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "    # store targets as before\n",
    "    y_train[y] = y_tr.reshape(-1, 1)\n",
    "    y_val[y]   = y_va.reshape(-1, 1)\n",
    "    y_test[y]  = y_te.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# moving to metal or CUDA GPU if available\n",
    "device = torch.device((\"cuda\" if torch.cuda.is_available() \n",
    "                       else \"mps\" if torch.backends.mps.is_available() \n",
    "                       else \"cpu\"))\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# general hyperparameters\n",
    "hidden_depth = None \n",
    "hidden_width = None \n",
    "learning_rate = 5e-5 # 1e-4 # 1e-5 \n",
    "activation_fun = nn.ReLU # nn.ReLU, nn.Tanh, nn.Sigmoid, nn.LeakyReLU\n",
    "\n",
    "# general critereon and regularization parameters\n",
    "criterion = nn.MSELoss()\n",
    "lambda_l1 = 1e-4 # baseline l1 regularization # 1e-5\n",
    "lambda_l2 = 1e-4 # baseline l2 regularization\n",
    "drop = 0.0\n",
    "\n",
    "# general parmeters\n",
    "patience = 25\n",
    "print_freq = 100\n",
    "epochs = 250\n",
    "batch_size = 4096 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = '21'\n",
    "models_21 = {}\n",
    "n_runs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model loader fun\n",
    "def load_model(w, d, run, lambda_l1, lambda_l2, drop, lr, activ = None, crit = None):\n",
    "    m = MLPModel(\n",
    "        input_dim  = X_train[year].shape[1],\n",
    "        depth      = d,\n",
    "        width      = w,\n",
    "        dropout    = drop,\n",
    "        activation = activation_fun,\n",
    "    ).to(device)\n",
    "    path = (\n",
    "        f\"models/hyperparam_test/mlp_y{year}\"\n",
    "        f\"_l1{lambda_l1}_l2{lambda_l2}\"\n",
    "        f\"_drop{drop}_lr{lr}\"\n",
    "        f\"_w{w}_d{d}_run{run+1}.pth\"\n",
    "    )\n",
    "    if activ is not None:\n",
    "        path = path.replace('.pth', f'_activ{activ}.pth')\n",
    "    if crit is not None:\n",
    "        path = path.replace('.pth', f'_crit{crit}.pth')\n",
    "    m.load_state_dict(torch.load(path, map_location=device))\n",
    "    m.eval()\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Test hyperparameters](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_1_'></a>[Constant scheme](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model with most data on multiple parameters\n",
    "l1_space = [lambda_l1]\n",
    "l2_space = [lambda_l2]\n",
    "dropout_space = [drop]\n",
    "learning_rate_space = [learning_rate]\n",
    "depth_space = [1, 2, 3, 4, 5, 6, 7]\n",
    "width_space = [8, 16, 32, 64, 128]\n",
    "best_models_size = {}\n",
    "history_size = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # current runtime: 2h 25m at 5 runs per model\n",
    "# for lambda_l1 in l1_space:\n",
    "#     for lambda_l2 in l2_space:\n",
    "#         for dropout in dropout_space:\n",
    "#             for learning_rate in learning_rate_space:\n",
    "#                 for hidden_depth in depth_space:\n",
    "#                     for hidden_width in width_space:\n",
    "#                         print(f\"\"\"Training model for year '{year}...: \n",
    "#                                 lambda_l1       ={lambda_l1:.0e}\n",
    "#                                 lambda_l2       ={lambda_l2:.0e}\n",
    "#                                 dropout         ={dropout:.0e}\n",
    "#                                 learning_rate   ={learning_rate:.0e}\n",
    "#                                 hidden_depth    ={hidden_depth}\n",
    "#                                 hidden_width    ={hidden_width}\"\"\")\n",
    "#                         for run in range(n_runs):\n",
    "#                             print(f\"Run {run+1} of {n_runs}\")\n",
    "#                             seed = 42+run   \n",
    "#                             np.random.seed(seed)\n",
    "#                             torch.manual_seed(seed)\n",
    "#                             # Initialize the model\n",
    "#                             input_dim = X_train[year].shape[1]\n",
    "#                             name = f'l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}'\n",
    "#                             models_21[name] = MLPModel(input_dim, depth=hidden_depth, width=hidden_width, dropout=dropout, activation=activation_fun).to(device)\n",
    "#                             optimizer = torch.optim.Adam(models_21[name].parameters(), lr=learning_rate)\n",
    "#                             train = MLPdataset(X_train[year], y_train[year])\n",
    "#                             val = MLPdataset(X_val[year], y_val[year])\n",
    "#                             best_models_size[name], history_size[name] = train_mlp(train,          \n",
    "#                                                             val,\n",
    "#                                                             models_21[name],\n",
    "#                                                             criterion,\n",
    "#                                                             epochs,\n",
    "#                                                             patience,\n",
    "#                                                             print_freq,\n",
    "#                                                             device,\n",
    "#                                                             optimizer,\n",
    "#                                                             lambda_l1=lambda_l1,\n",
    "#                                                             lambda_l2=lambda_l2,\n",
    "#                                                             batch_size=batch_size,\n",
    "#                                                             shuffle_train=True,\n",
    "#                                                             shuffle_val=False,\n",
    "#                                                             save_path=f'models/hyperparam_test/mlp_y{year}_l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}.pth'\n",
    "#                                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten_history(history_size, \n",
    "#                 save_csv='models/hyperparam_test/history/history_size.csv')\n",
    "# gc.collect()\n",
    "# torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_2_'></a>[Pyramid scheme](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model with most data on multiple parameters\n",
    "l1_space = [lambda_l1]\n",
    "l2_space = [lambda_l2]\n",
    "dropout_space = [drop]\n",
    "learning_rate_space = [learning_rate]\n",
    "depth_space = None\n",
    "width_space = [[32], \n",
    "               [32, 16], \n",
    "               [32, 16, 8], \n",
    "               [32, 16, 8, 4], \n",
    "               [32, 16, 8, 4, 2]]\n",
    "best_models_pyramid = {}\n",
    "history_pyramid = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # current runtime: 30m\n",
    "# for lambda_l1 in l1_space:\n",
    "#     for lambda_l2 in l2_space:\n",
    "#         for dropout in dropout_space:\n",
    "#             for learning_rate in learning_rate_space:\n",
    "#                 for hidden_width in width_space:\n",
    "#                     hidden_depth = len(hidden_width)\n",
    "#                     print(f\"\"\"Training model for year '{year}...: \n",
    "#                             lambda_l1       ={lambda_l1:.0e}\n",
    "#                             lambda_l2       ={lambda_l2:.0e}\n",
    "#                             dropout         ={dropout:.0e}\n",
    "#                             learning_rate   ={learning_rate:.0e}\n",
    "#                             hidden_depth    ={hidden_depth}\n",
    "#                             hidden_width    ={hidden_width}\"\"\")\n",
    "#                     for run in range(n_runs):\n",
    "#                         print(f\"Run {run+1} of {n_runs}\")\n",
    "#                         seed = 42+run\n",
    "#                         np.random.seed(seed)\n",
    "#                         torch.manual_seed(seed)\n",
    "#                         # Initialize the model\n",
    "#                         input_dim = X_train[year].shape[1]\n",
    "#                         name = f'l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}'\n",
    "#                         models_21[name] = MLPModel(input_dim, depth=hidden_depth, width=hidden_width, dropout=dropout, activation=activation_fun).to(device)\n",
    "#                         optimizer = torch.optim.Adam(models_21[name].parameters(), lr=learning_rate)\n",
    "#                         train = MLPdataset(X_train[year], y_train[year])\n",
    "#                         val = MLPdataset(X_val[year], y_val[year])\n",
    "#                         best_models_pyramid[name], history_pyramid[name] = train_mlp(train,          \n",
    "#                                                         val,\n",
    "#                                                         models_21[name],\n",
    "#                                                         criterion,\n",
    "#                                                         epochs,\n",
    "#                                                         patience,\n",
    "#                                                         print_freq,\n",
    "#                                                         device,\n",
    "#                                                         optimizer,\n",
    "#                                                         lambda_l1=lambda_l1,\n",
    "#                                                         lambda_l2=lambda_l2,\n",
    "#                                                         batch_size=batch_size,\n",
    "#                                                         shuffle_train=True,\n",
    "#                                                         shuffle_val=False,\n",
    "#                                                         save_path=f'models/hyperparam_test/mlp_y{year}_l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}.pth'\n",
    "#                                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten_history(history_pyramid, \n",
    "#                 save_csv='models/hyperparam_test/history/history_pyramid.csv')\n",
    "# gc.collect()\n",
    "# torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_2_1_'></a>[Depth and width comparison](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant-width grid\n",
    "const_widths = [8, 16, 32, 64, 128]\n",
    "const_depths = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "# pyramid sequences (each column of the right heatmap)\n",
    "pyr_seqs = [\n",
    "    [32],\n",
    "    [32,16],\n",
    "    [32,16,8],\n",
    "    [32,16,8,4],\n",
    "    [32,16,8,4,2],\n",
    "]\n",
    "Wc = len(const_widths)\n",
    "Dc = len(const_depths)\n",
    "Wp = len(pyr_seqs)\n",
    "\n",
    "# DataLoader\n",
    "val_ds     = MLPdataset(X_val[year], y_val[year])\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# constant-width loss matrix\n",
    "loss_const = np.zeros((Dc, Wc))\n",
    "for di, d in enumerate(const_depths):\n",
    "    for wj, w in enumerate(const_widths):\n",
    "        run_losses = []\n",
    "        for run in range(n_runs):\n",
    "            m = load_model(w, d, run, lambda_l1, lambda_l2, drop, learning_rate)\n",
    "            with torch.no_grad():\n",
    "                batch = [criterion(m(x.to(device)), y.to(device)).item()\n",
    "                         for x,y in val_loader]\n",
    "            run_losses.append(np.mean(batch))\n",
    "        loss_const[di, wj] = np.mean(run_losses)\n",
    "\n",
    "# pyramid loss matrix\n",
    "loss_pyr = np.full((Wp, Wp), np.nan)\n",
    "for sj, seq in enumerate(pyr_seqs):\n",
    "    depth = len(seq)\n",
    "    run_losses = []\n",
    "    for run in range(n_runs):\n",
    "        # we “flatten” seq into width param for loader\n",
    "        m = load_model(seq, depth, run, lambda_l1, lambda_l2, drop, learning_rate)\n",
    "        with torch.no_grad():\n",
    "            batch = [criterion(m(x.to(device)), y.to(device)).item()\n",
    "                     for x,y in val_loader]\n",
    "        run_losses.append(np.mean(batch))\n",
    "    loss_pyr[depth-1, sj] = np.mean(run_losses)\n",
    "\n",
    "# plot\n",
    "fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(12,5),\n",
    "                               gridspec_kw={'width_ratios':[4,1]})\n",
    "\n",
    "# color‐scale\n",
    "all_vals = loss_const.ravel()\n",
    "vmin    = all_vals.min()\n",
    "vmax    = all_vals.max()+0.005\n",
    "vcenter = vmin + 0.5*(vmax - vmin)\n",
    "norm    = TwoSlopeNorm(vmin=vmin, vcenter=vcenter, vmax=vmax)\n",
    "cmap    = 'viridis'\n",
    "\n",
    "# left: constant‐width heatmap\n",
    "im0 = ax0.imshow(loss_const, aspect='auto', cmap=cmap, norm=norm)\n",
    "im0 = ax0.imshow(loss_const, aspect='auto', cmap=cmap, norm=norm)\n",
    "ax0.set_xticks(range(Wc))\n",
    "ax0.set_xticklabels(const_widths)\n",
    "ax0.set_yticks(range(Dc))\n",
    "ax0.set_yticklabels(const_depths)\n",
    "ax0.set_xlabel('Hidden Width')\n",
    "ax0.set_ylabel('Hidden Depth')\n",
    "ax0.set_title(f'Constant Width')\n",
    "\n",
    "# right: collapsed‐x pyramid heatmap\n",
    "diag_pyr = np.diag(loss_pyr)\n",
    "diag_mat = diag_pyr[:, np.newaxis]\n",
    "\n",
    "im1 = ax1.imshow(diag_mat, aspect='auto', cmap=cmap, norm=norm)\n",
    "ax1.set_xticks([0])\n",
    "ax1.set_xticklabels(['Geometric\\npyramid'])\n",
    "ax1.set_yticks(np.arange(Wp))\n",
    "ax1.set_yticklabels(np.arange(1, Wp+1))\n",
    "ax1.set_ylabel('Hidden Depth')\n",
    "ax1.set_title('Pyramid Scheme')\n",
    "\n",
    "# shared colorbar\n",
    "cbar = fig.colorbar(im0, ax=[ax0,ax1], shrink=0.9, pad=0.02)\n",
    "cbar.set_label('Average Validation Loss (MSE)')\n",
    "\n",
    "plt.savefig('figs/width_depth.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre‐compute global vmin/vmax across all runs so color‐scale is consistent\n",
    "all_vals = []\n",
    "for run in range(n_runs):\n",
    "    # constant‐width for this run\n",
    "    tmp = []\n",
    "    for d in const_depths:\n",
    "        for w in const_widths:\n",
    "            m = load_model(w, d, run, lambda_l1, lambda_l2, drop, learning_rate)\n",
    "            with torch.no_grad():\n",
    "                batch = [criterion(m(x.to(device)), y.to(device)).item()\n",
    "                         for x,y in val_loader]\n",
    "            tmp.append(np.mean(batch))\n",
    "    all_vals.extend(tmp)\n",
    "\n",
    "    # pyramid diag for this run\n",
    "    diag = []\n",
    "    for seq in pyr_seqs:\n",
    "        depth = len(seq)\n",
    "        m = load_model(seq, depth, run, lambda_l1, lambda_l2, drop, learning_rate)\n",
    "        with torch.no_grad():\n",
    "            batch = [criterion(m(x.to(device)), y.to(device)).item()\n",
    "                     for x,y in val_loader]\n",
    "        diag.append(np.mean(batch))\n",
    "    all_vals.extend(diag)\n",
    "\n",
    "runs = list(range(n_runs))\n",
    "# split into pages of 4 runs each\n",
    "pages = [runs[i:i+4] for i in range(0, len(runs), 4)]\n",
    "\n",
    "for p, page_runs in enumerate(pages, start=1):\n",
    "    nrows = len(page_runs)\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows, 2,\n",
    "        figsize=(10, 3*nrows),\n",
    "        gridspec_kw={'width_ratios': [4, 1]},\n",
    "        constrained_layout=False  # turn off for manual colorbar placement\n",
    "    )\n",
    "\n",
    "    # if only one row, wrap axes\n",
    "    if nrows == 1:\n",
    "        axes = np.expand_dims(axes, 0)\n",
    "\n",
    "    for i, run in enumerate(page_runs):\n",
    "        ax0, ax1 = axes[i]\n",
    "\n",
    "        # ---- left: constant-width heatmap for this run ----\n",
    "        loss_const = np.zeros((Dc, Wc))\n",
    "        for di, d in enumerate(const_depths):\n",
    "            for wj, w in enumerate(const_widths):\n",
    "                m = load_model(w, d, run, lambda_l1, lambda_l2, drop, learning_rate)\n",
    "                with torch.no_grad():\n",
    "                    batch = [criterion(m(x.to(device)), y.to(device)).item()\n",
    "                             for x,y in val_loader]\n",
    "                loss_const[di, wj] = np.mean(batch)\n",
    "\n",
    "        im0 = ax0.imshow(loss_const, aspect='auto', cmap=cmap, norm=norm)\n",
    "        if i == 0:\n",
    "            ax0.set_title('Constant Width')\n",
    "        ax0.set_ylabel(f'Run {run+1}\\nHidden Depth')\n",
    "        ax0.set_xticks(range(Wc))\n",
    "        ax0.set_xticklabels(const_widths)\n",
    "        ax0.set_yticks(range(Dc))\n",
    "        ax0.set_yticklabels(const_depths)\n",
    "        if i == nrows - 1:\n",
    "            ax0.set_xlabel('Hidden Width')\n",
    "        else:\n",
    "            ax0.set_xlabel('')\n",
    "            ax0.tick_params(labelbottom=False)\n",
    "\n",
    "        # collapsed pyramid for this run\n",
    "        diag = []\n",
    "        for seq in pyr_seqs:\n",
    "            depth = len(seq)\n",
    "            m = load_model(seq, depth, run, lambda_l1, lambda_l2, drop, learning_rate)\n",
    "            with torch.no_grad():\n",
    "                batch = [criterion(m(x.to(device)), y.to(device)).item()\n",
    "                         for x,y in val_loader]\n",
    "            diag.append(np.mean(batch))\n",
    "        diag_mat = np.array(diag)[:, None]\n",
    "\n",
    "        im1 = ax1.imshow(diag_mat, aspect='auto', cmap=cmap, norm=norm)\n",
    "        if i == 0:\n",
    "            ax1.set_title('Pyramid Scheme')\n",
    "        ax1.set_xticks([0])\n",
    "        # only label the bottom subplot\n",
    "        if i == nrows - 1:\n",
    "            ax1.set_xticklabels(['Geometric\\npyramid'])\n",
    "        else:\n",
    "            ax1.set_xticklabels([])\n",
    "        ax1.set_yticks(range(Wp))\n",
    "        ax1.set_yticklabels(range(1, Wp+1))\n",
    "        ax1.set_ylabel('Hidden Depth')\n",
    "\n",
    "    # shared colorbar on the right of this page\n",
    "    cax = fig.add_axes([0.92,  # 92% from left\n",
    "                        0.11,  # 10% from bottom\n",
    "                        0.02,  # 2% figure‐width\n",
    "                        0.77   # 80% figure‐height\n",
    "                       ])\n",
    "    cbar = fig.colorbar(im0, cax=cax)\n",
    "    cbar.set_label('Validation Loss (MSE)')\n",
    "\n",
    "    fig.savefig(f'figs/width_depth_page{p}.png', dpi=300, bbox_inches='tight')\n",
    "    # plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_2_2_'></a>[Model convergence](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSVs\n",
    "df_size = pd.read_csv('models/hyperparam_test/history/history_size.csv')\n",
    "df_pyr  = pd.read_csv('models/hyperparam_test/history/history_pyramid.csv')\n",
    "\n",
    "# extract name\n",
    "df_size['widths'] = df_size['widths'].apply(ast.literal_eval)\n",
    "df_pyr ['widths'] = df_pyr ['widths'].apply(ast.literal_eval)\n",
    "\n",
    "df_size['width'] = df_size['widths'].apply(lambda w: w[0])\n",
    "df_pyr ['width'] = df_pyr ['widths'].apply(lambda w: w[0])\n",
    "\n",
    "df_size['name'] = df_size['width'].apply(\n",
    "    lambda w: f\"Constant width = {w}\"\n",
    ")\n",
    "\n",
    "width_to_str = (\n",
    "    df_pyr\n",
    "    .groupby('width')['widths_str']\n",
    "    .agg(lambda ss: max(ss, key=len))\n",
    "    .to_dict()\n",
    ")\n",
    "df_pyr['name'] = df_pyr['width'].map(\n",
    "    lambda w: f\"Pyramid width = {width_to_str[w]}\"\n",
    ")\n",
    "\n",
    "# combine\n",
    "df = pd.concat([df_size, df_pyr], ignore_index=True)\n",
    "\n",
    "# sort by width\n",
    "panel_df = (\n",
    "    df[['name','width']]            # pick only the two columns we care about\n",
    "      .drop_duplicates()            # one row per unique panel\n",
    "      .assign(is_pyr=lambda x: x['name'].str.startswith('Pyramid'))\n",
    "      .sort_values(['is_pyr','width'])  # constants (is_pyr=False) first, then pyramids; each by ascending width\n",
    ")\n",
    "\n",
    "plot_names = panel_df['name'].tolist()\n",
    "n = len(plot_names)\n",
    "n_cols = min(3, n)\n",
    "n_rows = math.ceil(n / n_cols)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols,\n",
    "                         figsize=(4*n_cols, 4*n_rows),\n",
    "                         sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "# build a unified color map over depths\n",
    "depths = sorted(df['depth'].unique())\n",
    "cmap   = plt.get_cmap('viridis')\n",
    "colors = {d: cmap(i/(len(depths)-1)) for i, d in enumerate(depths)}\n",
    "\n",
    "for idx, name in enumerate(plot_names):\n",
    "    ax = axes[idx]\n",
    "    sub_df = df[df['name'] == name]\n",
    "    for d in depths:\n",
    "        sd = sub_df[sub_df['depth'] == d]\n",
    "        for run in sd['run'].unique():\n",
    "            run_df = sd[sd['run'] == run].sort_values('epoch')\n",
    "            ax.plot(\n",
    "                run_df['epoch'],\n",
    "                run_df['val_loss'],\n",
    "                color=colors[d],\n",
    "                alpha=0.8,\n",
    "                linewidth=2,\n",
    "                label=(f\"d={d}\" if run == sd['run'].min() else None)\n",
    "            )\n",
    "\n",
    "    ax.set_title(name)\n",
    "    # ax.set_ylim(1.75e-2, 3e-2)\n",
    "    ax.set_ylim(1.1, 1.3)\n",
    "    ax.set_xlim(0, 250)\n",
    "    ax.set_xticks(np.linspace(0, 250, 6, dtype=int))\n",
    "\n",
    "    # only bottom row: x-label\n",
    "    if idx // n_cols == n_rows - 1:\n",
    "        ax.set_xlabel(\"Epoch\", fontsize=12)\n",
    "    else:\n",
    "        ax.set_xticklabels([])\n",
    "\n",
    "    # only leftmost column: y-label\n",
    "    if idx % n_cols == 0:\n",
    "        ax.set_ylabel(\"Validation Loss\", fontsize=12)\n",
    "\n",
    "# remove any unused axes\n",
    "for j in range(len(plot_names), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# global legend on right\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels,\n",
    "           title=\"Hidden Depth\",\n",
    "           loc='center left',\n",
    "           bbox_to_anchor=(1, 0.5),\n",
    "           fontsize=12)\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.savefig('figs/val_loss_history.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_3_'></a>[Regularization strength](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model with most data on multiple parameters\n",
    "l1_space = [0.0, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "l2_space = [0.0, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "dropout_space = [0.0, 0.05, 0.1, 0.2]\n",
    "learning_rate_space = [learning_rate] \n",
    "depth_space = [2] \n",
    "width_space = [64]\n",
    "best_models_reg = {}\n",
    "history_reg = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for lambda_l1 in l1_space:\n",
    "#     for lambda_l2 in l2_space:\n",
    "#         for dropout in dropout_space:\n",
    "#             for learning_rate in learning_rate_space:\n",
    "#                 for hidden_depth in depth_space:\n",
    "#                     for hidden_width in width_space:\n",
    "#                         print(f\"\"\"Training model for year '{year}...: \n",
    "#                                 lambda_l1       ={lambda_l1:.0e}\n",
    "#                                 lambda_l2       ={lambda_l2:.0e}\n",
    "#                                 dropout         ={dropout:.0e}\n",
    "#                                 learning_rate   ={learning_rate:.0e}\n",
    "#                                 hidden_depth    ={hidden_depth}\n",
    "#                                 hidden_width    ={hidden_width}\"\"\")\n",
    "#                         for run in range(n_runs):\n",
    "#                             print(f\"Run {run+1} of {n_runs}\")\n",
    "#                             seed = 42+run\n",
    "#                             np.random.seed(seed)\n",
    "#                             torch.manual_seed(seed)\n",
    "#                             # Initialize the model\n",
    "#                             input_dim = X_train[year].shape[1]\n",
    "#                             name = f'l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}'\n",
    "#                             models_21[name] = MLPModel(input_dim, depth=hidden_depth, width=hidden_width, dropout=dropout, activation=activation_fun).to(device)\n",
    "#                             optimizer = torch.optim.Adam(models_21[name].parameters(), lr=learning_rate)\n",
    "#                             train = MLPdataset(X_train[year], y_train[year])\n",
    "#                             val = MLPdataset(X_val[year], y_val[year])\n",
    "#                             best_models_reg[name], history_reg[name] = train_mlp(train,          \n",
    "#                                                             val,\n",
    "#                                                             models_21[name],\n",
    "#                                                             criterion,\n",
    "#                                                             epochs,\n",
    "#                                                             patience,\n",
    "#                                                             print_freq,\n",
    "#                                                             device,\n",
    "#                                                             optimizer,\n",
    "#                                                             lambda_l1=lambda_l1,\n",
    "#                                                             lambda_l2=lambda_l2,\n",
    "#                                                             batch_size=batch_size,\n",
    "#                                                             shuffle_train=True,\n",
    "#                                                             shuffle_val=False,\n",
    "#                                                             save_path=f'models/hyperparam_test/mlp_y{year}_l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}.pth'\n",
    "#                                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten_history(history_reg, \n",
    "#                 save_csv='models/hyperparam_test/history/history_reg.csv')\n",
    "# gc.collect()\n",
    "# torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model with most data on multiple parameters\n",
    "l1_space = [0.0, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "l2_space = [0.0, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "dropout_space = [0.0, 0.05 , 0.1, 0.2] \n",
    "learning_rate_space = [learning_rate] \n",
    "depth_space = [2] \n",
    "width_space = [64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "val_ds     = MLPdataset(X_val[year], y_val[year])\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# build (n_drop, n_l1, n_l2) mean-loss array\n",
    "n_l1   = len(l1_space)\n",
    "n_l2   = len(l2_space)\n",
    "n_drop = len(dropout_space)\n",
    "\n",
    "losses = np.zeros((n_drop, n_l1, n_l2), dtype=float)\n",
    "\n",
    "for di, drop in enumerate(dropout_space):\n",
    "    for i, l1 in enumerate(l1_space):\n",
    "        for j, l2 in enumerate(l2_space):\n",
    "            run_losses = []\n",
    "            # for each seed, load & eval\n",
    "            for run in range(n_runs):\n",
    "                # since depth_space & width_space each have one entry, \n",
    "                # we can just index 0 here — but this will generalize\n",
    "                tmp_losses = []\n",
    "                for d in depth_space:\n",
    "                    for w in width_space:\n",
    "                        m = load_model(w, d, run, l1, l2, drop, learning_rate, )\n",
    "                        with torch.no_grad():\n",
    "                            batch = [criterion(m(x.to(device)), y.to(device)).item()\n",
    "                                     for x,y in val_loader]\n",
    "                        tmp_losses.append(np.mean(batch))\n",
    "                run_losses.append(np.mean(tmp_losses))\n",
    "            # now average over runs\n",
    "            losses[di, i, j] = np.mean(run_losses)\n",
    "\n",
    "\n",
    "# global color‐scale\n",
    "vmin    = losses.min()\n",
    "vmax    = losses.max()\n",
    "vcenter = vmin + 0.5*(vmax - vmin)\n",
    "norm    = TwoSlopeNorm(vmin=vmin, vcenter=vcenter, vmax=vmax)\n",
    "cmap    = 'viridis'\n",
    "\n",
    "# layout: up to 2 columns\n",
    "ncols = min(2, n_drop)\n",
    "nrows = math.ceil(n_drop / ncols)\n",
    "\n",
    "fig, axes = plt.subplots(nrows, ncols,\n",
    "                         figsize=(5*ncols, 4*nrows),\n",
    "                         squeeze=False)\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for idx, drop in enumerate(dropout_space):\n",
    "    ax = axes_flat[idx]\n",
    "    im = ax.imshow(\n",
    "        losses[idx],\n",
    "        aspect='auto',\n",
    "        cmap=cmap,\n",
    "        norm=norm,\n",
    "        origin='lower',\n",
    "    )\n",
    "\n",
    "    # y = l1, x = l2\n",
    "    ax.set_yticks(np.arange(n_l1))\n",
    "    ax.set_yticklabels(l1_space)\n",
    "    ax.set_xticks(np.arange(n_l2))\n",
    "    ax.set_xticklabels(l2_space)\n",
    "\n",
    "    # only bottom row shows x‐labels\n",
    "    if idx < (nrows-1)*ncols:\n",
    "        ax.tick_params(labelbottom=False)\n",
    "    else:\n",
    "        ax.set_xlabel(r'$\\ell_2$')\n",
    "\n",
    "    # only first‐column shows y‐labels\n",
    "    if idx % ncols != 0:\n",
    "        ax.tick_params(labelleft=False)\n",
    "    else:\n",
    "        ax.set_ylabel(r'$\\ell_1$')\n",
    "\n",
    "    ax.set_title(f\"dropout = {drop:.2f}\")\n",
    "\n",
    "# clear unused\n",
    "for ax in axes_flat[n_drop:]:\n",
    "    fig.delaxes(ax)\n",
    "\n",
    "# colorbar at the right edge\n",
    "fig.subplots_adjust(right=0.88)\n",
    "cax = fig.add_axes([0.90, 0.15, 0.02, 0.7])  # [left, bottom, width, height]\n",
    "cbar = fig.colorbar(im, cax=cax)\n",
    "cbar.set_label('Average Validation Loss (MSE)')\n",
    "\n",
    "plt.savefig('figs/l1_l2_dropout_loss.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_4_'></a>[Pyramid](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model with most data on multiple parameters\n",
    "l1_space = [0.0, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "l2_space = [0.0, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "dropout_space = [0.0, 0.05, 0.1, 0.2]\n",
    "learning_rate_space = [learning_rate]\n",
    "depth_space = None\n",
    "width_space = [[32, 16, 8]]\n",
    "\n",
    "best_models_reg_pyramid = {}\n",
    "history_reg_pyramid = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # current runtime: 15h\n",
    "# for lambda_l1 in l1_space:\n",
    "#     for lambda_l2 in l2_space:\n",
    "#         for dropout in dropout_space:\n",
    "#             for learning_rate in learning_rate_space:\n",
    "#                 for hidden_width in width_space:\n",
    "#                     hidden_depth = len(hidden_width)\n",
    "#                     print(f\"\"\"Training model for year '{year}...: \n",
    "#                             lambda_l1       ={lambda_l1:.0e}\n",
    "#                             lambda_l2       ={lambda_l2:.0e}\n",
    "#                             dropout         ={dropout:.0e}\n",
    "#                             learning_rate   ={learning_rate:.0e}\n",
    "#                             hidden_depth    ={hidden_depth}\n",
    "#                             hidden_width    ={hidden_width}\"\"\")\n",
    "#                     for run in range(n_runs):\n",
    "#                         print(f\"Run {run+1} of {n_runs}\")\n",
    "#                         seed = 42+run\n",
    "#                         np.random.seed(seed)\n",
    "#                         torch.manual_seed(seed)\n",
    "#                         # Initialize the model\n",
    "#                         input_dim = X_train[year].shape[1]\n",
    "#                         name = f'l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}'\n",
    "#                         models_21[name] = MLPModel(input_dim, depth=hidden_depth, width=hidden_width, dropout=dropout, activation=activation_fun).to(device)\n",
    "#                         optimizer = torch.optim.Adam(models_21[name].parameters(), lr=learning_rate)\n",
    "#                         train = MLPdataset(X_train[year], y_train[year])\n",
    "#                         val = MLPdataset(X_val[year], y_val[year])\n",
    "#                         best_models_reg_pyramid[name], history_reg_pyramid[name] = train_mlp(train,          \n",
    "#                                                         val,\n",
    "#                                                         models_21[name],\n",
    "#                                                         criterion,\n",
    "#                                                         epochs,\n",
    "#                                                         patience,\n",
    "#                                                         print_freq,\n",
    "#                                                         device,\n",
    "#                                                         optimizer,\n",
    "#                                                         lambda_l1=lambda_l1,\n",
    "#                                                         lambda_l2=lambda_l2,\n",
    "#                                                         batch_size=batch_size,\n",
    "#                                                         shuffle_train=True,\n",
    "#                                                         shuffle_val=False,\n",
    "#                                                         save_path=f'models/hyperparam_test/mlp_y{year}_l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}.pth'\n",
    "#                                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten_history(history_reg_pyramid, \n",
    "#                 save_csv='models/hyperparam_test/history/history_reg_pyramid.csv')\n",
    "# gc.collect()\n",
    "# torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model with most data on multiple parameters\n",
    "l1_space = [0.0, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "l2_space = [0.0, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "dropout_space = [0.0, 0.05 , 0.1, 0.2] \n",
    "learning_rate_space = [learning_rate]\n",
    "depth_space = None\n",
    "width_space = [[32, 16, 8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "val_ds     = MLPdataset(X_val[year], y_val[year])\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# build (n_drop, n_l1, n_l2) mean-loss array\n",
    "n_l1   = len(l1_space)\n",
    "n_l2   = len(l2_space)\n",
    "n_drop = len(dropout_space)\n",
    "\n",
    "losses = np.zeros((n_drop, n_l1, n_l2), dtype=float)\n",
    "\n",
    "for di, drop in enumerate(dropout_space):\n",
    "    for i, l1 in enumerate(l1_space):\n",
    "        for j, l2 in enumerate(l2_space):\n",
    "            run_losses = []\n",
    "            # for each seed, load & eval\n",
    "            for run in range(n_runs):\n",
    "                # since depth_space & width_space each have one entry, \n",
    "                # we can just index 0 here — but this will generalize\n",
    "                tmp_losses = []\n",
    "                for w in width_space:\n",
    "                    d = len(w)  # since w is a list, we can use its length\n",
    "                    m = load_model(w, d, run, l1, l2, drop, learning_rate, )\n",
    "                    with torch.no_grad():\n",
    "                        batch = [criterion(m(x.to(device)), y.to(device)).item()\n",
    "                                    for x,y in val_loader]\n",
    "                    tmp_losses.append(np.mean(batch))\n",
    "                run_losses.append(np.mean(tmp_losses))\n",
    "            # now average over runs\n",
    "            losses[di, i, j] = np.mean(run_losses)\n",
    "\n",
    "\n",
    "# global color‐scale\n",
    "vmin    = losses.min()\n",
    "vmax    = losses.max()\n",
    "vcenter = vmin + 0.5*(vmax - vmin)\n",
    "norm    = TwoSlopeNorm(vmin=vmin, vcenter=vcenter, vmax=vmax)\n",
    "cmap    = 'viridis'\n",
    "\n",
    "# layout: up to 2 columns\n",
    "ncols = min(2, n_drop)\n",
    "nrows = math.ceil(n_drop / ncols)\n",
    "\n",
    "fig, axes = plt.subplots(nrows, ncols,\n",
    "                         figsize=(5*ncols, 4*nrows),\n",
    "                         squeeze=False)\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for idx, drop in enumerate(dropout_space):\n",
    "    ax = axes_flat[idx]\n",
    "    im = ax.imshow(\n",
    "        losses[idx],\n",
    "        aspect='auto',\n",
    "        cmap=cmap,\n",
    "        norm=norm,\n",
    "        origin='lower',\n",
    "    )\n",
    "\n",
    "    # y = l1, x = l2\n",
    "    ax.set_yticks(np.arange(n_l1))\n",
    "    ax.set_yticklabels(l1_space)\n",
    "    ax.set_xticks(np.arange(n_l2))\n",
    "    ax.set_xticklabels(l2_space)\n",
    "\n",
    "    # only bottom row shows x‐labels\n",
    "    if idx < (nrows-1)*ncols:\n",
    "        ax.tick_params(labelbottom=False)\n",
    "    else:\n",
    "        ax.set_xlabel(r'$\\ell_2$')\n",
    "\n",
    "    # only first‐column shows y‐labels\n",
    "    if idx % ncols != 0:\n",
    "        ax.tick_params(labelleft=False)\n",
    "    else:\n",
    "        ax.set_ylabel(r'$\\ell_1$')\n",
    "\n",
    "    ax.set_title(f\"dropout = {drop:.2f}\")\n",
    "\n",
    "# clear unused\n",
    "for ax in axes_flat[n_drop:]:\n",
    "    fig.delaxes(ax)\n",
    "\n",
    "# colorbar at the right edge\n",
    "fig.subplots_adjust(right=0.88)\n",
    "cax = fig.add_axes([0.90, 0.15, 0.02, 0.7])  # [left, bottom, width, height]\n",
    "cbar = fig.colorbar(im, cax=cax)\n",
    "cbar.set_label('Average Validation Loss (MSE)')\n",
    "\n",
    "plt.savefig('figs/l1_l2_dropout_loss_pyramid.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[Test of activation functions](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general hyperparameters\n",
    "hidden_depth = 2\n",
    "hidden_width = 64\n",
    "learning_rate = learning_rate\n",
    "\n",
    "# general critereon and regularization parameters\n",
    "criterion = nn.MSELoss()\n",
    "lambda_l1 = 1e-4 # baseline l1 regularization\n",
    "lambda_l2 = 1e-3 # baseline l2 regularization\n",
    "dropout = 0.0\n",
    "\n",
    "activation_space = {'ReLU':nn.ReLU, 'LeakyReLU':nn.LeakyReLU, 'Tanh':nn.Tanh, 'Sigmoid':nn.Sigmoid}\n",
    "\n",
    "best_models_activ = {}\n",
    "history_activ = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for year '21...: \n",
      "            activation      =ReLU\n",
      "            lambda_l1       =1e-04\n",
      "            lambda_l2       =1e-03\n",
      "            dropout         =0e+00\n",
      "            learning_rate   =5e-05\n",
      "            hidden_depth    =2\n",
      "            hidden_width    =64\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 72\n",
      "Best val loss: 1.13764E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.0_lr5e-05_w64_d2_run1_activReLU.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13273E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.0_lr5e-05_w64_d2_run2_activReLU.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.01441E+00  - Val Loss: 1.13431E+00\n",
      "Epoch 200/250  - Train Loss: 9.62465E-01  - Val Loss: 1.12504E+00\n",
      "Early stopping at epoch 223\n",
      "Best val loss: 1.12180E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.0_lr5e-05_w64_d2_run3_activReLU.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13586E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.0_lr5e-05_w64_d2_run4_activReLU.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 66\n",
      "Best val loss: 1.13169E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.0_lr5e-05_w64_d2_run5_activReLU.pth\n",
      "Training with activation nn.ReLU took 281.03 seconds.\n",
      "Average time per epoch: 0.67814967896875 seconds.\n",
      "Training model for year '21...: \n",
      "            activation      =LeakyReLU\n",
      "            lambda_l1       =1e-04\n",
      "            lambda_l2       =1e-03\n",
      "            dropout         =0e+00\n",
      "            learning_rate   =5e-05\n",
      "            hidden_depth    =2\n",
      "            hidden_width    =64\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 72\n",
      "Best val loss: 1.13777E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.0_lr5e-05_w64_d2_run1_activLeakyReLU.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13282E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.0_lr5e-05_w64_d2_run2_activLeakyReLU.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.01476E+00  - Val Loss: 1.13432E+00\n",
      "Epoch 200/250  - Train Loss: 9.62992E-01  - Val Loss: 1.12485E+00\n",
      "Early stopping at epoch 223\n",
      "Best val loss: 1.12155E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.0_lr5e-05_w64_d2_run3_activLeakyReLU.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13608E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.0_lr5e-05_w64_d2_run4_activLeakyReLU.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 66\n",
      "Best val loss: 1.13161E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.0_lr5e-05_w64_d2_run5_activLeakyReLU.pth\n",
      "Training with activation nn.LeakyReLU took 260.41 seconds.\n",
      "Average time per epoch: 0.6297239034334322 seconds.\n",
      "Training model for year '21...: \n",
      "            activation      =Tanh\n",
      "            lambda_l1       =1e-04\n",
      "            lambda_l2       =1e-03\n",
      "            dropout         =0e+00\n",
      "            learning_rate   =5e-05\n",
      "            hidden_depth    =2\n",
      "            hidden_width    =64\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 60\n",
      "Best val loss: 1.14281E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.0_lr5e-05_w64_d2_run1_activTanh.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 40\n",
      "Best val loss: 1.13211E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.0_lr5e-05_w64_d2_run2_activTanh.pth\n",
      "Run 3 of 5\n",
      "Early stopping at epoch 48\n",
      "Best val loss: 1.14168E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.0_lr5e-05_w64_d2_run3_activTanh.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 45\n",
      "Best val loss: 1.13783E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.0_lr5e-05_w64_d2_run4_activTanh.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 34\n",
      "Best val loss: 1.13303E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.0_lr5e-05_w64_d2_run5_activTanh.pth\n",
      "Training with activation nn.Tanh took 146.92 seconds.\n",
      "Average time per epoch: 0.6463150403832623 seconds.\n",
      "Training model for year '21...: \n",
      "            activation      =Sigmoid\n",
      "            lambda_l1       =1e-04\n",
      "            lambda_l2       =1e-03\n",
      "            dropout         =0e+00\n",
      "            learning_rate   =5e-05\n",
      "            hidden_depth    =2\n",
      "            hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 1.02904E+00  - Val Loss: 1.14252E+00\n",
      "Early stopping at epoch 161\n",
      "Best val loss: 1.14105E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.0_lr5e-05_w64_d2_run1_activSigmoid.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 35\n",
      "Best val loss: 1.13480E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.0_lr5e-05_w64_d2_run2_activSigmoid.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.02687E+00  - Val Loss: 1.14241E+00\n",
      "Early stopping at epoch 140\n",
      "Best val loss: 1.14081E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.0_lr5e-05_w64_d2_run3_activSigmoid.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 38\n",
      "Best val loss: 1.13553E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.0_lr5e-05_w64_d2_run4_activSigmoid.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13414E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.0_lr5e-05_w64_d2_run5_activSigmoid.pth\n",
      "Training with activation nn.Sigmoid took 273.12 seconds.\n",
      "Average time per epoch: 0.6820183694750085 seconds.\n"
     ]
    }
   ],
   "source": [
    "# for activ_name, activ_fun in activation_space.items():\n",
    "#     run_time = []\n",
    "#     start_time = time.time()\n",
    "#     print(f\"\"\"Training model for year '{year}...: \n",
    "#             activation      ={activ_name}\n",
    "#             lambda_l1       ={lambda_l1:.0e}\n",
    "#             lambda_l2       ={lambda_l2:.0e}\n",
    "#             dropout         ={dropout:.0e}\n",
    "#             learning_rate   ={learning_rate:.0e}\n",
    "#             hidden_depth    ={hidden_depth}\n",
    "#             hidden_width    ={hidden_width}\"\"\")\n",
    "#     for run in range(n_runs):\n",
    "#         print(f\"Run {run+1} of {n_runs}\")\n",
    "#         seed = 42+run\n",
    "#         np.random.seed(seed)\n",
    "#         torch.manual_seed(seed)\n",
    "#         # Initialize the model\n",
    "#         input_dim = X_train[year].shape[1]\n",
    "#         name = f'l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}_activ{activ_name}'\n",
    "#         models_21[name] = MLPModel(input_dim, depth=hidden_depth, width=hidden_width, dropout=dropout, activation=activ_fun).to(device)\n",
    "#         optimizer = torch.optim.Adam(models_21[name].parameters(), lr=learning_rate)\n",
    "#         train = MLPdataset(X_train[year], y_train[year])\n",
    "#         val = MLPdataset(X_val[year], y_val[year])\n",
    "#         best_models_activ[name], history_activ[name], time_ = train_mlp(train,          \n",
    "#                                         val,\n",
    "#                                         models_21[name],\n",
    "#                                         criterion,\n",
    "#                                         epochs,\n",
    "#                                         patience,\n",
    "#                                         print_freq,\n",
    "#                                         device,\n",
    "#                                         optimizer,\n",
    "#                                         lambda_l1=lambda_l1,\n",
    "#                                         lambda_l2=lambda_l2,\n",
    "#                                         batch_size=batch_size,\n",
    "#                                         shuffle_train=True,\n",
    "#                                         shuffle_val=False,\n",
    "#                                         save_path=f'models/hyperparam_test/mlp_y{year}_l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}_activ{activ_name}.pth',\n",
    "#                                         timing = True\n",
    "#                                         )\n",
    "#         run_time.append(time_)\n",
    "#     end_time = time.time()\n",
    "#     print(f\"Training with activation nn.{activ_name} took {end_time - start_time:.2f} seconds.\")\n",
    "#     print(f\"Average time per epoch: {np.concatenate(run_time).mean()} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_history_activ(history_activ, \n",
    "                save_csv='models/hyperparam_test/history/history_activ.csv')\n",
    "gc.collect()\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_activ = pd.read_csv('models/hyperparam_test/history/history_activ.csv')\n",
    "\n",
    "activations = sorted(df_activ['activ_name'].unique())\n",
    "n_acts = len(activations)+1\n",
    "cmap = plt.get_cmap('viridis', n_acts)\n",
    "color_map = {act: cmap(i) for i, act in enumerate(activations)}\n",
    "\n",
    "# create 2x2 grid\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "# plot\n",
    "for i, act in enumerate(activations):\n",
    "    ax = axes[i]\n",
    "    df_act = df_activ[df_activ['activ_name'] == act]\n",
    "    # Plot each run for this activation\n",
    "    for run_id, grp in df_act.groupby('run'):\n",
    "        ax.plot(\n",
    "            grp['epoch'],\n",
    "            grp['val_loss'],\n",
    "            color=color_map[act],\n",
    "            alpha=0.5,\n",
    "            linewidth=2,\n",
    "        )\n",
    "    ax.set_title(act)\n",
    "    # show x-label on bottom row \n",
    "    if i // 2 == 1:\n",
    "        ax.set_xlabel('Epoch', fontsize=12)\n",
    "    else:\n",
    "        ax.tick_params(labelbottom=False)\n",
    "    # show y-label on left column\n",
    "    if i % 2 == 0:\n",
    "        ax.set_ylabel('Validation Loss', fontsize=12)\n",
    "    else:\n",
    "        ax.tick_params(labelleft=False)\n",
    "    ax.set_ylim(1.1, 1.2)\n",
    "    ax.set_xlim(0, 250)\n",
    "\n",
    "# remove any unused subplots\n",
    "for j in range(len(activations), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('figs/activation_fun_history.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models for activation ReLU...\n",
      "Loading models for activation LeakyReLU...\n",
      "Loading models for activation Tanh...\n",
      "Loading models for activation Sigmoid...\n"
     ]
    }
   ],
   "source": [
    "# load the best model for each activation function for each run\n",
    "preds_activ = {}\n",
    "\n",
    "for activ_name, activ_fun in activation_space.items():\n",
    "    print(f\"Loading models for activation {activ_name}...\")\n",
    "    all_preds = []\n",
    "    activation_fun = activ_fun\n",
    "    for run in range(n_runs):\n",
    "        best_models_activ[f'activ{activ_name}_run{run+1}'] = load_model(\n",
    "            hidden_width, hidden_depth, run, \n",
    "            lambda_l1, lambda_l2, dropout, learning_rate, activ=activ_name\n",
    "        )\n",
    "        preds = predict_mlp(\n",
    "            best_models_activ[f'activ{activ_name}_run{run+1}'], \n",
    "            X_val[year], \n",
    "            y_val[year], \n",
    "            y_scalers[year],\n",
    "            batch_size=batch_size,\n",
    "            device=device,\n",
    "        )\n",
    "        all_preds.append(preds)\n",
    "    np.stack(all_preds, axis=0)\n",
    "    preds_activ[activ_name] = np.mean(all_preds, axis=0)\n",
    "\n",
    "df_preds_activ = pd.DataFrame(preds_activ)\n",
    "df_preds_activ['y_true'] = y_scalers[year].inverse_transform(y_val[year].reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_full = {}\n",
    "results_full = {}\n",
    "for activ_name in activation_space.keys():\n",
    "    y_true = df_preds_activ['y_true'].values\n",
    "    y_pred = df_preds_activ[activ_name].values\n",
    "    results_full[activ_name] = {\n",
    "        'RMSE': rmse_fun(y_true, y_pred),\n",
    "        'MAE': mae_fun(y_true, y_pred),\n",
    "        'AMADL': amadl_fun(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "for metric in ['RMSE', 'MAE', 'AMADL']:\n",
    "        vals = [\n",
    "        results_full['ReLU'][metric],\n",
    "        results_full['LeakyReLU'][metric],\n",
    "        results_full['Tanh'][metric],\n",
    "        results_full['Sigmoid'][metric],\n",
    "        ]\n",
    "        metrics_full[metric] = vals\n",
    "\n",
    "tab_activ = latex_table(list(activation_space.keys()),metrics_full)\n",
    "with open('tabs/activ_fun_perf.tex', 'w') as f:\n",
    "    f.write(tab_activ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define plot limits\n",
    "x_min, x_max = -0.5, 0.5\n",
    "bins = 100\n",
    "\n",
    "# 2×2 grid with shared axes\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 6),\n",
    "                         sharex=True, sharey=True)\n",
    "\n",
    "for ax, act in zip(axes.flat, activation_space):\n",
    "    res = df_preds_activ['y_true'] - df_preds_activ[act]\n",
    "\n",
    "    # density=True makes the histogram area = 1\n",
    "    ax.hist(res, bins=bins, density=True, alpha=0.7)\n",
    "\n",
    "    x = np.linspace(x_min, x_max, 500)\n",
    "    mu, sigma = res.mean(), res.std(ddof=0)\n",
    "    mu_lap    = np.median(res)\n",
    "    b_lap     = np.mean(np.abs(res - mu_lap))\n",
    "    nu        = 1\n",
    "\n",
    "    # Normal pdf\n",
    "    pdf_gauss = (1/(sigma*np.sqrt(2*np.pi))) * np.exp(-0.5*((x - mu)/sigma)**2)\n",
    "    ax.plot(x, pdf_gauss, \"C1--\", lw=2,\n",
    "            label=rf\"Normal($\\mu$={mu:.3f}, $\\sigma$={sigma:.3f})\")\n",
    "\n",
    "    # Laplace pdf\n",
    "    pdf_lap = (1/(2*b_lap)) * np.exp(-np.abs(x - mu_lap)/b_lap)\n",
    "    ax.plot(x, pdf_lap, \"C3:\", lw=2,\n",
    "            label=rf\"Laplace($\\mu_L$={mu_lap:.3f}, b={b_lap:.3f})\")\n",
    "\n",
    "    # Student-t pdf\n",
    "    pdf_t = student_t.pdf(x, df=nu, loc=mu, scale=sigma)\n",
    "    ax.plot(x, pdf_t, \"C6-.\", lw=2,\n",
    "            label=rf\"Student-$t_{{{nu}}}$($\\mu$={mu:.3f}, $\\sigma$={sigma:.3f})\")\n",
    "\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_title(act)\n",
    "    ax.legend(fontsize=\"small\")\n",
    "\n",
    "# Only label the outer axes\n",
    "axes[0, 0].set_ylabel('Density')\n",
    "axes[1, 0].set_ylabel('Density')\n",
    "axes[1, 0].set_xlabel('Residual')\n",
    "axes[1, 1].set_xlabel('Residual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figs/residuals_activations_density.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc7_'></a>[Test of criterion functions](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general hyperparameters\n",
    "hidden_depth = 2 \n",
    "hidden_width = 64 \n",
    "learning_rate = learning_rate\n",
    "\n",
    "# general critereon and regularization parameters\n",
    "lambda_l1 = 1e-4 # baseline l1 regularization\n",
    "lambda_l2 = 1e-3 # baseline l2 regularization\n",
    "dropout = 0.0\n",
    "activation_fun = nn.ReLU\n",
    "\n",
    "criterion_space = {'MSE':nn.MSELoss(),'MAE':nn.L1Loss(),'Huber':nn.HuberLoss(delta=0.5)}\n",
    "\n",
    "best_models_crit = {}\n",
    "history_crit = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for year '21...: \n",
      "            criterion       =MSE\n",
      "            lambda_l1       =1e-04\n",
      "            lambda_l2       =1e-03\n",
      "            dropout         =0e+00\n",
      "            learning_rate   =5e-05\n",
      "            hidden_depth    =2\n",
      "            hidden_width    =64\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 72\n",
      "Best val loss: 1.13764E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.0_lr5e-05_w64_d2_run1_critMSE.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13273E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.0_lr5e-05_w64_d2_run2_critMSE.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 1.01441E+00  - Val Loss: 1.13431E+00\n",
      "Epoch 200/250  - Train Loss: 9.62465E-01  - Val Loss: 1.12504E+00\n",
      "Early stopping at epoch 223\n",
      "Best val loss: 1.12180E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.0_lr5e-05_w64_d2_run3_critMSE.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13586E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.0_lr5e-05_w64_d2_run4_critMSE.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 66\n",
      "Best val loss: 1.13169E+00\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.0_lr5e-05_w64_d2_run5_critMSE.pth\n",
      "Training with criterion MSELoss() took 373.72 seconds.\n",
      "Average time per epoch: 0.903829154985383 seconds.\n",
      "Training model for year '21...: \n",
      "            criterion       =MAE\n",
      "            lambda_l1       =1e-04\n",
      "            lambda_l2       =1e-03\n",
      "            dropout         =0e+00\n",
      "            learning_rate   =5e-05\n",
      "            hidden_depth    =2\n",
      "            hidden_width    =64\n",
      "Run 1 of 5\n",
      "Epoch 100/250  - Train Loss: 6.42044E-01  - Val Loss: 6.83228E-01\n",
      "Early stopping at epoch 111\n",
      "Best val loss: 6.83049E-01\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.0_lr5e-05_w64_d2_run1_critMAE.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 32\n",
      "Best val loss: 6.80779E-01\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.0_lr5e-05_w64_d2_run2_critMAE.pth\n",
      "Run 3 of 5\n",
      "Early stopping at epoch 74\n",
      "Best val loss: 6.83988E-01\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.0_lr5e-05_w64_d2_run3_critMAE.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 74\n",
      "Best val loss: 6.82234E-01\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.0_lr5e-05_w64_d2_run4_critMAE.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 6.81952E-01\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.0_lr5e-05_w64_d2_run5_critMAE.pth\n",
      "Training with criterion L1Loss() took 210.84 seconds.\n",
      "Average time per epoch: 0.6643359075837577 seconds.\n",
      "Training model for year '21...: \n",
      "            criterion       =Huber\n",
      "            lambda_l1       =1e-04\n",
      "            lambda_l2       =1e-03\n",
      "            dropout         =0e+00\n",
      "            learning_rate   =5e-05\n",
      "            hidden_depth    =2\n",
      "            hidden_width    =64\n",
      "Run 1 of 5\n",
      "Early stopping at epoch 77\n",
      "Best val loss: 2.44164E-01\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.0_lr5e-05_w64_d2_run1_critHuber.pth\n",
      "Run 2 of 5\n",
      "Early stopping at epoch 28\n",
      "Best val loss: 2.43086E-01\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.0_lr5e-05_w64_d2_run2_critHuber.pth\n",
      "Run 3 of 5\n",
      "Epoch 100/250  - Train Loss: 2.34005E-01  - Val Loss: 2.44237E-01\n",
      "Early stopping at epoch 103\n",
      "Best val loss: 2.44190E-01\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.0_lr5e-05_w64_d2_run3_critHuber.pth\n",
      "Run 4 of 5\n",
      "Early stopping at epoch 74\n",
      "Best val loss: 2.43715E-01\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.0_lr5e-05_w64_d2_run4_critHuber.pth\n",
      "Run 5 of 5\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 2.43300E-01\n",
      "Model saved to models/hyperparam_test/mlp_y21_l10.0001_l20.001_drop0.0_lr5e-05_w64_d2_run5_critHuber.pth\n",
      "Training with criterion HuberLoss() took 205.01 seconds.\n",
      "Average time per epoch: 0.6643294159708697 seconds.\n"
     ]
    }
   ],
   "source": [
    "# for crit, crit_fun in criterion_space.items():\n",
    "#     run_time = []\n",
    "#     start_time = time.time()\n",
    "#     print(f\"\"\"Training model for year '{year}...: \n",
    "#             criterion       ={crit}\n",
    "#             lambda_l1       ={lambda_l1:.0e}\n",
    "#             lambda_l2       ={lambda_l2:.0e}\n",
    "#             dropout         ={dropout:.0e}\n",
    "#             learning_rate   ={learning_rate:.0e}\n",
    "#             hidden_depth    ={hidden_depth}\n",
    "#             hidden_width    ={hidden_width}\"\"\")\n",
    "#     for run in range(n_runs):\n",
    "#         print(f\"Run {run+1} of {n_runs}\")\n",
    "#         seed = 42+run\n",
    "#         np.random.seed(seed)\n",
    "#         torch.manual_seed(seed)\n",
    "#         # Initialize the model\n",
    "#         input_dim = X_train[year].shape[1]\n",
    "#         name = f'l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}_crit{crit}'\n",
    "#         models_21[name] = MLPModel(input_dim, depth=hidden_depth, width=hidden_width, dropout=dropout, activation=activation_fun).to(device)\n",
    "#         optimizer = torch.optim.Adam(models_21[name].parameters(), lr=learning_rate)\n",
    "#         train = MLPdataset(X_train[year], y_train[year])\n",
    "#         val = MLPdataset(X_val[year], y_val[year])\n",
    "#         best_models_crit[name], history_crit[name], time_ = train_mlp(train,          \n",
    "#                                         val,\n",
    "#                                         models_21[name],\n",
    "#                                         crit_fun,\n",
    "#                                         epochs,\n",
    "#                                         patience,\n",
    "#                                         print_freq,\n",
    "#                                         device,\n",
    "#                                         optimizer,\n",
    "#                                         lambda_l1=lambda_l1,\n",
    "#                                         lambda_l2=lambda_l2,\n",
    "#                                         batch_size=batch_size,\n",
    "#                                         shuffle_train=True,\n",
    "#                                         shuffle_val=False,\n",
    "#                                         save_path=f'models/hyperparam_test/mlp_y{year}_l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}_crit{crit}.pth',\n",
    "#                                         timing = True\n",
    "#                                         )\n",
    "#         run_time.append(time_)\n",
    "#     end_time = time.time()\n",
    "#     print(f\"Training with criterion {crit_fun} took {end_time - start_time:.2f} seconds.\")\n",
    "#     print(f\"Average time per epoch: {np.concatenate(run_time).mean()} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_history_crit(history_crit, \n",
    "                save_csv='models/hyperparam_test/history/history_crit.csv')\n",
    "gc.collect()\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crit = pd.read_csv('models/hyperparam_test/history/history_crit.csv')\n",
    "\n",
    "criterions = sorted(df_crit['crit_name'].unique(), reverse=True)\n",
    "n_crits = len(criterions)+1\n",
    "cmap = plt.get_cmap('viridis', n_crits)\n",
    "color_map = {crit: cmap(i) for i, crit in enumerate(criterions)}\n",
    "\n",
    "\n",
    "# 2×2 grid with shared axes\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "gs  = gridspec.GridSpec(2, 4, height_ratios=[1, 1])\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0,0:2])\n",
    "ax2 = fig.add_subplot(gs[0,2:4])\n",
    "ax3 = fig.add_subplot(gs[1,1:3])\n",
    "axes = [ax1, ax2, ax3]\n",
    "\n",
    "# plot\n",
    "for i, crit in enumerate(criterions):\n",
    "    ax = axes[i]\n",
    "    df_cr = df_crit[df_crit['crit_name'] == crit]\n",
    "    # Plot each run for this criterion\n",
    "    for run_id, grp in df_cr.groupby('run'):\n",
    "        ax.plot(\n",
    "            grp['epoch'],\n",
    "            grp['val_loss'],\n",
    "            color=color_map[crit],\n",
    "            alpha=0.5,\n",
    "            linewidth=2,\n",
    "        )\n",
    "    ax.set_title(crit)\n",
    "    ax.set_ylabel(f'Validation Loss ({crit})', fontsize=12)\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_xlim(0, 250)\n",
    "\n",
    "ax1.set_ylim(1.1, 1.2)\n",
    "ax2.set_ylim(0.68, 0.7)\n",
    "ax3.set_ylim(0.24, 0.25)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('figs/criterion_fun_history.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models for criterion MSE...\n",
      "Loading models for criterion MAE...\n",
      "Loading models for criterion Huber...\n"
     ]
    }
   ],
   "source": [
    "# load the best model for each criterion function for each run\n",
    "preds_crit = {}\n",
    "\n",
    "for crit_name, crit_fun in criterion_space.items():\n",
    "    print(f\"Loading models for criterion {crit_name}...\")\n",
    "    all_preds = []\n",
    "    for run in range(n_runs):\n",
    "        best_models_crit[f'crit{crit_name}_run{run+1}'] = load_model(\n",
    "            hidden_width, hidden_depth, run, \n",
    "            lambda_l1, lambda_l2, dropout, learning_rate, crit=crit_name\n",
    "        )\n",
    "        preds = predict_mlp(\n",
    "            best_models_crit[f'crit{crit_name}_run{run+1}'], \n",
    "            X_val[year], \n",
    "            y_val[year], \n",
    "            y_scalers[year],\n",
    "            batch_size=batch_size,\n",
    "            device=device,\n",
    "        )\n",
    "        all_preds.append(preds)\n",
    "    np.stack(all_preds, axis=0)\n",
    "    preds_crit[crit_name] = np.mean(all_preds, axis=0)\n",
    "\n",
    "df_preds_crit = pd.DataFrame(preds_crit)\n",
    "df_preds_crit['y_true'] = y_scalers[year].inverse_transform(y_val[year].reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_full = {}\n",
    "results_full = {}\n",
    "for crit_name in criterion_space.keys():\n",
    "    y_true = df_preds_crit['y_true'].values\n",
    "    y_pred = df_preds_crit[crit_name].values\n",
    "    results_full[crit_name] = {\n",
    "        'RMSE': rmse_fun(y_true, y_pred),\n",
    "        'MAE': mae_fun(y_true, y_pred),\n",
    "        'AMADL': amadl_fun(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "for metric in ['RMSE', 'MAE', 'AMADL']:\n",
    "        vals = [\n",
    "        results_full['MSE'][metric],\n",
    "        results_full['MAE'][metric],\n",
    "        results_full['Huber'][metric],\n",
    "        ]\n",
    "        metrics_full[metric] = vals\n",
    "\n",
    "tab_crit = latex_table(list(criterion_space.keys()),metrics_full)\n",
    "with open('tabs/crit_fun_perf.tex', 'w') as f:\n",
    "    f.write(tab_crit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define plot limits\n",
    "x_min, x_max = -0.5, 0.5\n",
    "bins = 100\n",
    "\n",
    "# 2×2 grid with shared axes\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "gs  = gridspec.GridSpec(2, 4, height_ratios=[1, 1])\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0,0:2])\n",
    "ax2 = fig.add_subplot(gs[0,2:4],  sharex=ax1, sharey=ax1)\n",
    "ax2.tick_params(axis='y', labelleft=False)\n",
    "ax3 = fig.add_subplot(gs[1,1:3],  sharex=ax1, sharey=ax1)\n",
    "axes = [ax1, ax2, ax3]\n",
    "\n",
    "for ax, act in zip(axes, criterion_space):\n",
    "    res = df_preds_crit['y_true'] - df_preds_crit[act]\n",
    "\n",
    "    # density=True makes the histogram area = 1\n",
    "    ax.hist(res, bins=bins, density=True, alpha=0.7)\n",
    "\n",
    "    x = np.linspace(x_min, x_max, 500)\n",
    "    mu, sigma = res.mean(), res.std(ddof=0)\n",
    "    mu_lap    = np.median(res)\n",
    "    b_lap     = np.mean(np.abs(res - mu_lap))\n",
    "    nu        = 1\n",
    "\n",
    "    # Normal pdf\n",
    "    pdf_gauss = (1/(sigma*np.sqrt(2*np.pi))) * np.exp(-0.5*((x - mu)/sigma)**2)\n",
    "    ax.plot(x, pdf_gauss, \"C1--\", lw=2,\n",
    "            label=rf\"Normal($\\mu$={mu:.3f}, $\\sigma$={sigma:.3f})\")\n",
    "\n",
    "    # Laplace pdf\n",
    "    pdf_lap = (1/(2*b_lap)) * np.exp(-np.abs(x - mu_lap)/b_lap)\n",
    "    ax.plot(x, pdf_lap, \"C3:\", lw=2,\n",
    "            label=rf\"Laplace($\\mu_L$={mu_lap:.3f}, b={b_lap:.3f})\")\n",
    "\n",
    "    # Student-t pdf\n",
    "    pdf_t = student_t.pdf(x, df=nu, loc=mu, scale=sigma)\n",
    "    ax.plot(x, pdf_t, \"C6-.\", lw=2,\n",
    "            label=rf\"Student-$t_{{{nu}}}$($\\mu$={mu:.3f}, $\\sigma$={sigma:.3f})\")\n",
    "\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_title(act)\n",
    "    ax.legend(fontsize=\"small\")\n",
    "    ax.set_xlabel('Residual')\n",
    "\n",
    "# Only label the outer axes\n",
    "ax1.set_ylabel('Density')\n",
    "ax3.set_ylabel('Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figs/residuals_criterion_density.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

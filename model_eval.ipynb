{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Model Evaluation](#toc0_)\n",
    "\n",
    "This notebook contains the code to evaluate the models on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Model Evaluation](#toc1_)    \n",
    "- [Import libraries](#toc2_)    \n",
    "- [Import data](#toc3_)    \n",
    "- [Prepare data for training](#toc4_)    \n",
    "  - [Neural network data](#toc4_1_)    \n",
    "  - [Linear model data](#toc4_2_)    \n",
    "- [Train Neural Network](#toc5_)    \n",
    "  - [Constant width](#toc5_1_)    \n",
    "  - [Pyramid](#toc5_2_)    \n",
    "- [Train linear models](#toc6_)    \n",
    "  - [OLS](#toc6_1_)    \n",
    "  - [LASSO](#toc6_2_)    \n",
    "  - [Naïve](#toc6_3_)    \n",
    "- [Summarize the results](#toc7_)    \n",
    "  - [Variable importance](#toc7_1_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Import libraries](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "from libs.models import *\n",
    "from libs.functions import *\n",
    "\n",
    "plt.rcParams.update({'font.size': 12, 'figure.figsize': (10, 4), 'figure.dpi': 300})\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Import data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df = pd.read_csv('data/data.csv')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "# omxcb = pd.read_csv('data/omxcb_constit.csv')\n",
    "# omxcb['timestamp'] = pd.to_datetime(omxcb['timestamp'])\n",
    "# # df = df.merge(omxcb[['timestamp', 'ticker']], on=['timestamp','ticker'], how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Prepare data for training](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare expanding window splits\n",
    "periods = {\n",
    "    '21' : '2019-12-31', # 2021 is the test set\n",
    "    '22' : '2020-12-31', # 2022 is the test set\n",
    "    '23' : '2021-12-31', # 2023 is the test set\n",
    "    '24': '2022-12-31' # 2024 is the test set\n",
    "}\n",
    "\n",
    "# identify dummy vs. numeric columns\n",
    "feature_cols = [col for col in df.columns if col not in ['timestamp', 'ticker', 'target']]\n",
    "nace_cols = [c for c in feature_cols if c.startswith('NACE_')]\n",
    "dummy_cols = ['divi','divo'] # sin removed\n",
    "macro_cols = ['discount', 'tms', 'dp', 'ep', 'svar'] # 'bm_macro'\n",
    "\n",
    "# nummeric cols = cols not in cat and macro cols\n",
    "numeric_cols = [c for c in feature_cols if c not in dummy_cols and c not in nace_cols and c not in macro_cols]\n",
    "\n",
    "# feature_cols = numeric_cols + dummy_cols + nace_cols # reorder columns to have numeric first\n",
    "\n",
    "df_raw = df.copy(deep=True)\n",
    "df_raw['timestamp'] = pd.to_datetime(df_raw['timestamp'])\n",
    "\n",
    "# drop data from 2025\n",
    "df_raw = df_raw[df_raw['timestamp'] < '2025-01-01']\n",
    "df_raw = df_raw[df_raw['timestamp'] < '2024-12-31']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = df[numeric_cols].values         # shape = (n_rows, P_c)\n",
    "X = df[macro_cols].values           # shape = (n_rows, P_x)\n",
    "\n",
    "# 1) compute all pairwise products with broadcasting:\n",
    "#    this gives shape (n_rows, P_c, P_x)\n",
    "K = C[:,:,None] * X[:,None,:]\n",
    "\n",
    "# 2) reshape to (n_rows, P_c * P_x)\n",
    "Z = K.reshape(len(df), -1)\n",
    "\n",
    "# 3) build the column names in the same order\n",
    "xc_names = [\n",
    "    f\"{c}_x_{m}\"\n",
    "    for c in numeric_cols\n",
    "    for m in macro_cols\n",
    "]\n",
    "\n",
    "# 4) wrap back into a DataFrame\n",
    "df_xc = pd.DataFrame(Z, columns=xc_names, index=df.index)\n",
    "\n",
    "feature_cols = numeric_cols + xc_names + dummy_cols + nace_cols\n",
    "numeric_cols = numeric_cols + xc_names\n",
    "cat_cols = dummy_cols + nace_cols\n",
    "df_z = df_raw.merge(df_xc, left_index=True, right_index=True)\n",
    "# drop macro_cols\n",
    "df_z = df_z.drop(columns=macro_cols)\n",
    "# sort columns by feature_cols\n",
    "df_norm = df_z[['timestamp', 'ticker', 'target'] + feature_cols]\n",
    "\n",
    "y_values = df_norm['target'].values.astype('float32')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_1_'></a>[Neural network data](#toc0_)\n",
    "Including a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare containers\n",
    "X_train, X_val, X_test = {}, {}, {}\n",
    "y_train, y_val, y_test = {}, {}, {}\n",
    "preprocessors = {}\n",
    "y_scalers = {}\n",
    "y_scalers['21'] = None\n",
    "y_scalers['22'] = None\n",
    "y_scalers['23'] = None\n",
    "y_scalers['24'] = None\n",
    "\n",
    "for y, period in periods.items():\n",
    "    period = pd.to_datetime(period)\n",
    "\n",
    "    # split masks\n",
    "    tr_mask = df_norm['timestamp'] < period\n",
    "    va_mask = (df_norm['timestamp'] >= period) & \\\n",
    "              (df_norm['timestamp'] - pd.DateOffset(years=1) < period)\n",
    "    te_mask = (df_norm['timestamp'] - pd.DateOffset(years=1) >= period) & \\\n",
    "              (df_norm['timestamp'] - pd.DateOffset(years=2) < period)\n",
    "\n",
    "    # extract raw feature DataFrames\n",
    "    X_tr_df = df_norm.loc[tr_mask, feature_cols].copy()\n",
    "    X_va_df = df_norm.loc[va_mask, feature_cols].copy()\n",
    "    X_te_df = df_norm.loc[te_mask, feature_cols].copy()\n",
    "    y_tr    = y_values[tr_mask]\n",
    "    y_va    = y_values[va_mask]\n",
    "    y_te    = y_values[te_mask]\n",
    "\n",
    "    # compute winsorization bounds on train\n",
    "    lower = X_tr_df[numeric_cols].quantile(0.01)\n",
    "    upper = X_tr_df[numeric_cols].quantile(0.99)\n",
    "\n",
    "    # apply clipping to train, val, test\n",
    "    X_tr_df[numeric_cols] = X_tr_df[numeric_cols].clip(lower=lower, upper=upper, axis=1)\n",
    "    X_va_df[numeric_cols] = X_va_df[numeric_cols].clip(lower=lower, upper=upper, axis=1)\n",
    "    X_te_df[numeric_cols] = X_te_df[numeric_cols].clip(lower=lower, upper=upper, axis=1)\n",
    "\n",
    "    # now fit scaler on numeric only\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', StandardScaler(), numeric_cols),\n",
    "        ('cat', 'passthrough',  cat_cols)\n",
    "    ])\n",
    "    preprocessor.fit(X_tr_df)\n",
    "    preprocessors[y] = preprocessor\n",
    "\n",
    "    # transform all splits\n",
    "    X_train[y] = preprocessor.transform(X_tr_df).astype('float32')\n",
    "    X_val[y]   = preprocessor.transform(X_va_df).astype('float32')\n",
    "    X_test[y]  = preprocessor.transform(X_te_df).astype('float32')\n",
    "\n",
    "    # fit standard scaler on y values\n",
    "    y_scaler = StandardScaler()\n",
    "    y_scaler.fit(y_tr.reshape(-1, 1))\n",
    "    y_scalers[y] = y_scaler\n",
    "    y_tr = y_scaler.transform(y_tr.reshape(-1, 1)).flatten()\n",
    "    y_va = y_scaler.transform(y_va.reshape(-1, 1)).flatten()\n",
    "    y_te = y_scaler.transform(y_te.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # store targets as before\n",
    "    y_train[y] = y_tr.reshape(-1, 1)\n",
    "    y_val[y]   = y_va.reshape(-1, 1)\n",
    "    y_test[y]  = y_te.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_2_'></a>[Linear model data](#toc0_)\n",
    "Excluding the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Xlin_train, Xlin_test = {}, {}\n",
    "ylin_train, ylin_test = {}, {}\n",
    "preprocessors_lin = {}\n",
    "\n",
    "cat_cols_lin = cat_cols + ['const']\n",
    "feature_cols_lin = feature_cols + ['const']\n",
    "\n",
    "for y, period in periods.items():\n",
    "    period = pd.to_datetime(period)\n",
    "    tr_mask = df_norm['timestamp']- pd.DateOffset(years=1) < period\n",
    "    te_mask = (df_norm['timestamp'] - pd.DateOffset(years=1) >= period) & \\\n",
    "              (df_norm['timestamp'] - pd.DateOffset(years=2) < period)\n",
    "\n",
    "    # extract feature DataFrames\n",
    "    X_tr_df = df_norm.loc[tr_mask, feature_cols]\n",
    "    X_te_df = df_norm.loc[te_mask, feature_cols]\n",
    "    y_tr = y_values[tr_mask]\n",
    "    y_te = y_values[te_mask]\n",
    "\n",
    "    # add constant column for linear regression\n",
    "    X_tr_df['const'] = 1\n",
    "    X_te_df['const'] = 1\n",
    "\n",
    "    # compute winsorization bounds on train\n",
    "    lower = X_tr_df[numeric_cols].quantile(0.01)\n",
    "    upper = X_tr_df[numeric_cols].quantile(0.99)\n",
    "\n",
    "    # apply clipping to train, test\n",
    "    X_tr_df[numeric_cols] = X_tr_df[numeric_cols].clip(lower=lower, upper=upper, axis=1)\n",
    "    X_te_df[numeric_cols] = X_te_df[numeric_cols].clip(lower=lower, upper=upper, axis=1)\n",
    "\n",
    "    # fit scaler only on training set\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', StandardScaler(), numeric_cols),\n",
    "        ('cat', 'passthrough', cat_cols_lin)\n",
    "    ])\n",
    "    preprocessor.fit(X_tr_df)\n",
    "    preprocessors_lin[y] = preprocessor\n",
    "\n",
    "    # ttransform splits\n",
    "    Xlin_train[y] = preprocessor.transform(X_tr_df).astype('float32')\n",
    "    Xlin_test[y]  = preprocessor.transform(X_te_df).astype('float32')\n",
    "\n",
    "    # targets\n",
    "    ylin_train[y] = y_tr\n",
    "    ylin_test[y]  = y_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Train Neural Network](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_1_'></a>[Constant width](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# moving to metal or CUDA GPU if available\n",
    "device = torch.device((\"cuda\" if torch.cuda.is_available() \n",
    "                       else \"mps\" if torch.backends.mps.is_available() \n",
    "                       else \"cpu\"))\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# general hyperparameters\n",
    "hidden_depth = 4 # only hidden, excluding in- and output layers\n",
    "hidden_width = 16 # int for all being equal width; list for different widths\n",
    "learning_rate = 1e-4\n",
    "activation_fun = nn.ReLU # nn.ReLU nn.Tanh nn.Sigmoid nn.LeakyReLU\n",
    "\n",
    "# general critereon and regularization parameters\n",
    "criterion = nn.MSELoss() # nn.HuberLoss()\n",
    "lambda_l1 = 1e-5 # 1e-3 # l1 regularization\n",
    "lambda_l2 = 1e-4 # 1e-4 # l2 regularization\n",
    "dropout = 0.0\n",
    "\n",
    "# general parmeters\n",
    "patience = 25\n",
    "print_freq = 250\n",
    "epochs = 250\n",
    "batch_size = 4096\n",
    "\n",
    "n_runs = 5 # number of runs for each model to average over\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Year 21: running 5 restarts ===\n",
      "Run 1/5, seed=42\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13554E+00\n",
      "Model saved to models/mlp_y21_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run1.pth\n",
      "Run 2/5, seed=43\n",
      "Early stopping at epoch 199\n",
      "Best val loss: 1.12751E+00\n",
      "Model saved to models/mlp_y21_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run2.pth\n",
      "Run 3/5, seed=44\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13737E+00\n",
      "Model saved to models/mlp_y21_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run3.pth\n",
      "Run 4/5, seed=45\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13544E+00\n",
      "Model saved to models/mlp_y21_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run4.pth\n",
      "Run 5/5, seed=46\n",
      "Early stopping at epoch 216\n",
      "Best val loss: 1.11590E+00\n",
      "Model saved to models/mlp_y21_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run5.pth\n",
      "Averaged predictions for year 21 computed.\n",
      "\n",
      "=== Year 22: running 5 restarts ===\n",
      "Run 1/5, seed=42\n",
      "Early stopping at epoch 31\n",
      "Best val loss: 7.07082E-01\n",
      "Model saved to models/mlp_y22_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run1.pth\n",
      "Run 2/5, seed=43\n",
      "Early stopping at epoch 74\n",
      "Best val loss: 7.08427E-01\n",
      "Model saved to models/mlp_y22_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run2.pth\n",
      "Run 3/5, seed=44\n",
      "Early stopping at epoch 56\n",
      "Best val loss: 7.07481E-01\n",
      "Model saved to models/mlp_y22_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run3.pth\n",
      "Run 4/5, seed=45\n",
      "Early stopping at epoch 34\n",
      "Best val loss: 7.07526E-01\n",
      "Model saved to models/mlp_y22_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run4.pth\n",
      "Run 5/5, seed=46\n",
      "Early stopping at epoch 64\n",
      "Best val loss: 7.00461E-01\n",
      "Model saved to models/mlp_y22_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run5.pth\n",
      "Averaged predictions for year 22 computed.\n",
      "\n",
      "=== Year 23: running 5 restarts ===\n",
      "Run 1/5, seed=42\n",
      "Early stopping at epoch 44\n",
      "Best val loss: 9.71384E-01\n",
      "Model saved to models/mlp_y23_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run1.pth\n",
      "Run 2/5, seed=43\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 9.54819E-01\n",
      "Model saved to models/mlp_y23_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run2.pth\n",
      "Run 3/5, seed=44\n",
      "Early stopping at epoch 39\n",
      "Best val loss: 9.66513E-01\n",
      "Model saved to models/mlp_y23_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run3.pth\n",
      "Run 4/5, seed=45\n",
      "Early stopping at epoch 47\n",
      "Best val loss: 9.68659E-01\n",
      "Model saved to models/mlp_y23_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run4.pth\n",
      "Run 5/5, seed=46\n",
      "Early stopping at epoch 41\n",
      "Best val loss: 9.47128E-01\n",
      "Model saved to models/mlp_y23_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run5.pth\n",
      "Averaged predictions for year 23 computed.\n",
      "\n",
      "=== Year 24: running 5 restarts ===\n",
      "Run 1/5, seed=42\n",
      "Early stopping at epoch 40\n",
      "Best val loss: 7.07026E-01\n",
      "Model saved to models/mlp_y24_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run1.pth\n",
      "Run 2/5, seed=43\n",
      "Early stopping at epoch 38\n",
      "Best val loss: 7.09599E-01\n",
      "Model saved to models/mlp_y24_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run2.pth\n",
      "Run 3/5, seed=44\n",
      "Early stopping at epoch 45\n",
      "Best val loss: 7.08193E-01\n",
      "Model saved to models/mlp_y24_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run3.pth\n",
      "Run 4/5, seed=45\n",
      "Early stopping at epoch 44\n",
      "Best val loss: 7.07269E-01\n",
      "Model saved to models/mlp_y24_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run4.pth\n",
      "Run 5/5, seed=46\n",
      "Early stopping at epoch 47\n",
      "Best val loss: 7.15142E-01\n",
      "Model saved to models/mlp_y24_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run5.pth\n",
      "Averaged predictions for year 24 computed.\n"
     ]
    }
   ],
   "source": [
    "best_models = {}\n",
    "history   = {}\n",
    "mlp_pred  = {}\n",
    "\n",
    "for y, period in periods.items():\n",
    "    print(f\"\\n=== Year {y}: running {n_runs} restarts ===\")\n",
    "    all_run_preds = []\n",
    "\n",
    "    for run in range(n_runs):\n",
    "        # set random seed for reproducibility (change for each run)\n",
    "        seed = 42 + run\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        print(f\"Run {run+1}/{n_runs}, seed={seed}\")\n",
    "\n",
    "        # instantiate a fresh model & optimizer\n",
    "        input_dim = X_train[y].shape[1]\n",
    "        model = MLPModel(input_dim,\n",
    "                         depth=hidden_depth,\n",
    "                         width=hidden_width,\n",
    "                         dropout=dropout,\n",
    "                         activation=activation_fun).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                      lr=learning_rate)\n",
    "\n",
    "\n",
    "        # wrap datasets\n",
    "        train_ds = MLPdataset(X_train[y], y_train[y])\n",
    "        val_ds   = MLPdataset(X_val[y],   y_val[y])\n",
    "\n",
    "        # train\n",
    "        trained_model, hist = train_mlp(train_ds,\n",
    "                                        val_ds,\n",
    "                                        model,\n",
    "                                        criterion,\n",
    "                                        epochs,\n",
    "                                        patience,\n",
    "                                        print_freq,\n",
    "                                        device,\n",
    "                                        optimizer=optimizer,\n",
    "                                        lambda_l1=lambda_l1,\n",
    "                                        lambda_l2=lambda_l2,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle_train=True,\n",
    "                                        shuffle_val=False,\n",
    "                                        save_path=f'models/mlp_y{y}_l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}.pth'\n",
    "                                        )\n",
    "\n",
    "        # predict on test set\n",
    "        preds = predict_mlp(trained_model,\n",
    "                            X_test[y],\n",
    "                            y_test=y_test[y],\n",
    "                            scaler= y_scalers[y],\n",
    "                            batch_size=batch_size,\n",
    "                            device=device)\n",
    "        all_run_preds.append(preds)\n",
    "\n",
    "        # optionally store the last run’s model & history\n",
    "        best_models[(y, run)] = trained_model\n",
    "        history[(y, run)]     = hist\n",
    "\n",
    "    # stack (n_runs, n_samples) average over axis=0\n",
    "    all_run_preds = np.stack(all_run_preds, axis=0)\n",
    "    mlp_pred[y]   = np.mean(all_run_preds, axis=0)\n",
    "\n",
    "    print(f\"Averaged predictions for year {y} computed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_2_'></a>[Pyramid](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model with most data on multiple parameters\n",
    "hidden_depth_pyr = None\n",
    "hidden_width_pyr = [32, 16, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Year 21: running 5 restarts ===\n",
      "Run 1/5, seed=42\n",
      "Early stopping at epoch 71\n",
      "Best val loss: 1.13797E+00\n",
      "Model saved to models/mlp_y21_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run1.pth\n",
      "Run 2/5, seed=43\n",
      "Early stopping at epoch 191\n",
      "Best val loss: 1.09939E+00\n",
      "Model saved to models/mlp_y21_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run2.pth\n",
      "Run 3/5, seed=44\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 1.13664E+00\n",
      "Model saved to models/mlp_y21_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run3.pth\n",
      "Run 4/5, seed=45\n",
      "Epoch 250/250  - Train Loss: 8.79312E-01  - Val Loss: 1.11458E+00\n",
      "Best val loss: 1.11349E+00\n",
      "Model saved to models/mlp_y21_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run4.pth\n",
      "Run 5/5, seed=46\n",
      "Early stopping at epoch 38\n",
      "Best val loss: 1.13155E+00\n",
      "Model saved to models/mlp_y21_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run5.pth\n",
      "Averaged predictions for year 21 computed.\n",
      "\n",
      "=== Year 22: running 5 restarts ===\n",
      "Run 1/5, seed=42\n",
      "Early stopping at epoch 71\n",
      "Best val loss: 6.98848E-01\n",
      "Model saved to models/mlp_y22_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run1.pth\n",
      "Run 2/5, seed=43\n",
      "Early stopping at epoch 74\n",
      "Best val loss: 7.02643E-01\n",
      "Model saved to models/mlp_y22_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run2.pth\n",
      "Run 3/5, seed=44\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 7.07970E-01\n",
      "Model saved to models/mlp_y22_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run3.pth\n",
      "Run 4/5, seed=45\n",
      "Early stopping at epoch 64\n",
      "Best val loss: 6.99254E-01\n",
      "Model saved to models/mlp_y22_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run4.pth\n",
      "Run 5/5, seed=46\n",
      "Early stopping at epoch 40\n",
      "Best val loss: 7.08259E-01\n",
      "Model saved to models/mlp_y22_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run5.pth\n",
      "Averaged predictions for year 22 computed.\n",
      "\n",
      "=== Year 23: running 5 restarts ===\n",
      "Run 1/5, seed=42\n",
      "Early stopping at epoch 27\n",
      "Best val loss: 9.49043E-01\n",
      "Model saved to models/mlp_y23_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run1.pth\n",
      "Run 2/5, seed=43\n",
      "Early stopping at epoch 26\n",
      "Best val loss: 9.56480E-01\n",
      "Model saved to models/mlp_y23_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run2.pth\n",
      "Run 3/5, seed=44\n",
      "Early stopping at epoch 51\n",
      "Best val loss: 9.69087E-01\n",
      "Model saved to models/mlp_y23_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run3.pth\n",
      "Run 4/5, seed=45\n",
      "Early stopping at epoch 45\n",
      "Best val loss: 9.53210E-01\n",
      "Model saved to models/mlp_y23_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run4.pth\n",
      "Run 5/5, seed=46\n",
      "Early stopping at epoch 58\n",
      "Best val loss: 9.82522E-01\n",
      "Model saved to models/mlp_y23_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run5.pth\n",
      "Averaged predictions for year 23 computed.\n",
      "\n",
      "=== Year 24: running 5 restarts ===\n",
      "Run 1/5, seed=42\n",
      "Early stopping at epoch 34\n",
      "Best val loss: 7.11688E-01\n",
      "Model saved to models/mlp_y24_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run1.pth\n",
      "Run 2/5, seed=43\n",
      "Early stopping at epoch 38\n",
      "Best val loss: 7.08744E-01\n",
      "Model saved to models/mlp_y24_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run2.pth\n",
      "Run 3/5, seed=44\n",
      "Early stopping at epoch 49\n",
      "Best val loss: 7.08777E-01\n",
      "Model saved to models/mlp_y24_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run3.pth\n",
      "Run 4/5, seed=45\n",
      "Early stopping at epoch 54\n",
      "Best val loss: 7.21102E-01\n",
      "Model saved to models/mlp_y24_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run4.pth\n",
      "Run 5/5, seed=46\n",
      "Early stopping at epoch 50\n",
      "Best val loss: 7.13668E-01\n",
      "Model saved to models/mlp_y24_l11e-05_l20.0001_drop0.0_lr0.0001_w16_d4_run5.pth\n",
      "Averaged predictions for year 24 computed.\n"
     ]
    }
   ],
   "source": [
    "best_models_pyr = {}\n",
    "history_pyr   = {}\n",
    "mlp_pred_pyr  = {}\n",
    "\n",
    "for y, period in periods.items():\n",
    "    print(f\"\\n=== Year {y}: running {n_runs} restarts ===\")\n",
    "    all_run_preds = []\n",
    "\n",
    "    for run in range(n_runs):\n",
    "        # set random seed for reproducibility (change for each run)\n",
    "        seed = 42 + run\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        print(f\"Run {run+1}/{n_runs}, seed={seed}\")\n",
    "\n",
    "        # instantiate a fresh model & optimizer\n",
    "        input_dim = X_train[y].shape[1]\n",
    "        model = MLPModel(input_dim,\n",
    "                         depth=len(hidden_width_pyr),\n",
    "                         width=hidden_width_pyr,\n",
    "                         dropout=dropout,\n",
    "                         activation=activation_fun).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                      lr=learning_rate)\n",
    "\n",
    "\n",
    "        # wrap datasets\n",
    "        train_ds = MLPdataset(X_train[y], y_train[y])\n",
    "        val_ds   = MLPdataset(X_val[y],   y_val[y])\n",
    "\n",
    "        # train\n",
    "        trained_model, hist = train_mlp(train_ds,\n",
    "                                        val_ds,\n",
    "                                        model,\n",
    "                                        criterion,\n",
    "                                        epochs,\n",
    "                                        patience,\n",
    "                                        print_freq,\n",
    "                                        device,\n",
    "                                        optimizer=optimizer,\n",
    "                                        lambda_l1=lambda_l1,\n",
    "                                        lambda_l2=lambda_l2,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle_train=True,\n",
    "                                        shuffle_val=False,\n",
    "                                        save_path=f'models/mlp_y{y}_l1{lambda_l1}_l2{lambda_l2}_drop{dropout}_lr{learning_rate}_w{hidden_width}_d{hidden_depth}_run{run+1}.pth'\n",
    "                                        )\n",
    "\n",
    "        # predict on test set\n",
    "        preds = predict_mlp(trained_model,\n",
    "                            X_test[y],\n",
    "                            y_test=y_test[y],\n",
    "                            scaler= y_scalers[y], # preprocessors[y] if hasattr(preprocessors[y], 'inverse_transform') else None,\n",
    "                            batch_size=batch_size,\n",
    "                            device=device)\n",
    "        all_run_preds.append(preds)\n",
    "\n",
    "        # optionally store the last run’s model & history\n",
    "        best_models_pyr[(y, run)] = trained_model\n",
    "        history_pyr[(y, run)]     = hist\n",
    "\n",
    "    # stack (n_runs, n_samples) average over axis=0\n",
    "    all_run_preds = np.stack(all_run_preds, axis=0)\n",
    "    mlp_pred_pyr[y] = np.mean(all_run_preds, axis=0)\n",
    "\n",
    "    print(f\"Averaged predictions for year {y} computed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[Train linear models](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc6_1_'></a>[OLS](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating OLS for 21...\n",
      "Estimating OLS for 22...\n",
      "Estimating OLS for 23...\n",
      "Estimating OLS for 24...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johan/Documents/04 Uni/10 Thesis/git/MastersThesis/libs/functions.py:197: RuntimeWarning: invalid value encountered in sqrt\n",
      "  se = np.sqrt(np.diag(cov)).reshape(-1, 1)\n"
     ]
    }
   ],
   "source": [
    "# linear model\n",
    "# estimate the parameters\n",
    "ols_est = {}\n",
    "ols_pred = {}\n",
    "ols_coefs = {}\n",
    "\n",
    "\n",
    "for y in periods.keys():\n",
    "    print(f\"Estimating OLS for {y}...\")\n",
    "    x_tr = Xlin_train[y]\n",
    "    y_tr = ylin_train[y]\n",
    "    x_te = Xlin_test[y]\n",
    "    y_te = ylin_test[y]\n",
    "\n",
    "\n",
    "    # estimate the parameters\n",
    "    ols_est[y] = estimate(y_tr, x_tr)\n",
    "    # ols_pred_train[y] = ols_est[y]['b_hat'] @ x_tr.T\n",
    "    ols_pred[y] = ols_est[y]['b_hat'] @ x_te.T\n",
    "    ols_coefs[y] = ols_est[y]['b_hat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_coefs_df = pd.DataFrame(ols_coefs, index=feature_cols_lin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc6_2_'></a>[LASSO](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating LASSO for 21...\n",
      "Estimating LASSO for 22...\n",
      "Estimating LASSO for 23...\n",
      "Estimating LASSO for 24...\n"
     ]
    }
   ],
   "source": [
    "# linear model\n",
    "# create a grid using numpy.geomspace\n",
    "penalty_grid = np.geomspace(1e-7, 100, num = 1000)\n",
    "lasso_est = {}\n",
    "# lasso_pred_train = {}\n",
    "lasso_pred = {}\n",
    "lasso_coefs = {}\n",
    "\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", ConvergenceWarning)\n",
    "    for y in periods.keys():\n",
    "        print(f\"Estimating LASSO for {y}...\")\n",
    "        x_tr = Xlin_train[y]\n",
    "        y_tr = ylin_train[y]\n",
    "        x_te = Xlin_test[y]\n",
    "        y_te = ylin_test[y]\n",
    "\n",
    "        # estimate the model using LassoCV\n",
    "        fit_CV = LassoCV(cv=5, alphas=penalty_grid, max_iter=1000, eps=1e-3, n_jobs=-1).fit(x_tr,y_tr)\n",
    "        # lasso_pred_train[y] = fit_CV.predict(x_tr)\n",
    "        lasso_pred[y] = fit_CV.predict(x_te)\n",
    "\n",
    "        # store the coefficients\n",
    "        coef = fit_CV.coef_\n",
    "        lasso_coefs[y] = coef\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_coefs_df = pd.DataFrame(lasso_coefs, index=feature_cols_lin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc6_3_'></a>[Naïve](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_pred = {}\n",
    "\n",
    "for y in periods.keys():\n",
    "    # naive prediction is the mean of the training set\n",
    "    pred = np.mean(ylin_train[y])\n",
    "    naive_pred[y] = np.full_like(ylin_test[y], pred, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc7_'></a>[Summarize the results](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dfs = []\n",
    "\n",
    "for y, period in periods.items():\n",
    "    # rebuild masks\n",
    "    te_mask = ((df_norm['timestamp'] - pd.DateOffset(years=1) >= period) &\n",
    "               (df_norm['timestamp'] - pd.DateOffset(years=2) <  period))\n",
    "    X_te_df = df_norm.loc[te_mask, feature_cols]\n",
    "    idx = X_te_df.index\n",
    "\n",
    "\n",
    "    pred_dfs.append(pd.DataFrame({\n",
    "        'period':    y,\n",
    "        'timestamp': df_norm.loc[idx, 'timestamp'],\n",
    "        'ticker':    df_norm.loc[idx, 'ticker'],\n",
    "        'y_true':    df_norm.loc[idx, 'target'].values.astype('float32'),\n",
    "        # 'discount':  df.loc[idx, 'discount'].values.astype('float32'),\n",
    "        'Naïve':  naive_pred[y],\n",
    "        'OLS':    ols_pred[y].flatten(),\n",
    "        'LASSO':  lasso_pred[y],\n",
    "        'MLP':    mlp_pred[y],\n",
    "        'MLP-Pyr': mlp_pred_pyr[y],\n",
    "    }, index=idx))\n",
    "\n",
    "all_preds = pd.concat(pred_dfs).sort_index()\n",
    "\n",
    "# save predictions\n",
    "all_preds.to_csv('data/predictions.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{clccccc}\n",
      "\\hline\\hline \\\\ [-1.8ex]\n",
      " &  & Naïve & OLS & LASSO & MLP & MLP-Pyr \\\\ \n",
      " \\hline \n",
      "\\multirow[c]{5}{*}{\\rotatebox{90}{RMSE}} \n",
      "& 2021 & 0.10729 & 0.11654 & 0.10664 & 0.10645 & 0.10584 \\\\ \n",
      " & 2022 & 0.12309 & 0.15493 & 0.12635 & 0.12426 & 0.12480 \\\\ \n",
      " & 2023 & 0.10608 & 0.12202 & 0.11306 & 0.10570 & 0.10586 \\\\ \n",
      " & 2024 & 0.10975 & 0.11566 & 0.11193 & 0.10936 & 0.10946 \\\\ \n",
      " & Total & 0.11177 & 0.12839 & 0.11471 & 0.11171 & 0.11177 \\\\ \n",
      "\\hline\\multirow[c]{5}{*}{\\rotatebox{90}{MAE}} \n",
      "& 2021 & 0.06921 & 0.08429 & 0.07233 & 0.06970 & 0.06962 \\\\ \n",
      " & 2022 & 0.08553 & 0.10331 & 0.08992 & 0.08718 & 0.08776 \\\\ \n",
      " & 2023 & 0.06752 & 0.08290 & 0.07387 & 0.06659 & 0.06638 \\\\ \n",
      " & 2024 & 0.06864 & 0.07542 & 0.07162 & 0.06842 & 0.06820 \\\\ \n",
      " & Total & 0.07277 & 0.08660 & 0.07697 & 0.07303 & 0.07305 \\\\ \n",
      "\\hline\\multirow[c]{5}{*}{\\rotatebox{90}{AMADL}} \n",
      "& 2021 & -0.00444 & -0.00268 & -0.00602 & -0.00479 & -0.00476 \\\\ \n",
      " & 2022 & 0.01058 & 0.01557 & 0.01525 & 0.01006 & 0.01309 \\\\ \n",
      " & 2023 & 0.00543 & 0.00731 & 0.00634 & 0.00357 & 0.00550 \\\\ \n",
      " & 2024 & 0.00389 & 0.00887 & 0.00738 & 0.00358 & 0.00385 \\\\ \n",
      " & Total & 0.00380 & 0.00718 & 0.00564 & 0.00305 & 0.00436 \\\\ \n",
      "\\hline\\hline\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "all_preds = pd.read_csv('data/predictions.csv', index_col=0, parse_dates=['timestamp'])\n",
    "all_preds['period'] = all_preds['period'].astype('string')\n",
    "\n",
    "metrics_full = {}\n",
    "results_full = {}\n",
    "methods = ['Naïve',\n",
    "           'OLS', \n",
    "           'LASSO', \n",
    "           'MLP',\n",
    "           'MLP-Pyr',\n",
    "           ]\n",
    "for method in methods:\n",
    "    for y, period in periods.items():\n",
    "        y_true = all_preds.loc[all_preds['period'] == y, 'y_true']\n",
    "        y_pred = all_preds.loc[all_preds['period'] == y, method]\n",
    "        key = f'{method}{y}'\n",
    "        results_full[key] = {\n",
    "            'RMSE': rmse_fun(y_pred, y_true),\n",
    "            'MAE': mae_fun(y_pred, y_true),\n",
    "            # 'MADL': madl_fun(y_pred, y_true),\n",
    "            'AMADL': amadl_fun(y_pred, y_true, delta=0.5)\n",
    "        }\n",
    "    key = f'{method}Total'\n",
    "    results_full[key] = {\n",
    "        'RMSE': rmse_fun(all_preds[method], all_preds['y_true']),\n",
    "        'MAE': mae_fun(all_preds[method], all_preds['y_true']),\n",
    "        # 'MADL': madl_fun(all_preds[method], all_preds['y_true']),\n",
    "        'AMADL' : amadl_fun(all_preds[method], all_preds['y_true'], delta=0.5)\n",
    "    }\n",
    "    \n",
    "for y in list(periods.keys()) + ['Total']:\n",
    "    if y != 'Total':\n",
    "        name = '20' + y\n",
    "    else:\n",
    "        name = y\n",
    "    for metric in ['RMSE',\n",
    "                   'MAE',\n",
    "                #    'MADL',\n",
    "                   'AMADL']:\n",
    "        key = f'*{metric}*{name}'\n",
    "        vals = [\n",
    "            results_full[f'Naïve{y}'][metric],\n",
    "            results_full[f'OLS{y}'][metric],\n",
    "            results_full[f'LASSO{y}'][metric],\n",
    "            results_full[f'MLP{y}'][metric],\n",
    "            results_full[f'MLP-Pyr{y}'][metric],\n",
    "        ]\n",
    "        metrics_full[key] = vals\n",
    "\n",
    "tab_full = latex_table_grouped(methods,metrics_full)\n",
    "\n",
    "with open('tabs/prediction_results.tex', 'w') as f:\n",
    "    f.write(tab_full)\n",
    "# print(tab_full)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc7_1_'></a>[Variable importance](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "period_order   = list(periods.keys())             # ['21','22','23','24']\n",
    "run_order      = list(range(n_runs))              # [0,1,2]\n",
    "\n",
    "imp_by_model_year = defaultdict(list)\n",
    "\n",
    "# ols and lasso\n",
    "for y in period_order:\n",
    "    X_eval, y_eval = Xlin_test[y], ylin_test[y].flatten()\n",
    "\n",
    "    beta_ols    = ols_coefs_df[y].loc[feature_cols_lin].values\n",
    "    beta_lasso  = lasso_coefs_df[y].loc[feature_cols_lin].values \n",
    "\n",
    "    imp_by_model_year['OLS'].append(\n",
    "        importance_lin(beta_ols, X_eval, y_eval, const='const', \n",
    "                       feature=feature_cols_lin)\n",
    "    )\n",
    "    imp_by_model_year['LASSO'].append(\n",
    "        importance_lin(beta_lasso, X_eval, y_eval, const='const', \n",
    "                       feature=feature_cols_lin)\n",
    "    )\n",
    "\n",
    "# constant width mlp\n",
    "for y in period_order:\n",
    "    X_eval, y_eval = X_test[y], y_test[y].flatten()\n",
    "\n",
    "    # mean across runs first\n",
    "    imp_runs = []\n",
    "    for r in run_order:\n",
    "        model = best_models[(y, r)]\n",
    "        imp_runs.append(importance_nn(model, X_eval, y_eval, device))\n",
    "    imp_by_model_year['MLP'].append( np.mean(imp_runs, axis=0) )\n",
    "\n",
    "# pyramid model\n",
    "for y in period_order:\n",
    "    X_eval, y_eval = X_test[y], y_test[y].flatten()\n",
    "\n",
    "    imp_runs = []\n",
    "    for r in run_order:\n",
    "        model = best_models_pyr[(y, r)]\n",
    "        imp_runs.append(importance_nn(model, X_eval, y_eval, device))\n",
    "    imp_by_model_year['MLP-pyr'].append( np.mean(imp_runs, axis=0) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_by_model = {\n",
    "    m : np.mean(vectors, axis=0)           # arithmetic mean over years\n",
    "    for m, vectors in imp_by_model_year.items()\n",
    "}\n",
    "\n",
    "# build a single DataFrame  (rows = features, cols = models)\n",
    "imp_df = pd.DataFrame(\n",
    "    { m : vec for m, vec in imp_by_model.items() },\n",
    "    index = feature_cols\n",
    ")\n",
    "\n",
    "base_names = pd.Series(imp_df.index).map(group_label).values\n",
    "imp_df_agg = (\n",
    "    imp_df\n",
    "      .assign(base = base_names)\n",
    "      .groupby('base', sort=False)\n",
    "      .sum()\n",
    ")\n",
    "\n",
    "# optional: order rows by overall importance\n",
    "imp_df_agg = imp_df_agg.loc[\n",
    "    imp_df_agg.mean(axis=1).sort_values(ascending=False).index\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(1.5*len(imp_df_agg.columns),\n",
    "                                0.2*len(imp_df_agg)))\n",
    "\n",
    "im = ax.imshow(imp_df_agg.values, aspect='auto', cmap='Blues', vmin=0, vmax=0.05)\n",
    "ax.set_xticks(np.arange(len(imp_df_agg.columns)))\n",
    "ax.set_xticklabels(imp_df_agg.columns, ha='center')\n",
    "ax.set_yticks(np.arange(len(imp_df_agg.index)))\n",
    "ax.set_yticklabels(imp_df_agg.index, fontsize=10)\n",
    "\n",
    "# cbar = fig.colorbar(im, ax=ax, fraction=0.02, pad=0.02)\n",
    "# cbar.ax.set_ylabel(\"Normalised $\\Delta$MSE\",\n",
    "#                    rotation=-90, va='bottom')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('figs/variable_importance.png', bbox_inches='tight')\n",
    "# plt.show()\n",
    "plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
